# A first analysis using data in a database {#sec-working_with_databases}

![](../images/lter_penguins.png){width="250"}

*Artwork by [\@allison_horst](https://x.com/allison_horst)*

Before we start working with healthcare data spread across a database using the OMOP Common Data Model, let's first do a simpler analysis. In this case, we will do a quick data analysis with R using a simple dataset held in a database to understand the general approach. For this we'll use data from the [`palmerpenguins`](https://allisonhorst.github.io/palmerpenguins/) package, which contains data on penguins collected from the [Palmer Station](https://en.wikipedia.org/wiki/Palmer_Station) in Antarctica.

## Getting set up

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(dbplyr)
library(ggplot2)
library(DBI)
library(duckdb)
library(palmerpenguins)
```

## Taking a peek at the data

The package [`palmerpenguins`](https://allisonhorst.github.io/palmerpenguins/) contains two datasets, one of them called [`penguins`](https://allisonhorst.github.io/palmerpenguins/reference/penguins.html), which we will use in this chapter. We can get an overview of the data using the [`glimpse()`](https://pillar.r-lib.org/reference/glimpse.html) command.

```{r}
glimpse(penguins)
```

Or we could take a look at the first rows of the data using [`head()`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head):

```{r}
head(penguins, 5)
```

## Inserting data into a database

By default, the data provided by the package is local (stored in memory on your computer), so let's first put it into a [DuckDB](https://duckdb.org/) database. We need to first create the database.

```{r}
con <- dbConnect(drv = duckdb())
```

See that now we have created an empty DuckDB database. We can easily add the penguins data to it.

```{r}
dbWriteTable(conn = con, name = "penguins", value = penguins)
```

With the function `dbListTables()` we can list the tables of a database. In our case, we can see it now has one table:

```{r}
dbListTables(conn = con)
```

And now that the data is in a database we could use SQL directly to get the first rows that we saw before.

```{r}
dbGetQuery(conn = con, statement = "SELECT * FROM penguins LIMIT 5")
```

As you can see we have the same data that we had locally but now it's located inside the DuckDB database we created.

::: {.callout-tip collapse="true"}
### Connecting to databases from R

Database connections from R can be made using the [DBI package](https://dbi.r-dbi.org/). The back-end for `DBI` is facilitated by database-specific driver packages. In the code snippets above, we created a new, empty, in-process DuckDB database to which we then added our dataset. But we could have instead connected to an existing duckdb database. This could, for example, look like:

```{r, eval = FALSE}
con <- dbConnect(drv = duckdb(dbdir = "my_duckdb_database.ducdkb"))
```

Note that if you point to a non-existing DuckDB file, this will be created with an empty database.

In this book, for simplicity, we will mostly be working with in-process DuckDB databases with synthetic data. However, when analysing real patient data we will be more often working with client-server databases, where we are connecting from our computer to a central server with the database or working with data held in the cloud. The approaches shown throughout this book will work in the same way for these other types of database management systems, but the way to connect to the database will be different (although still using DBI). In general, creating connections is supported by associated back-end packages. For example a connection to a Postgres database would use the RPostgres R package and look something like this:

```{r, eval=FALSE}
con <- dbConnect(drv = Postgres(),
                 dbname = "my_database",
                 host = "my_server",
                 user = "user",
                 password = "password")
```

For more examples on how to connect to databases using the DBI package please see [Connecting with DBI](TODO).
:::

## Translation from R to SQL

Instead of using SQL to query our database, we might instead want to use the same R code as before. However, instead of working with the local dataset, now we will need it to query the data held in the database. To do this, first we can create a reference to the table in the database as such:

```{r}
penguins_db <- tbl(src = con, "penguins")
penguins_db
```

Once we have this reference, we can then use it with familiar looking R code.

```{r}
head(penguins_db, 5)
```

The magic here is provided by the [`dbplyr`](https://dbplyr.tidyverse.org) package, which takes the R code and converts it into SQL. In this case the query looks like the SQL we wrote directly before.

```{r}
head(penguins_db, 5) |> 
  show_query()
```

## Example analysis

More complicated SQL can also be generated by using familiar [`dplyr`](https://dplyr.tidyverse.org) code. For example, we could get a summary of bill length by species like so:

```{r, warning=FALSE}
penguins_db |>
  group_by(species) |>
  summarise(
    n = n(),
    min_bill_length_mm = min(bill_length_mm, na.rm = TRUE),
    mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),
    max_bill_length_mm = max(bill_length_mm, na.rm = TRUE)
  ) |>
  mutate(min_max_bill_length_mm = paste0(
    min_bill_length_mm, " to ", max_bill_length_mm
  )) |>
  select("species", "mean_bill_length_mm", "min_max_bill_length_mm")
```

The benefit of using [`dbplyr`](https://dbplyr.tidyverse.org) now becomes quite clear if we take a look at the corresponding SQL that is generated for us:

```{r, warning=FALSE}
penguins_db |>
  group_by(species) |>
  summarise(
    n = n(),
    min_bill_length_mm = min(bill_length_mm, na.rm = TRUE),
    mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),
    max_bill_length_mm = max(bill_length_mm, na.rm = TRUE)
  ) |>
  mutate(min_max_bill_length_mm = paste0(
    min_bill_length_mm, " to ", max_bill_length_mm
  )) |>
  select("species", "mean_bill_length_mm", "min_max_bill_length_mm") |>
  show_query()
```

Instead of having to write this somewhat complex SQL specific to DuckDB, we can use the friendlier `dplyr` syntax that will be more familiar if you're coming from an R programming background.

::: {.callout-note collapse="true"}
### Translation to different SQL dialects

Note this same R code will also work for other SQL dialects such as Postgres, SQL server, Snowflake and Spark. Here you can see the different generated translations:

```{r, echo=FALSE}
library(dbplyr)
args <- list(
  species = character(),
  island = character(),
  bill_length_mm = numeric(),
  bill_depth_mm = numeric(),
  flipper_length_mm = integer(),
  body_mass_g = integer(),
  sex = character(),
  year = integer()
)
summaryQuery <- function(x) {
  x |>
    group_by(species) |>
    summarise(
      n = n(),
      min_bill_length_mm = min(bill_length_mm, na.rm = TRUE),
      mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),
      max_bill_length_mm = max(bill_length_mm, na.rm = TRUE)
    ) |>
    mutate(min_max_bill_length_mm = paste0(
      min_bill_length_mm, " to ", max_bill_length_mm
    )) |>
    select("species", "mean_bill_length_mm", "min_max_bill_length_mm") |>
    show_query()
}
```

::: panel-tabset

### Postgres

```{r, echo=FALSE}
con2 <- simulate_postgres()
lazy_frame(!!!args, con = con2)  |>
  summaryQuery()
```

### SQL Server

```{r, echo=FALSE}
con2 <- simulate_mssql()
lazy_frame(!!!args, con = con2)  |>
  summaryQuery()
```

### Redshift

```{r, echo=FALSE}
con2 <- simulate_redshift()
lazy_frame(!!!args, con = con2)  |>
  summaryQuery()
```

### Snowflake

```{r, echo=FALSE}
con2 <- simulate_snowflake()
lazy_frame(!!!args, con = con2)  |>
  summaryQuery()
```

### Spark

```{r, echo=FALSE}
con2 <- simulate_spark_sql()
lazy_frame(!!!args, con = con2)  |>
  summaryQuery()
```

:::

Note that even though the different SQL statements look similar, each SQL dialect has its own particularities. Using the [`dbplyr`](https://dbplyr.tidyverse.org) approach allows us to support multiple different SQL dialects and back-ends by just writing R code.
:::

Not having to worry about the SQL translation behind our queries allows us to query the database in a simple way even for more complex questions. For instance, suppose now that we are particularly interested in the body mass variable. We can first notice that there are a couple of missing records for this.

```{r}
penguins_db |>
  mutate(missing_body_mass_g = if_else(is.na(body_mass_g), 1, 0)) |>
  group_by(species, missing_body_mass_g) |>
  tally()
```

We can get the mean for each of the species, first dropping those two missing records:

```{r, warning=FALSE, message=FALSE}
penguins_db |>
  group_by(species) |>
  summarise(mean_body_mass_g = round(mean(body_mass_g, na.rm = TRUE)))
```

We could also make a histogram of values for each of the species using the `ggplot2` package. Here we would bring our data back into R before creating our plot with the [`collect()`](https://dplyr.tidyverse.org/reference/collect.html) function.

```{r, warning=FALSE, message=FALSE}
penguins_db |>
  select("species", "body_mass_g") |> 
  collect() |>
  ggplot(aes(group = species, fill = species)) +
  facet_grid(species ~ .) +
  geom_histogram(aes(body_mass_g), colour = "black", binwidth = 100) +
  xlab("Body mass (g)") +
  theme_bw() +
  theme(legend.position = "none")
```

Now let's look at the relationship between body mass and bill depth.

```{r, warning=FALSE, message=FALSE}
penguins_db |>
  select("species", "body_mass_g", "bill_depth_mm") |> 
  collect() |>
  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Bill depth (mm)") +
  ylab("Body mass (g)") +
  theme_bw() +
  theme(legend.position = "none")
```

We see a negative correlation between body mass and bill depth, which seems rather unexpected. But what about if we stratify this query by species?

```{r, warning=FALSE, message=FALSE}
penguins_db |>
  select("species", "body_mass_g", "bill_depth_mm") |>
  collect() |>
  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +
  facet_grid(species ~ .) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Bill depth (mm)") +
  ylab("Body mass (g)") +
  theme_bw() +
  theme(legend.position = "none")
```

As well as having an example of working with data in database from R, you also have an example of [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)!

## Disconnecting from the database

Now that we've reached the end of this example, we can close our connection to the database.

```{r}
dbDisconnect(conn = con)
```

## Further reading

-   [R for Data Science (Chapter 13: Relational data)](https://r4ds.hadley.nz/databases)

-   [Writing SQL with dbplyr](https://dbplyr.tidyverse.org/articles/sql.html)

-   [Data Carpentry: SQL databases and R](https://lessons.datacarpentry.org/R-ecology-lesson/05-r-and-databases.html)
