# Functions for analytic pipelines {#sec-dbplyr_verbs}

We saw in the previous chapter that we can use familiar dplyr verbs with data held in a database. In the last chapter we were working with just a single table which we loaded into the database. When working with databases we will though typically be working with multiple tables (which we'll see later will be true when working with data in the OMOP CDM format). For this chapter we will see more tidyverse functionality that can be used with data in a database, this time using the `nycflights13` data. As we can see, now we have a set of related tables with data on flights departing from New York City airports in 2013.

![](images/relational-01.png)

Let's load the required libraries, add our data to a duckdb database, and then create references to each of these tables.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(dbplyr)
library(tidyr)
library(duckdb)
library(DBI)

db <- dbConnect(duckdb(), dbdir = ":memory:")
copy_nycflights13(db)

airports_db <- tbl(db, "airports")
airports_db |> glimpse()

flights_db <- tbl(db, "flights")
flights_db |> glimpse()

weather_db <- tbl(db, "weather")
weather_db |> glimpse()

planes_db <- tbl(db, "planes")
planes_db |> glimpse()

airlines_db <- tbl(db, "airlines")
airlines_db |> glimpse()
```

### Tidyverse functions

For almost all analyses we want to go from having our starting data spread out across multiple tables in the database to a single tidy table containing the data we need for an analysis. We can often get to our tidy analytic dataset using the below tidyverse functions (most of which coming from dplyr, but a couple also from the tidyr package). These functions all work with data in a database by generating SQL that will have the same purpose as if these functions were being run against data in R.

::: callout-important
Remember until we use `compute()` or `collect()` (or printing the first few rows of the result), all we're doing is translating R code into SQL.
:::

| Purpose                  | Functions                                                | Desription                                                                                                                                                                                                        |
|-----------------|--------------------------------------|-----------------|
| Mutating                 | filter, arrange, distinct                                | These functions are used for selecting and reordering rows in a dataset. For example, `filter` subsets rows based on conditions, `arrange` sorts rows, and `distinct` removes duplicate rows.                     |
| Column Transformation    | mutate, select, relocate, rename                         | These functions are used to create new columns or change existing ones. `mutate` adds or modifies columns, `select` chooses specific columns, and `relocate` changes column order.                                |
| Aggregation              | count tally, summarise                                   | These functions are used for summarizing data, such as computing counts, sums, or other statistics for each group in the data                                                                                     |
| Grouping and ungrouping  | group_by, ungroup                                        | These functions are used to group data by one or more variables and to remove grouping. `group_by` allows operations like aggregation by groups, and `ungroup` removes the grouping.                              |
| Data merging and joining | inner_join, left_join, right_join, anti_join, cross_join | These functions are used to combine data from different tables based on common columns. For example, `inner_join` returns only the rows with matching keys, and `left_join` retains all rows from the left table. |
| Data reshaping           | pivot_wider, pivot_wider                                 | These functions are used to reshape data between wide and long formats. `pivot_wider` turns long data into wide, and `pivot_longer` does the opposite.                                                            |
| Data union               | union_all                                                | This function combines two datasets, keeping all rows from both, and removing duplicates.                                                                                                                         |
| Randomly selects rows    | slice_sample                                             | We can use this to take a random subset of our data by specifying the number or proportion of rows to sample.                                                                                                     |

::: {.callout-tip collapse="true"}
## Behind the scenes

By using the above functions we can use the same code regardless of whether the data was held in the database or locally in R. This is because the functions used above are generic functions which behave differently depending on the type of input they are given. Let's take `inner_join()` for example. We can see that this function is a S3 generic function (with S3 being the most common object-oriented system used in R).

```{r, message=FALSE, warning=FALSE}
library(sloop)
ftype(inner_join)
```

Among others, the references we create to tables in a database have `tbl_lazy` as a class attribute. Meanwhile, we can see that when collected into r the object changes to have different attributes, one of which being `data.frame`

```{r, message=FALSE, warning=FALSE}
class(flights_db)
class(flights_db |> head(1) |> collect())
```

We can see that `inner_join()` has different methods for `tbl_lazy` and `data.frame`.

```{r, message=FALSE, warning=FALSE}
s3_methods_generic("inner_join")
```

When working with references to tables in the database the `tbl_lazy` method will be used.

```{r, message=FALSE, warning=FALSE}
s3_dispatch(flights_db |> 
              inner_join(planes_db))
```

But once we bring data into r, the `data.frame` method will be used.

```{r, message=FALSE, warning=FALSE}
s3_dispatch(flights_db |> head(1) |> collect() |> 
              inner_join(planes_db |> head(1) |> collect()))
```
:::

## Getting to an analytic dataset

To see a little more on how we can use the above functions, let's say we want to do an analysis of late flights from JFK airport. We want to see whether there is some relationship between plane characteristics and the risk of delay (which we arbitrarily define as arriving more than 30 minutes after they were expected).

For this we'll first use the `filter()` and `select()` dplyr verbs to get the data from the flights table. Note, we'll rename arr_delay to just delay.

```{r, message=FALSE, warning=FALSE}
delayed_flights_db <- flights_db |> 
  filter(!is.na(arr_delay),
        origin == "JFK") |> 
  mutate(was_delayed = if_else(arr_delay > 30, 1L, 0L)) |> 
  select(dest, 
         distance, 
         carrier, 
         tailnum, 
         "delay" = "arr_delay",
          was_delayed)
```

::: {.callout-note collapse="true"}
## Show query

```{r, message=FALSE, warning=FALSE, echo=FALSE}
delayed_flights_db |> 
  show_query()
```
:::

And when executed, our results will look like the following

```{r, message=FALSE, warning=FALSE}
delayed_flights_db
```

Now we'll add plane characteristics from the planes table. We will use an inner join so that only records for which we have the plane characteristics will be kept. Just for the sake of making a simple example, we'll categorise planes as large or small, with small planes defined as having 100 or less seats.

```{r, message=FALSE, warning=FALSE}
delayed_flights_db <- delayed_flights_db |> 
  inner_join(planes_db |> 
              select(tailnum, 
                     seats),
            by = join_by(tailnum)) |> 
  mutate(size = case_when(seats > 100 ~ "Large", 
                          seats <= 100 ~ "Small"))
```

It is important to note that our first query was not executed, as we didn't use either `compute()` or `collect()`, so we'll now have added our join to the original query.

::: {.callout-note collapse="true"}
## Show query

```{r, message=FALSE, warning=FALSE, echo=FALSE}
delayed_flights_db |> 
  show_query()
```
:::

And when executed, our results will look like the following

```{r, message=FALSE, warning=FALSE}
delayed_flights_db
```

With this, we can now get the median delay (along with the count of records) grouped by size. We can see in fact that most flights arrived early (had a negative delay)

```{r, message=FALSE, warning=FALSE}
delay_summary_grouped_db <- delayed_flights_db |> 
  summarise(n = n(),
            min = min(delay, na.rm = TRUE),
            q05 = quantile(delay, 0.05, na.rm = TRUE),
            q25 = quantile(delay, 0.25, na.rm = TRUE), 
            median_delay = median(delay, na.rm = TRUE),
            q75 = quantile(delay, 0.75, na.rm = TRUE), 
            q95 = quantile(delay, 0.95, na.rm = TRUE), 
            max = max(delay, na.rm = TRUE),
            .by = c("size")) 
```

::: {.callout-note collapse="true"}
## Show query

```{r, message=FALSE, warning=FALSE, echo=FALSE}
delay_summary_grouped_db |> 
  show_query()
```
:::

```{r, message=FALSE, warning=FALSE}
delay_summary_grouped_db
```

All this we done in the database via translated SQL. But say we wanted to do additional analysis in R, we could collect our analytic dataset and go from there. For example, say we wanted to fit a logistic regression with was_delayed as our dependent variable and size and distance as the explanatory variables. We would collect our data and then continue on from that point in R working with our collected data.

```{r, message=FALSE, warning=FALSE}
library(parsnip)
library(gtsummary)
delayed_flights <- delayed_flights_db |> 
  collect() |> 
  mutate(was_delayed = as.factor(was_delayed))

logistic_reg() |> 
  set_engine("glm") |> 
  fit(was_delayed ~ size + distance, data = delayed_flights) |> 
  tbl_regression()
```
