[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-this-book-for-me",
    "href": "index.html#is-this-book-for-me",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "Is this book for me?",
    "text": "Is this book for me?\nWe’ve written this book for anyone interested in a working with databases using a tidyverse style approach. That is, human centered, consistent, composable, and inclusive (see https://design.tidyverse.org/unifying.html for more details on these principles).\nNew to R? We recommend you compliment the book with R for data science\nNew to databases? We recommend you take a look at some web tutorials on SQL, such as SQLBolt or SQLZoo\nNew to the OMOP CDM? We’d recommend you pare this book with The Book of OHDSI",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-is-the-book-organised",
    "href": "index.html#how-is-the-book-organised",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "How is the book organised?",
    "text": "How is the book organised?\nThe book is divided into two parts. The first half of the book is focused on the general principles for working with databases from R. In these chapters you will see how you can use familiar tidyverse-style code to build up analytic pipelines that start with data held in a database and end with your analytic results. The second half of the book is focused on working with data in the OMOP Common Data Model (CDM) format, a widely used data format for health care data. In these chapters you will see how to work with this data format using the general principles from the first half of the book along with a set of R packages that have been built for the OMOP CDM.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "Citation",
    "text": "Citation\nTO ADD",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "License",
    "text": "License\n This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "Code",
    "text": "Code\nThe source code for the book can be found at this Github repository",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#renv",
    "href": "index.html#renv",
    "title": "Tidy R programming with databases: applications with the OMOP common data model",
    "section": "renv",
    "text": "renv\nThis book is rendered using the following version of packages:\n\n\n\n\n\n\n\n\nPackages\nVersion\nLink\n\n\n\n\nbit64\n4.6.0-1\n🔗\n\n\nCDMConnector\n2.0.0\n🔗\n\n\ncli\n3.6.4\n🔗\n\n\nclock\n0.7.2\n🔗\n\n\nCodelistGenerator\n3.4.1\n🔗\n\n\nCohortCharacteristics\n0.5.1\n🔗\n\n\nCohortConstructor\n0.3.5\n🔗\n\n\nDBI\n1.2.3\n🔗\n\n\ndbplyr\n2.5.0\n🔗\n\n\ndm\n1.0.11\n🔗\n\n\ndplyr\n1.1.4\n🔗\n\n\nduckdb\n1.2.1\n🔗\n\n\nggplot2\n3.5.1\n🔗\n\n\nhere\n1.0.1\n🔗\n\n\nLahman\n12.0-0\n🔗\n\n\nomock\n0.3.2\n🔗\n\n\nomopgenerics\n1.1.1\n🔗\n\n\npalmerpenguins\n0.1.1\n🔗\n\n\nPatientProfiles\n1.3.1\n🔗\n\n\npurrr\n1.0.4\n🔗\n\n\nsloop\n1.0.1\n🔗\n\n\nstringr\n1.5.1\n🔗\n\n\ntidyr\n1.3.1\n🔗\n\n\n\n\n\n\n\nNote here only the packages called explicitly are mentioned for the full list of packages and versions used see the book renv file in github.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Getting started with working databases from R",
    "section": "",
    "text": "In this first half of the book we will see how we can work with databases from R. In the following chapters we’ll see that when working with data held in a relational database we can leverage various open-source R packages to help us perform tidyverse-style data analyses.\n\nIn 1  A first analysis using data in a database we will perform a simple data analysis from start to finish using a table in a database.\nIn 2  Core verbs for analytic pipelines utilising a database we will see in more detail how familiar dplyr functions can be used to combine data spread across different tables in a database into an analytic dataset which we can then bring into R for further analysis.\nIn 3  Supported expressions for database queries we will see how we can perform more complex data manipulation via translation of R code into SQL specific to the database management system being used.\nIn 4  Building analytic pipelines for a data model we will see how we can build data pipelines by creating a data model in R to represent the relational database we’re working with and creating functions and methods to work with it.",
    "crumbs": [
      "Getting started with working databases from R"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html",
    "href": "working_with_databases_from_r.html",
    "title": "1  A first analysis using data in a database",
    "section": "",
    "text": "1.1 Getting set up\nArtwork by @allison_horst\nBefore we start thinking about working with healthcare data spread across a database using the OMOP common data model, let’s first do a simpler analysis. In this case we will do a quick data analysis with R using a simpler dataset held in a database to understand the general approach. For this we’ll use data from palmerpenguins package, which contains data on penguins collected from the Palmer Station in Antarctica.\nAssuming that you have R and RStudio already set up1, first we need to install a few packages not included in base R if we don’t already have them.\ninstall.packages(\"dplyr\")\ninstall.packages(\"dbplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"DBI\")\ninstall.packages(\"duckdb\")\ninstall.packages(\"palmerpenguins\")\nOnce installed, we can load them like so.\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#taking-a-peek-at-the-data",
    "href": "working_with_databases_from_r.html#taking-a-peek-at-the-data",
    "title": "1  A first analysis using data in a database",
    "section": "1.2 Taking a peek at the data",
    "text": "1.2 Taking a peek at the data\nThe package palmerpenguins contains two datasets, one of them called penguins, which we will use in this chapter. We can get an overview of the data using the glimpse() command.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nOr we could take a look at the first rows of the data using head() :\n\nhead(penguins, 5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#inserting-data-into-a-database",
    "href": "working_with_databases_from_r.html#inserting-data-into-a-database",
    "title": "1  A first analysis using data in a database",
    "section": "1.3 Inserting data into a database",
    "text": "1.3 Inserting data into a database\nLet’s put our penguins data into a duckdb database. We need to first create the database and then add the penguins data to it.\n\ndb &lt;- dbConnect(drv = duckdb())\ndbWriteTable(db, \"penguins\", penguins)\n\nWe can see that our database now has one table:\n\ndbListTables(db)\n\n[1] \"penguins\"\n\n\nAnd now that the data is in a database we could use SQL to get the first rows that we saw before.\n\ndbGetQuery(db, \"SELECT * FROM penguins LIMIT 5\")\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n\n\n\n\n\n\n\n\nConnecting to databases from R\n\n\n\n\n\nDatabase connections from R can be made using the DBI package. The back-end for DBI is facilitated by database specific driver packages. In the code snipets above we created a new, empty, in-process duckdb database to which we then added our dataset. But we could have instead connected to an existing duckdb database. This could, for example, look like\n\ndb &lt;- dbConnect(drv = duckdb(), \n                dbdir = here(\"my_duckdb_database.ducdkb\"))\n\nIn this book for simplicity we will mostly be working with in-process duckdb databases with synthetic data. However, when analysing real patient data we will be more often working with client-server databases, where we are connecting from our computer to a central server with the database or working with data held in the cloud. The approaches shown throughout this book will work in the same way for these other types of database management systems, but the way to connect to the database will be different (although still using DBI). In general, creating connections is supported by associated back-end packages. For example a connection to a Postgres database would use the RPostgres R package and look something like:\n\ndb &lt;- dbConnect(Postgres(),\n                dbname = Sys.getenv(\"CDM5_POSTGRESQL_DBNAME\"),\n                host = Sys.getenv(\"CDM5_POSTGRESQL_HOST\"),\n                user = Sys.getenv(\"CDM5_POSTGRESQL_USER\"),\n                password = Sys.getenv(\"CDM5_POSTGRESQL_PASSWORD\"))",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#translation-from-r-to-sql",
    "href": "working_with_databases_from_r.html#translation-from-r-to-sql",
    "title": "1  A first analysis using data in a database",
    "section": "1.4 Translation from R to SQL",
    "text": "1.4 Translation from R to SQL\nInstead of using SQL to query our database, we might instead want to use the same R code as before. However, instead of working with the local dataset, now we will need it to query the data held in the database. To do this, first we can create a reference to the table in the database as such:\n\npenguins_db &lt;- tbl(db, \"penguins\")\npenguins_db\n\n# Source:   table&lt;penguins&gt; [?? x 8]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOnce we have this reference, we can then use it with familiar looking R code.\n\nhead(penguins_db, 5)\n\n# Source:   SQL [?? x 8]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe magic here is provided by the dbplyr package, which takes the R code and converts it into SQL. In this case the query looks like the SQL we wrote directly before.\n\nhead(penguins_db, 5) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT penguins.*\nFROM penguins\nLIMIT 5",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#example-analysis",
    "href": "working_with_databases_from_r.html#example-analysis",
    "title": "1  A first analysis using data in a database",
    "section": "1.5 Example analysis",
    "text": "1.5 Example analysis\nMore complicated SQL can also be generated by using familiar dplyr code. For example, we could get a summary of bill length by species like so:\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    n = n(),\n    min_bill_length_mm = min(bill_length_mm),\n    mean_bill_length_mm = mean(bill_length_mm),\n    max_bill_length_mm = max(bill_length_mm)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(\n    min_bill_length_mm,\n    \" to \",\n    max_bill_length_mm\n  )) |&gt;\n  select(\n    \"species\",\n    \"mean_bill_length_mm\",\n    \"min_max_bill_length_mm\"\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  species   mean_bill_length_mm min_max_bill_length_mm\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;chr&gt;                 \n1 Adelie                   38.8 32.1 to 46.0          \n2 Chinstrap                48.8 40.9 to 58.0          \n3 Gentoo                   47.5 40.9 to 59.6          \n\n\nThe benefit of using dbplyr now becomes quite clear if we take a look at the corresponding SQL that is generated for us:\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    n = n(),\n    min_bill_length_mm = min(bill_length_mm),\n    mean_bill_length_mm = mean(bill_length_mm),\n    max_bill_length_mm = max(bill_length_mm)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(min, \" to \", max)) |&gt;\n  select(\n    \"species\",\n    \"mean_bill_length_mm\",\n    \"min_max_bill_length_mm\"\n  ) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  species,\n  mean_bill_length_mm,\n  CONCAT_WS('', .Primitive(\"min\"), ' to ', .Primitive(\"max\")) AS min_max_bill_length_mm\nFROM (\n  SELECT\n    species,\n    COUNT(*) AS n,\n    MIN(bill_length_mm) AS min_bill_length_mm,\n    AVG(bill_length_mm) AS mean_bill_length_mm,\n    MAX(bill_length_mm) AS max_bill_length_mm\n  FROM penguins\n  GROUP BY species\n) q01\n\n\nInstead of having to write this somewhat complex SQL specific to duckdb we can use the friendlier dplyr syntax that may well be more familiar if coming from an R programming background.\nNot having to worry about the SQL translation behind our queries allows us to interrogate the database in a simple way even for more complex questions. For instance, suppose now that we are particularly interested in the body mass variable. We can first notice that there are a couple of missing records for this.\n\npenguins_db |&gt;\n  mutate(missing_body_mass_g = if_else(\n    is.na(body_mass_g), 1, 0\n  )) |&gt;\n  group_by(species, missing_body_mass_g) |&gt;\n  tally()\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  species   missing_body_mass_g     n\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie                      0   151\n2 Gentoo                      0   123\n3 Adelie                      1     1\n4 Gentoo                      1     1\n5 Chinstrap                   0    68\n\n\nWe can get the mean for each of the species (dropping those two missing records).\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(mean_body_mass_g = round(mean(body_mass_g, na.rm = TRUE)))\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  species   mean_body_mass_g\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie                3701\n2 Chinstrap             3733\n3 Gentoo                5076\n\n\nWe could also make a histogram of values for each of the species. Here we would collect our data back into R before creating our plot.\n\npenguins_db |&gt;\n  select(\"species\", \"body_mass_g\") |&gt; \n  collect() |&gt;\n  ggplot(aes(group = species, fill = species)) +\n  facet_grid(species ~ .) +\n  geom_histogram(aes(body_mass_g), colour = \"black\", binwidth = 100) +\n  xlab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow let’s look at the relationship between body mass and bill depth.\n\npenguins |&gt;\n  select(\"species\", \"body_mass_g\", \"bill_depth_mm\") |&gt; \n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere we see a negative correlation between body mass and bill depth which seems rather unexpected. But what about if we stratify this query by species?\n\npenguins |&gt;\n  select(\"species\", \"body_mass_g\", \"bill_depth_mm\") |&gt;\n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  facet_grid(species ~ .) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs well as having an example of working with data in database from R, you also have an example of Simpson’s paradox!",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#disconnecting-from-the-database",
    "href": "working_with_databases_from_r.html#disconnecting-from-the-database",
    "title": "1  A first analysis using data in a database",
    "section": "1.6 Disconnecting from the database",
    "text": "1.6 Disconnecting from the database\nNow that we’ve reached the end of this example, we can close our connection to the database using the DBI package.\n\ndbDisconnect(db)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#further-reading",
    "href": "working_with_databases_from_r.html#further-reading",
    "title": "1  A first analysis using data in a database",
    "section": "1.7 Further reading",
    "text": "1.7 Further reading\n\nR for Data Science (Chapter 13: Relational data)\nWriting SQL with dbplyr\nData Carpentry: SQL databases and R",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_verbs.html",
    "href": "tidyverse_verbs.html",
    "title": "2  Core verbs for analytic pipelines utilising a database",
    "section": "",
    "text": "2.0.1 Tidyverse functions\nWe saw in the previous chapter that we can use familiar dplyr verbs with data held in a database. In the last chapter we were working with just a single table which we loaded into the database. When working with databases we will though typically be working with multiple tables (as we’ll see later when working with data in the OMOP CDM format). For this chapter we will see more tidyverse functionality that can be used with data in a database, this time using the nycflights13 data. As we can see, now we have a set of related tables with data on flights departing from New York City airports in 2013.\nLet’s load the required libraries, add our data to a duckdb database, and then create references to each of these tables.\nFor almost all analyses we want to go from having our starting data spread out across multiple tables in the database to a single tidy table containing all the data we need for the specific analysis. We can often get to our tidy analytic dataset using the below tidyverse functions (most of which coming from dplyr, but a couple also from the tidyr package). These functions all work with data in a database by generating SQL that will have the same purpose as if these functions were being run against data in R.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "href": "tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "title": "2  Core verbs for analytic pipelines utilising a database",
    "section": "2.1 Getting to an analytic dataset",
    "text": "2.1 Getting to an analytic dataset\nTo see a little more on how we can use the above functions, let’s say we want to do an analysis of late flights from JFK airport. We want to see whether there is some relationship between plane characteristics and the risk of delay.\nFor this we’ll first use the filter() and select() dplyr verbs to get the data from the flights table. Note, we’ll rename arr_delay to just delay.\n\ndelayed_flights_db &lt;- flights_db |&gt; \n  filter(!is.na(arr_delay),\n        origin == \"JFK\") |&gt; \n  select(dest, \n         distance, \n         carrier, \n         tailnum, \n         \"delay\" = \"arr_delay\")\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT dest, distance, carrier, tailnum, arr_delay AS delay\nFROM flights\nWHERE (NOT((arr_delay IS NULL))) AND (origin = 'JFK')\n\n\n\n\n\nWhen executed, our results will look like the following:\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 5]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   dest  distance carrier tailnum delay\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 MIA       1089 AA      N619AA     33\n 2 BQN       1576 B6      N804JB    -18\n 3 MCO        944 B6      N593JB     -8\n 4 PBI       1028 B6      N793JB     -2\n 5 TPA       1005 B6      N657JB     -3\n 6 LAX       2475 UA      N29129      7\n 7 BOS        187 B6      N708JB     -4\n 8 ATL        760 DL      N3739P     -8\n 9 SFO       2586 UA      N532UA     14\n10 RSW       1074 B6      N635JB      4\n# ℹ more rows\n\n\nNow we’ll add plane characteristics from the planes table. We will use an inner join so that only records for which we have the plane characteristics are kept.\n\ndelayed_flights_db &lt;- delayed_flights_db |&gt; \n  inner_join(planes_db |&gt; \n              select(tailnum, \n                     seats),\n            by = join_by(tailnum))\n\nNote that our first query was not executed, as we didn’t use either compute() or collect(), so we’ll now have added our join to the original query.\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT LHS.*, seats\nFROM (\n  SELECT dest, distance, carrier, tailnum, arr_delay AS delay\n  FROM flights\n  WHERE (NOT((arr_delay IS NULL))) AND (origin = 'JFK')\n) LHS\nINNER JOIN planes\n  ON (LHS.tailnum = planes.tailnum)\n\n\n\n\n\nAnd when executed, our results will look like the following:\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   dest  distance carrier tailnum delay seats\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 BQN       1576 B6      N804JB    -18   200\n 2 MCO        944 B6      N593JB     -8   200\n 3 PBI       1028 B6      N793JB     -2   200\n 4 BOS        187 B6      N708JB     -4   200\n 5 ATL        760 DL      N3739P     -8   189\n 6 SJU       1598 B6      N794JB    -21   200\n 7 PHX       2153 US      N535UW      0   379\n 8 BOS        187 B6      N805JB    -10   200\n 9 LAS       2248 B6      N558JB     -6   200\n10 SLC       1990 DL      N3763D     -9   189\n# ℹ more rows\n\n\nGetting to this tidy dataset has been done in the database via R code translated to SQL. With this, we can now collect our analytic dataset into R and go from there (for example, to perform locally statistical analyses which might not be possible to run in a database).\n\ndelayed_flights &lt;- delayed_flights_db |&gt; \n  collect() \n\ndelayed_flights |&gt; \n glimpse()\n\nRows: 93,298\nColumns: 6\n$ dest     &lt;chr&gt; \"BQN\", \"MCO\", \"PBI\", \"BOS\", \"ATL\", \"SJU\", \"PHX\", \"BOS\", \"LAS\"…\n$ distance &lt;dbl&gt; 1576, 944, 1028, 187, 760, 1598, 2153, 187, 2248, 1990, 2586,…\n$ carrier  &lt;chr&gt; \"B6\", \"B6\", \"B6\", \"B6\", \"DL\", \"B6\", \"US\", \"B6\", \"B6\", \"DL\", \"…\n$ tailnum  &lt;chr&gt; \"N804JB\", \"N593JB\", \"N793JB\", \"N708JB\", \"N3739P\", \"N794JB\", \"…\n$ delay    &lt;dbl&gt; -18, -8, -2, -4, -8, -21, 0, -10, -6, -9, -8, 2, 1, 44, -15, …\n$ seats    &lt;int&gt; 200, 200, 200, 200, 189, 200, 379, 200, 200, 189, 178, 182, 2…",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html",
    "href": "tidyverse_expressions.html",
    "title": "3  Supported expressions for database queries",
    "section": "",
    "text": "3.1 Data types\nIn the previous chapter, Chapter 2, we saw that there are a core set of tidyverse functions that can be used with databases to extract data for analysis. The SQL code used in the previous chapter would be the same for all database management systems, with only joins and variable selection being used.\nFor more complex data pipleines we will, however, often need to incorporate additional expressions within these functions. Because of differences across database management systems, the SQL these pipelines get translated to can vary. Moreover, some expressions may only be supported for some subset of databases. When writing code which we want to work across different database management systems we therefore need to keep in mind what is supported where. To help with this, the sections below show the available translations for common expressions we might wish to use.\nLet’s first load the packages which these expressions come from. In addition to base R types, bit64 adds support for integer64. The stringr package provides functions for working with strings, while clock has various functions for working with dates. Many other useful expressions will come from dplyr itself.\nCommonly used data types are consistently supported across database backends. We can use the base as.numeric(), as.integer(), as.charater(), as.Date(), and as.POSIXct(). We can also use as.integer64() from the bit64 package to coerce to integer64, and the as_date() and as_datetime() from the clock package instead of as.Date() and as.POSIXct(), respectively.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#data-types",
    "href": "tidyverse_expressions.html#data-types",
    "title": "3  Supported expressions for database queries",
    "section": "",
    "text": "Show SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC) AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC(38, 0)) AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS VARCHAR(MAX))\n\ntranslate_sql(as.Date(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as.logical(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS BIT)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#comparison-and-logical-operators",
    "href": "tidyverse_expressions.html#comparison-and-logical-operators",
    "title": "3  Supported expressions for database queries",
    "section": "3.2 Comparison and logical operators",
    "text": "3.2 Comparison and logical operators\nBase R comparison operators, such as &lt;, &lt;=, ==, &gt;=, &gt;, are also well supported in all database backends. Logical operators, such as & and | can also be used as if the data was in R.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#conditional-statements",
    "href": "tidyverse_expressions.html#conditional-statements",
    "title": "3  Supported expressions for database queries",
    "section": "3.3 Conditional statements",
    "text": "3.3 Conditional statements\nThe base ifelse function, along with if_else and case_when from dplyr are translated for each database backend. As can be seen in the translations, case_when maps to the SQL CASE WHEN statement.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#working-with-strings",
    "href": "tidyverse_expressions.html#working-with-strings",
    "title": "3  Supported expressions for database queries",
    "section": "3.4 Working with strings",
    "text": "3.4 Working with strings\nCompared to the previous sections, there is much more variation in support of functions to work with strings across database management systems. In particular, although various useful stringr functions do have translations ubiquitously it can be seen below that more translations are available for some databases compared to others.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; TRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g'))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, 'b')\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; (NOT(REGEXP_MATCHES(`var`, 'b')))\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, '[aeiou]')\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`,'^(?:'||'a'))\n\ntranslate_sql(str_ends(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES((?:`var`,'a'||')$')\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_redshift())\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_redshift())\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_redshift())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_redshift())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_redshift())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_postgres())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_postgres())\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_postgres())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_postgres())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; TRIM(`var`)\n\ntranslate_sql(str_squish(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(TRIM(`var`), '\\\\s+', ' ')\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 1.0, 1.0)\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 1.0, 1.0)\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var),\n              con = simulate_spark_sql())\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LEN(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_mssql())\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var),\n              con = simulate_mssql())\n\nError in `str_to_title()`:\n! `str_to_title()` is not available in this SQL variant.\n\ntranslate_sql(str_trim(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var),\n              con = simulate_mssql())\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_mssql())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"),\n              con = simulate_mssql())\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#working-with-dates",
    "href": "tidyverse_expressions.html#working-with-dates",
    "title": "3  Supported expressions for database queries",
    "section": "3.5 Working with dates",
    "text": "3.5 Working with dates\nLike with strings, support for working with dates is somewhat mixed. In general, we would use functions from the clock package such as get_day(), get_month(), get_year() to extract parts from a date, add_days() to add or subtract days to a date, and date_count_between() to get the number of days between two date variables.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL (1.0) day)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL (1.0) year)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_duckdb())\n\nError in `difftime()`:\n! Don't know how to translate `difftime()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATEDIFF('day', `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_duckdb())\n\nError in date_count_between(date_1, date_2, \"year\"): The only supported value for `precision` on SQL backends is \"day\"\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_redshift())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_redshift())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 day')\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 year')\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (CAST(`date_2` AS DATE) - CAST(`date_1` AS DATE))\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_postgres())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_postgres())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_snowflake())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_snowflake())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('DAY', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('MONTH', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('YEAR', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, 1.0)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; ADD_MONTHS('`date_1`', 1.0 * 12.0)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATEDIFF(`date_2`, `date_1`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_spark_sql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_spark_sql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_mssql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_mssql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#data-aggregation",
    "href": "tidyverse_expressions.html#data-aggregation",
    "title": "3  Supported expressions for database queries",
    "section": "3.6 Data aggregation",
    "text": "3.6 Data aggregation\nWithin the context of using summarise(), we can get aggregated results across entire columns using functions such as n(), n_distinct(), sum(), min(), max(), mean(), and sd(). As can be seen below, the SQL for these calculations is similar across different database management systems.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT row(`x`)) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), a = \"a\", con = simulate_mssql()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(CAST(IIF(`x` = 1.0, 1, 0) AS BIT)) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDEV(`x`) AS `sd`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#window-functions",
    "href": "tidyverse_expressions.html#window-functions",
    "title": "3  Supported expressions for database queries",
    "section": "3.7 Window functions",
    "text": "3.7 Window functions\nIn the previous section we saw how aggregate functions can be used to perform operations across entire columns. Window functions differ in that they perform calculations across rows that are in some way related to a current row. For these we now use mutate() instead of using summarise().\nWe can use window functions like cumsum() and cummean() to calculate running totals and averages, or lag() and lead() to help compare rows to their preceding or following rows.\nGiven that window functions compare rows to rows before or after them, we will often use arrange() to specify the order of rows. This will translate into a ORDER BY clause in the SQL. In addition, we may well also want to apply window functions within some specific groupings in our data. Using group_by() would result in a PARTITION BY clause in the translated SQL so that window function operates on each group independently.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_duckdb()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_duckdb()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_postgres()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_postgres()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_redshift()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_redshift()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_snowflake()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_snowflake()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_spark_sql()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_spark_sql()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_mssql()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_mssql()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "href": "tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "title": "3  Supported expressions for database queries",
    "section": "3.8 Calculating quantiles, including the median",
    "text": "3.8 Calculating quantiles, including the median\nSo far we’ve seen that we can perform various data manipulations and calculate summary statistics for different database management systems using the same R code. Although the translated SQL has been different, the databases all supported similar approaches to perform these queries.\nA case where this is not the case is when we are interested in summarising distributions of the data and estimating quantiles. For example, let’s take estimating the median as an example. Some databases only support calculating the median as an aggregation function similar to how min, mean, and max were calculated above. However, some others only support it as a window function like lead and lag above. Unfortunately this means that for some databases quantiles can only be calculated using the summarise aggregation approach, while in others only the mutate window approach can be used.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT MEDIAN(`x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `df`.*, MEDIAN(`x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nℹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nℹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT MEDIAN(`x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `df`.*, MEDIAN(`x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_mssql()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `summarise()` is not supported for SQL\n  Server.\nℹ Use a combination of `distinct()` and `mutate()` for the same result:\n  `mutate(&lt;col&gt; = median(x, na.rm = TRUE)) %&gt;% distinct(&lt;col&gt;)`\n\nlazy_frame(x = c(1,2), con = simulate_mssql()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "dbplyr_packages.html",
    "href": "dbplyr_packages.html",
    "title": "4  Building analytic pipelines for a data model",
    "section": "",
    "text": "4.1 Defining a data model\nIn the previous chapters we’ve seen that after connecting to a database we can create references to the various tables we’ve interested in it and write bespoke analytic code to query them. However, if we are working with the same database over and over again we are likely to want to build some tooling for tasks we are often performing.\nTo see how we can develop a data model with associated methods and functions we’ll use the Lahman baseball data. We can see below how the data is stored across various related tables.\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(cli)\nlibrary(dbplyr)\nlibrary(Lahman)\n\ndb &lt;- dbConnect(duckdb(), dbdir = \":memory:\")\ncopy_lahman(db)\nInstead of manually creating references to tables of interest as we go, we will write a function to create a single reference to the Lahman data.\nlahmanFromCon &lt;- function(con) {\n  lahmanRef &lt;- c(\n    \"AllstarFull\", \"Appearances\", \"AwardsManagers\", \"AwardsPlayers\", \"AwardsManagers\",\n    \"AwardsShareManagers\", \"Batting\", \"BattingPost\", \"CollegePlaying\", \"Fielding\",\n    \"FieldingOF\", \"FieldingOFsplit\", \"FieldingPost\", \"HallOfFame\", \"HomeGames\",\n    \"LahmanData\", \"Managers\", \"ManagersHalf\", \"Parks\", \"People\", \"Pitching\",\n    \"PitchingPost\", \"Salaries\", \"Schools\", \"SeriesPost\", \"Teams\", \"TeamsFranchises\",\n    \"TeamsHalf\"\n  ) |&gt;\n    set_names() |&gt; \n    map(\\(x) tbl(con, x))\n  class(lahmanRef) &lt;- c(\"lahman_ref\", class(lahmanRef))\n  lahmanRef\n}\nWith this function we can now easily get references to all our lahman tables in one go using our lahmanFromCon() function.\nlahman &lt;- lahmanFromCon(db)\n\nlahman$People |&gt;\n  glimpse()\n\nRows: ??\nColumns: 26\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n$ playerID     &lt;chr&gt; \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abada…\n$ birthYear    &lt;int&gt; 1981, 1934, 1939, 1954, 1972, 1985, 1850, 1877, 1869, 186…\n$ birthMonth   &lt;int&gt; 12, 2, 8, 9, 8, 12, 11, 4, 11, 10, 9, 3, 10, 2, 8, 9, 6, …\n$ birthDay     &lt;int&gt; 27, 5, 5, 8, 25, 17, 4, 15, 11, 14, 20, 16, 22, 16, 17, 1…\n$ birthCity    &lt;chr&gt; \"Denver\", \"Mobile\", \"Mobile\", \"Orange\", \"Palm Beach\", \"La…\n$ birthCountry &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"D.R.\", \"USA\", \"USA\", …\n$ birthState   &lt;chr&gt; \"CO\", \"AL\", \"AL\", \"CA\", \"FL\", \"La Romana\", \"PA\", \"PA\", \"V…\n$ deathYear    &lt;int&gt; NA, 2021, 1984, NA, NA, NA, 1905, 1957, 1962, 1926, NA, 1…\n$ deathMonth   &lt;int&gt; NA, 1, 8, NA, NA, NA, 5, 1, 6, 4, NA, 2, 6, NA, NA, NA, N…\n$ deathDay     &lt;int&gt; NA, 22, 16, NA, NA, NA, 17, 6, 11, 27, NA, 13, 11, NA, NA…\n$ deathCountry &lt;chr&gt; NA, \"USA\", \"USA\", NA, NA, NA, \"USA\", \"USA\", \"USA\", \"USA\",…\n$ deathState   &lt;chr&gt; NA, \"GA\", \"GA\", NA, NA, NA, \"NJ\", \"FL\", \"VT\", \"CA\", NA, \"…\n$ deathCity    &lt;chr&gt; NA, \"Atlanta\", \"Atlanta\", NA, NA, NA, \"Pemberton\", \"Fort …\n$ nameFirst    &lt;chr&gt; \"David\", \"Hank\", \"Tommie\", \"Don\", \"Andy\", \"Fernando\", \"Jo…\n$ nameLast     &lt;chr&gt; \"Aardsma\", \"Aaron\", \"Aaron\", \"Aase\", \"Abad\", \"Abad\", \"Aba…\n$ nameGiven    &lt;chr&gt; \"David Allan\", \"Henry Louis\", \"Tommie Lee\", \"Donald Willi…\n$ weight       &lt;int&gt; 215, 180, 190, 190, 184, 235, 192, 170, 175, 169, 220, 19…\n$ height       &lt;int&gt; 75, 72, 75, 75, 73, 74, 72, 71, 71, 68, 74, 71, 70, 78, 7…\n$ bats         &lt;fct&gt; R, R, R, R, L, L, R, R, R, L, R, R, R, R, R, L, R, L, L, …\n$ throws       &lt;fct&gt; R, R, R, R, L, L, R, R, R, L, R, R, R, R, L, L, R, L, R, …\n$ debut        &lt;chr&gt; \"2004-04-06\", \"1954-04-13\", \"1962-04-10\", \"1977-07-26\", \"…\n$ bbrefID      &lt;chr&gt; \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abada…\n$ finalGame    &lt;chr&gt; \"2015-08-23\", \"1976-10-03\", \"1971-09-26\", \"1990-10-03\", \"…\n$ retroID      &lt;chr&gt; \"aardd001\", \"aaroh101\", \"aarot101\", \"aased001\", \"abada001…\n$ deathDate    &lt;date&gt; NA, 2021-01-22, 1984-08-16, NA, NA, NA, 1905-05-17, 1957…\n$ birthDate    &lt;date&gt; 1981-12-27, 1934-02-05, 1939-08-05, 1954-09-08, 1972-08-…",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "dbplyr_packages.html#defining-a-data-model",
    "href": "dbplyr_packages.html#defining-a-data-model",
    "title": "4  Building analytic pipelines for a data model",
    "section": "",
    "text": "The dm package\n\n\n\n\n\nIn this chapter we will be creating a bespoke data model for our database. This approach can be further extended using the dm package, which also provides various helpful functions for creating a data model and working with it.\nSimilar to above, we can use dm to create a single object to access our database tables.\n\nlibrary(dm)\nlahman_dm &lt;- dm(batting = tbl(db, \"Batting\"), \n                people = tbl(db, \"People\"))\nlahman_dm\n\n── Table source ────────────────────────────────────────────────────────────────\nsrc:  DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `batting`, `people`\nColumns: 48\nPrimary keys: 0\nForeign keys: 0\n\n\nUsing this approach, we can make use of various utility functions. For example here we specify primary and foreign keys and then check that the key constraints are satisfied.\n\nlahman_dm &lt;- lahman_dm %&gt;%\n  dm_add_pk(people, playerID) %&gt;%\n  dm_add_fk(batting, playerID, people) \n\nlahman_dm\n\n── Table source ────────────────────────────────────────────────────────────────\nsrc:  DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `batting`, `people`\nColumns: 48\nPrimary keys: 1\nForeign keys: 1\n\ndm_examine_constraints(lahman_dm)\n\nℹ All constraints satisfied.\n\n\nFor more information on the dm package see https://dm.cynkra.com/index.html",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "dbplyr_packages.html#creating-functions-for-the-data-model",
    "href": "dbplyr_packages.html#creating-functions-for-the-data-model",
    "title": "4  Building analytic pipelines for a data model",
    "section": "4.2 Creating functions for the data model",
    "text": "4.2 Creating functions for the data model\nWe can also now make various functions specific to our Lahman data model to facilitate data analyses. Given we know the structure of the data, we can build a set of functions that abstract away some of the complexities of working with data in a database.\nLet’s start by making a small function to get the teams players have played for. We can see that the code we use follows on from the last couple of chapters.\n\ngetTeams &lt;- function(lahman, name = \"Barry Bonds\") {\n  lahman$Batting |&gt;\n    inner_join(\n      lahman$People |&gt;\n        mutate(full_name = paste0(nameFirst, \" \", nameLast)) |&gt;\n        filter(full_name %in% name) |&gt;\n        select(\"playerID\"),\n      by = join_by(playerID)\n    ) |&gt;\n    select(\n      \"teamID\",\n      \"yearID\"\n    ) |&gt;\n    distinct() |&gt;\n    left_join(lahman$Teams,\n      by = join_by(teamID, yearID)\n    ) |&gt;\n    select(\"name\") |&gt;\n    distinct()\n}\n\nNow we can easily get the different teams a player represented. We can see how changing the player name changes the SQL that is getting run behind the scenes.\n\ngetTeams(lahman, \"Babe Ruth\")\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  name            \n  &lt;chr&gt;           \n1 Boston Braves   \n2 New York Yankees\n3 Boston Red Sox  \n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT DISTINCT q01.*\nFROM (\n  SELECT \"name\"\n  FROM (\n    SELECT DISTINCT q01.*\n    FROM (\n      SELECT teamID, yearID\n      FROM Batting\n      INNER JOIN (\n        SELECT playerID\n        FROM (\n          SELECT People.*, CONCAT_WS('', nameFirst, ' ', nameLast) AS full_name\n          FROM People\n        ) q01\n        WHERE (full_name IN ('Babe Ruth'))\n      ) RHS\n        ON (Batting.playerID = RHS.playerID)\n    ) q01\n  ) LHS\n  LEFT JOIN Teams\n    ON (LHS.teamID = Teams.teamID AND LHS.yearID = Teams.yearID)\n) q01\n\n\n\n\n\n\ngetTeams(lahman, \"Barry Bonds\")\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  name                \n  &lt;chr&gt;               \n1 San Francisco Giants\n2 Pittsburgh Pirates  \n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT DISTINCT q01.*\nFROM (\n  SELECT \"name\"\n  FROM (\n    SELECT DISTINCT q01.*\n    FROM (\n      SELECT teamID, yearID\n      FROM Batting\n      INNER JOIN (\n        SELECT playerID\n        FROM (\n          SELECT People.*, CONCAT_WS('', nameFirst, ' ', nameLast) AS full_name\n          FROM People\n        ) q01\n        WHERE (full_name IN ('Barry Bonds'))\n      ) RHS\n        ON (Batting.playerID = RHS.playerID)\n    ) q01\n  ) LHS\n  LEFT JOIN Teams\n    ON (LHS.teamID = Teams.teamID AND LHS.yearID = Teams.yearID)\n) q01\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right time to collect data into R\n\n\n\n\n\nThe function collect() brings data out of the database and into R. When working with large datasets, as is often the case when interacting with a database, we typically want to keep as much computation as possible on the database side. In the case of our getTeams() function, for example, it does everything on the database side and so collecting will just bring out the result of the teams the person played for. In this case we could also use pull() to get our result out as a vector rather that a data frame.\n\ngetTeams(lahman, \"Barry Bonds\") |&gt;\n  collect()\n\n# A tibble: 2 × 1\n  name                \n  &lt;chr&gt;               \n1 San Francisco Giants\n2 Pittsburgh Pirates  \n\ngetTeams(lahman, \"Barry Bonds\") |&gt;\n  pull()\n\n[1] \"San Francisco Giants\" \"Pittsburgh Pirates\"  \n\n\nIn other cases however we may need to collect data so as to perform further analysis steps that are not possible using SQL. This might be the case for plotting or for other analytic steps like fitting statistical models. In such cases we should try to only bring out the data that we need (as we will likely have much less memory available on our local computer than is available for the database).\n\n\n\nSimilarly we could make a function to add the a player’s year of birth to a table.\n\naddBirthCountry &lt;- function(lahmanTbl){\n  lahmanTbl |&gt; \n    left_join(lahman$People |&gt; \n              select(\"playerID\", \"birthCountry\"),\n              join_by(\"playerID\"))\n}\n\n\nlahman$Batting |&gt;\n  addBirthCountry()\n\n# Source:   SQL [?? x 23]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# ℹ more rows\n# ℹ 11 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;, birthCountry &lt;chr&gt;\n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT Batting.*, birthCountry\nFROM Batting\nLEFT JOIN People\n  ON (Batting.playerID = People.playerID)\n\n\n\n\n\n\nlahman$Pitching |&gt;\n  addBirthCountry()\n\n# Source:   SQL [?? x 31]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   playerID  yearID stint teamID lgID      W     L     G    GS    CG   SHO    SV\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL        1     0    11     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL        3     0    45     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL        2     1    25     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL        4     2    47     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL        3     6    73     0     0     0    38\n 6 aardsda01   2010     1 SEA    AL        0     6    53     0     0     0    31\n 7 aardsda01   2012     1 NYA    AL        0     0     1     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL        2     2    43     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL        1     1    33     0     0     0     0\n10 aasedo01    1977     1 BOS    AL        6     2    13    13     4     2     0\n# ℹ more rows\n# ℹ 19 more variables: IPouts &lt;int&gt;, H &lt;int&gt;, ER &lt;int&gt;, HR &lt;int&gt;, BB &lt;int&gt;,\n#   SO &lt;int&gt;, BAOpp &lt;dbl&gt;, ERA &lt;dbl&gt;, IBB &lt;int&gt;, WP &lt;int&gt;, HBP &lt;int&gt;, BK &lt;int&gt;,\n#   BFP &lt;int&gt;, GF &lt;int&gt;, R &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;,\n#   birthCountry &lt;chr&gt;\n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT Pitching.*, birthCountry\nFROM Pitching\nLEFT JOIN People\n  ON (Pitching.playerID = People.playerID)\n\n\n\n\n\nWe could then use our addBirthCountry() function as part of a larger query to summarise the proportion of players from each country over time (based on their presence in the batting table).\n\nplot_data &lt;- lahman$Batting |&gt;\n  select(playerID, yearID) |&gt; \n  addBirthCountry() |&gt;\n  filter(yearID &gt; 1960) |&gt; \n  mutate(birthCountry = case_when(\n    birthCountry == \"USA\" ~ \"USA\",\n    birthCountry == \"D.R.\" ~ \"Dominican Republic\",\n    birthCountry == \"Venezuela\" ~ \"Venezuela\",\n    birthCountry == \"P.R.\" ~ \"Puerto Rico \",\n    birthCountry == \"Cuba\" ~ \"Cuba\",\n    birthCountry == \"CAN\" ~ \"Canada\",\n    birthCountry == \"Mexico\" ~ \"Mexico\",\n    .default = \"Other\"\n  )) |&gt; \n  summarise(n = n(), .by = c(\"yearID\", \"birthCountry\")) |&gt; \n  group_by(yearID) |&gt;\n  mutate(percentage = n / sum(n) * 100) |&gt; \n  ungroup() |&gt; \n  collect()\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT q01.*, (n / SUM(n) OVER (PARTITION BY yearID)) * 100.0 AS percentage\nFROM (\n  SELECT yearID, birthCountry, COUNT(*) AS n\n  FROM (\n    SELECT\n      playerID,\n      yearID,\n      CASE\nWHEN (birthCountry = 'USA') THEN 'USA'\nWHEN (birthCountry = 'D.R.') THEN 'Dominican Republic'\nWHEN (birthCountry = 'Venezuela') THEN 'Venezuela'\nWHEN (birthCountry = 'P.R.') THEN 'Puerto Rico '\nWHEN (birthCountry = 'Cuba') THEN 'Cuba'\nWHEN (birthCountry = 'CAN') THEN 'Canada'\nWHEN (birthCountry = 'Mexico') THEN 'Mexico'\nELSE 'Other'\nEND AS birthCountry\n    FROM (\n      SELECT Batting.playerID AS playerID, yearID, birthCountry\n      FROM Batting\n      LEFT JOIN People\n        ON (Batting.playerID = People.playerID)\n    ) q01\n    WHERE (yearID &gt; 1960.0)\n  ) q01\n  GROUP BY yearID, birthCountry\n) q01\n\n\n\n\n\n\nlibrary(ggplot2)\nplot_data |&gt; \n  ggplot() +\n  geom_col(aes(yearID, \n              percentage, \n              fill = birthCountry), width=1) + \n  theme_minimal() + \n  theme(legend.title = element_blank(), \n        legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefining methods for the data model\n\n\n\n\n\nAs part of our lahmanFromCon() function our data model object has the class “lahman_ref”. Therefore as well as creating user-facing functions to work with our lahman data model, we can also define methods for this object.\n\nclass(lahman)\n\n[1] \"lahman_ref\" \"list\"      \n\n\nWith this we can make some specific methods for a “lahman_ref” object. For example, we can define a print method like so:\n\nprint.lahman_ref &lt;- function(x, ...) {\n  len &lt;- length(names(x))\n  cli_h1(\"# Lahman reference - {len} tables\")\n  cli_li(paste(\n    \"{.strong tables:}\",\n    paste(names(x), collapse = \", \")\n  ))\n  invisible(x)\n}\n\nNow we can see a summary of our lahman data model when we print the object.\n\nlahman\n\n\n\n\n── # Lahman reference - 28 tables ──────────────────────────────────────────────\n\n\n• tables: AllstarFull, Appearances, AwardsManagers, AwardsPlayers,\nAwardsManagers, AwardsShareManagers, Batting, BattingPost, CollegePlaying,\nFielding, FieldingOF, FieldingOFsplit, FieldingPost, HallOfFame, HomeGames,\nLahmanData, Managers, ManagersHalf, Parks, People, Pitching, PitchingPost,\nSalaries, Schools, SeriesPost, Teams, TeamsFranchises, TeamsHalf\n\n\nAnd we can see that this print is being done by the method we defined.\n\nlibrary(sloop)\ns3_dispatch(print(lahman))\n\n=&gt; print.lahman_ref\n   print.list\n * print.default",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "dbplyr_packages.html#building-efficient-analytic-pipelines",
    "href": "dbplyr_packages.html#building-efficient-analytic-pipelines",
    "title": "4  Building analytic pipelines for a data model",
    "section": "4.3 Building efficient analytic pipelines",
    "text": "4.3 Building efficient analytic pipelines\n\n4.3.1 The risk of “clean” R code\nFollowing on from the above approach, we might think it a good idea to make another function addBirthYear(). We can then use it along with our addBirthCountry() to get a summarise average salary by birth country and birth year.\n\naddBirthYear &lt;- function(lahmanTbl){\n  lahmanTbl |&gt; \n    left_join(lahman$People |&gt; \n              select(\"playerID\", \"birthYear\"),\n              join_by(\"playerID\"))\n}\n\nlahman$Salaries |&gt; \n  addBirthCountry() |&gt; \n  addBirthYear() |&gt; \n  summarise(average_salary = mean(salary), \n            .by = c(\"birthCountry\", \"birthYear\"))\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   birthCountry birthYear average_salary\n   &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;\n 1 USA               1960       1030321.\n 2 USA               1952        498378.\n 3 USA               1956        986760.\n 4 USA               1961        811250.\n 5 USA               1950        625076.\n 6 Nicaragua         1954       2083440.\n 7 Panama            1945        875000 \n 8 CAN               1961       1080292.\n 9 Venezuela         1948        632500 \n10 Cuba              1942        250000 \n# ℹ more rows\n\n\nAlthough the R code on the face of it looks fine, when we look at the SQL we can see that our query has two joins to the People table. One join gets information on the birth country and the other on the birth year.\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT birthCountry, birthYear, AVG(salary) AS average_salary\nFROM (\n  SELECT\n    Salaries.*,\n    \"People...2\".birthCountry AS birthCountry,\n    \"People...3\".birthYear AS birthYear\n  FROM Salaries\n  LEFT JOIN People \"People...2\"\n    ON (Salaries.playerID = \"People...2\".playerID)\n  LEFT JOIN People \"People...3\"\n    ON (Salaries.playerID = \"People...3\".playerID)\n) q01\nGROUP BY birthCountry, birthYear\n\n\n\n\n\nTo improve performance, we could instead have a single function to get both of these, birth country and birth year, at the same time.\n\naddCharacteristics &lt;- function(lahmanTbl){\n  lahmanTbl |&gt; \n    left_join(lahman$People |&gt; \n              select(\"playerID\", \"birthYear\", \"birthCountry\"),\n              join_by(\"playerID\"))\n}\n\nlahman$Salaries |&gt; \n  addCharacteristics() |&gt; \n  summarise(average_salary = mean(salary), \n            .by = c(\"birthCountry\", \"birthYear\"))\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   birthCountry birthYear average_salary\n   &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;\n 1 D.R.              1985       1531438.\n 2 USA               1966       1761151.\n 3 Venezuela         1974       4269365.\n 4 Cuba              1987       4932700.\n 5 Panama            1981        555833.\n 6 USA               1978       3133596.\n 7 CAN               1985        501000 \n 8 P.R.              1959        297786.\n 9 USA               1961        811250.\n10 USA               1990        728740.\n# ℹ more rows\n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT birthCountry, birthYear, AVG(salary) AS average_salary\nFROM (\n  SELECT Salaries.*, birthYear, birthCountry\n  FROM Salaries\n  LEFT JOIN People\n    ON (Salaries.playerID = People.playerID)\n) q01\nGROUP BY birthCountry, birthYear\n\n\n\n\n\nNow this query outputs the same result but is simpler than the previous one, thus lowering the computational cost of the analysis. All this is to show that when working with databases we should keep in mind what is going on behind the scenes in terms of the SQL code actually being executed.\n\n\n4.3.2 Piping and SQL\nAlthough piping functions has little impact on performance when using R with data in memory, when working with a database the SQL generated will differ when using multiple function calls (with a separate operation specified in each) instead of multiple operations within a single function call.\nFor example, a single mutate function creating two new variables would generate the below SQL.\n\nlahman$People |&gt; \n  mutate(birthDatePlus1 = \n           add_years(birthDate, 1L),\n         birthDatePlus10 = \n           add_years(birthDate, 10L)) |&gt; \n  select(\"playerID\", \n         \"birthDatePlus1\",\n         \"birthDatePlus10\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  playerID,\n  DATE_ADD(birthDate, INTERVAL (1) year) AS birthDatePlus1,\n  DATE_ADD(birthDate, INTERVAL (10) year) AS birthDatePlus10\nFROM People\n\n\nWhereas the SQL will be different if these were created using multiple mutate calls (with now one being created in a sub-query).\n\nlahman$People |&gt; \n  mutate(birthDatePlus1 = \n           add_years(birthDate, 1L)) |&gt; \n  mutate(birthDatePlus10 = \n           add_years(birthDate, 10L)) |&gt; \n  select(\"playerID\", \n         \"birthDatePlus1\",\n         \"birthDatePlus10\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  playerID,\n  birthDatePlus1,\n  DATE_ADD(birthDate, INTERVAL (10) year) AS birthDatePlus10\nFROM (\n  SELECT People.*, DATE_ADD(birthDate, INTERVAL (1) year) AS birthDatePlus1\n  FROM People\n) q01\n\n\n\n\n4.3.3 Computing intermediate queries\nLet’s say we want to summarise home runs in the batting table and stike outs in the pitching table by the college players attended and their birth year. We could do this like so:\n\nplayers_with_college &lt;- lahman$People |&gt; \n  select(playerID, birthYear) |&gt; \n  inner_join(lahman$CollegePlaying |&gt; \n              filter(!is.na(schoolID)) |&gt; \n              select(playerID, schoolID) |&gt; \n              distinct(),\n            by = join_by(playerID))\n\nlahman$Batting |&gt; \n  left_join(players_with_college,\n            by = join_by(playerID)) |&gt; \n  summarise(home_runs = sum(H, na.rm = TRUE), \n                        .by = c(schoolID, birthYear)) |&gt; \n  collect()\n\n# A tibble: 6,206 × 3\n   schoolID   birthYear home_runs\n   &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1 pennst          1981         0\n 2 virginia        1987        27\n 3 cacerri         1971         3\n 4 usc             1947        11\n 5 lsu             1927      1832\n 6 wake            1915        72\n 7 pepperdine      1969         1\n 8 lsu             1978         2\n 9 miamidade       1982         0\n10 cincy           1950         5\n# ℹ 6,196 more rows\n\nlahman$Pitching |&gt; \n  left_join(players_with_college,\n            by = join_by(playerID)) |&gt; \n  summarise(strike_outs = sum(SO, na.rm = TRUE), \n                        .by = c(schoolID, birthYear))|&gt; \n  collect()\n\n# A tibble: 3,662 × 3\n   schoolID   birthYear strike_outs\n   &lt;chr&gt;          &lt;int&gt;       &lt;dbl&gt;\n 1 michigan        1967         888\n 2 texas           1958         549\n 3 nmstate         1968          98\n 4 stanford        1972         218\n 5 beloitwi        1872           3\n 6 upenn           1964          14\n 7 arkansas        1962         537\n 8 mntclairst      1961          46\n 9 incante         1893         526\n10 illinois        1979          19\n# ℹ 3,652 more rows\n\n\nLooking at the SQL we can see, however, that there is some duplication, because as part of each full query we have run our players_with_college query.\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(H) AS home_runs\nFROM (\n  SELECT Batting.*, birthYear, schoolID\n  FROM Batting\n  LEFT JOIN (\n    SELECT People.playerID AS playerID, birthYear, schoolID\n    FROM People\n    INNER JOIN (\n      SELECT DISTINCT playerID, schoolID\n      FROM CollegePlaying\n      WHERE (NOT((schoolID IS NULL)))\n    ) RHS\n      ON (People.playerID = RHS.playerID)\n  ) RHS\n    ON (Batting.playerID = RHS.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(SO) AS strike_outs\nFROM (\n  SELECT Pitching.*, birthYear, schoolID\n  FROM Pitching\n  LEFT JOIN (\n    SELECT People.playerID AS playerID, birthYear, schoolID\n    FROM People\n    INNER JOIN (\n      SELECT DISTINCT playerID, schoolID\n      FROM CollegePlaying\n      WHERE (NOT((schoolID IS NULL)))\n    ) RHS\n      ON (People.playerID = RHS.playerID)\n  ) RHS\n    ON (Pitching.playerID = RHS.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n\n\n\nTo avoid this we could instead make use of the compute() function to force the computation of this first, intermediate, query to a temporary table in the database.\n\nplayers_with_college &lt;- players_with_college |&gt; \n  compute()\n\nNow we have a temporary table with the result of our players_with_college query, and we can use this in both of our aggregation queries.\n\nplayers_with_college |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM dbplyr_8zJ2ZCPi8p\n\n\n\nlahman$Batting |&gt; \n  left_join(players_with_college,\n            by = join_by(playerID)) |&gt; \n  summarise(home_runs = sum(H, na.rm = TRUE), \n                        .by = c(schoolID, birthYear)) |&gt; \n  collect()\n\n# A tibble: 6,206 × 3\n   schoolID  birthYear home_runs\n   &lt;chr&gt;         &lt;int&gt;     &lt;dbl&gt;\n 1 kentucky       1972       157\n 2 longbeach      1968        19\n 3 elon           1921         1\n 4 lehigh         1901         1\n 5 ucla           1952       306\n 6 usc            1947        11\n 7 tamukvill      1978         0\n 8 stanford       1972        55\n 9 lsu            1927      1832\n10 wake           1915        72\n# ℹ 6,196 more rows\n\nlahman$Pitching |&gt; \n  left_join(players_with_college,\n            by = join_by(playerID)) |&gt; \n  summarise(strike_outs = sum(SO, na.rm = TRUE), \n                        .by = c(schoolID, birthYear))|&gt; \n  collect()\n\n# A tibble: 3,662 × 3\n   schoolID   birthYear strike_outs\n   &lt;chr&gt;          &lt;int&gt;       &lt;dbl&gt;\n 1 pennst          1981         340\n 2 swesterntx      1883          41\n 3 ilparkl         1970         350\n 4 texas           1958         549\n 5 beloitwi        1872           3\n 6 okstate         1977          81\n 7 bethanywv       1922           2\n 8 nmnmjco         1972         232\n 9 flhills         1956         134\n10 stthomasfl      1973          42\n# ℹ 3,652 more rows\n\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(H) AS home_runs\nFROM (\n  SELECT Batting.*, birthYear, schoolID\n  FROM Batting\n  LEFT JOIN dbplyr_8zJ2ZCPi8p\n    ON (Batting.playerID = dbplyr_8zJ2ZCPi8p.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(SO) AS strike_outs\nFROM (\n  SELECT Pitching.*, birthYear, schoolID\n  FROM Pitching\n  LEFT JOIN dbplyr_8zJ2ZCPi8p\n    ON (Pitching.playerID = dbplyr_8zJ2ZCPi8p.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n\n\n\nIn this case the SQL from our initial approach was not so complicated. However, you can imagine that without using computation to intermediate tables, the SQL associated with a series of data manipulations could quickly become unmanageable. Moreover, we can end up with inefficient code that repeatedly gets the same result as part of a larger query. Therefore although we don’t want to overuse computation of intermediate queries, it is often a necessity when creating our analytic pipelines.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "omop.html",
    "href": "omop.html",
    "title": "Working with the OMOP CDM from R",
    "section": "",
    "text": "In this second half of the book we will see how we can work with data in the OMOP CDM format from R.\n\nIn 5  Creating a CDM reference we will see how to create a cdm_reference in R, a data model that contains references to the OMOP CDM tables and provides the foundation for analysis.\nThe OMOP CDM is a person-centric model, and the person and observation period tables are two key tables for any analysis. In ?sec-omop_person_obs_period we will see more on how these tables can be used as the starting point for identifying your study participants.\nThe OMOP CDM standarises the content of health care data via the OMOP CDM vocabulary tables, which provides a set of standard concepts to represent different clinical events. The vocabulary tables are described in ?sec-omop_vocabularies, with these tables playing a fundamental role when we identify the clinical events of interest for our study.\nClinical records associated with individuals are spread across various OMOP CDM tables, covering various domains. In ?sec-omop_clinical_tables we will see how these tables represent events and link back to the person and vocabulary tables.",
    "crumbs": [
      "Working with the OMOP CDM from R"
    ]
  },
  {
    "objectID": "cdm_reference.html",
    "href": "cdm_reference.html",
    "title": "5  Creating a CDM reference",
    "section": "",
    "text": "5.1 The OMOP common data model (CDM) layout\nThe OMOP CDM standardises the structure of healthcare data. Data is stored across a system of tables with established relationships between them. In other words, the OMOP CDM provides a relational database structure, with version 5.4 of the OMOP CDM shown below.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#creating-a-reference-to-the-omop-cdm",
    "href": "cdm_reference.html#creating-a-reference-to-the-omop-cdm",
    "title": "5  Creating a CDM reference",
    "section": "5.2 Creating a reference to the OMOP CDM",
    "text": "5.2 Creating a reference to the OMOP CDM\nAs we saw in Chapter 4, creating a data model in R to represent the OMOP CDM can provide a basis for analytic pipelines using the data. Luckily for us, we won’t have to create functions and methods for this ourselves. Instead, we will use the omopgenerics package which defines a data model for OMOP CDM data and the CDMConnector package which provides functions for connecting to a OMOP CDM data held in a database.\nTo see how this works we will use the omock package to create example data in the format of the OMOP CDM, which we will then copy to a duckdb database.\n\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(here)\nlibrary(dplyr)\nlibrary(omock)\nlibrary(omopgenerics)\nlibrary(CDMConnector)\nlibrary(palmerpenguins)\n\ncdm_local &lt;- mockCdmReference() |&gt;\n    mockPerson(nPerson = 100) |&gt;\n    mockObservationPeriod() |&gt;\n    mockConditionOccurrence() |&gt;\n    mockDrugExposure() |&gt;\n    mockObservation() |&gt;\n    mockMeasurement() |&gt;\n    mockVisitOccurrence() |&gt;\n    mockProcedureOccurrence()\n\ndb &lt;- dbConnect(drv = duckdb())\n\ncdm &lt;- insertCdmTo(cdm = cdm_local,\n                   to = dbSource(con = db, writeSchema = \"main\"))\n\nNow that we have OMOP CDM data in a database, we can use the function cdmFromCon() from CDMConnector to create our cdm reference. Note that as well as specifying the schema containing our OMOP CDM tables, we will also specify a write schema where any database tables we create during our analysis will be stored. Often our OMOP CDM tables will be in a schema that we only have read-access to and we’ll have another schema where we can have write-access and where intermediate tables can be created for a given study.\n\ncdm &lt;- cdmFromCon(db, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cdmName = \"example_data\")\n\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of example_data ───────────────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence,\ncondition_occurrence, drug_exposure, procedure_occurrence, measurement,\nobservation, cdm_source, concept, vocabulary, concept_relationship,\nconcept_synonym, concept_ancestor, drug_strength\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -\n\n\n\n\n\n\n\n\nSetting a write prefix\n\n\n\n\n\nWe can also specify a write prefix and this will be used whenever permanent tables are created in the write schema. This can be useful when we’re sharing our write schema with others and want to avoid table name conflicts and easily drop tables created as part of a particular study.\n\ncdm &lt;- cdmFromCon(con = db,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  writePrefix = \"my_study_\",\n                  cdmName = \"example_data\")\n\n\n\n\nWe can see that we now have an object that contains references to all the OMOP CDM tables. We can reference specific tables using the “$” or “[[ … ]]” operators.\n\ncdm$person\n\n# Source:   table&lt;person&gt; [?? x 18]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         1              8532          1975              4            3\n 2         2              8532          1962              5            5\n 3         3              8532          1994              4            9\n 4         4              8532          1968              4           21\n 5         5              8507          1983             11            4\n 6         6              8532          1979              9            3\n 7         7              8532          1994              7           19\n 8         8              8507          1977              5            9\n 9         9              8507          1993              2           13\n10        10              8507          1961              2           10\n# ℹ more rows\n# ℹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ncdm[[\"observation_period\"]]\n\n# Source:   table&lt;observation_period&gt; [?? x 5]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   observation_period_id person_id observation_period_s…¹ observation_period_e…²\n                   &lt;int&gt;     &lt;int&gt; &lt;date&gt;                 &lt;date&gt;                \n 1                     1         1 1983-12-19             2018-03-30            \n 2                     2         2 1989-12-21             2016-11-30            \n 3                     3         3 2004-10-07             2009-06-21            \n 4                     4         4 1973-03-27             2009-08-10            \n 5                     5         5 1993-02-16             2012-10-09            \n 6                     6         6 1999-09-14             2017-07-27            \n 7                     7         7 2017-12-17             2019-06-26            \n 8                     8         8 2005-12-20             2018-03-03            \n 9                     9         9 2003-11-22             2008-11-02            \n10                    10        10 2014-03-21             2018-03-10            \n# ℹ more rows\n# ℹ abbreviated names: ¹​observation_period_start_date,\n#   ²​observation_period_end_date\n# ℹ 1 more variable: period_type_concept_id &lt;int&gt;\n\n\nNote that here we have first created a local version of the cdm with all the tables of interest with omock, then copied it to a duckdb database, and finally crated a reference to it with CDMConnector, so that we can work with the final cdm object as we normally would for one created with our own healthcare data. In that case we would directly use cdmFromCon with our own database information. Throughout this chapter, however, we will keep working with the mock dataset.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#cdm-attributes",
    "href": "cdm_reference.html#cdm-attributes",
    "title": "5  Creating a CDM reference",
    "section": "5.3 CDM attributes",
    "text": "5.3 CDM attributes\n\n5.3.1 CDM name\nOur cdm reference will be associated with a name. By default this name will be taken from the cdm_source_name field from the cdm_source table. We will use the function cdmName from omopgenerics to get it.\n\ncdm &lt;- cdmFromCon(db,\n  cdmSchema = \"main\", \n  writeSchema = \"main\")\ncdm$cdm_source\n\n# Source:   table&lt;cdm_source&gt; [?? x 10]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  cdm_source_name cdm_source_abbreviation cdm_holder source_description\n  &lt;chr&gt;           &lt;chr&gt;                   &lt;chr&gt;      &lt;chr&gt;             \n1 mock            &lt;NA&gt;                    &lt;NA&gt;       &lt;NA&gt;              \n# ℹ 6 more variables: source_documentation_reference &lt;chr&gt;,\n#   cdm_etl_reference &lt;chr&gt;, source_release_date &lt;date&gt;,\n#   cdm_release_date &lt;date&gt;, cdm_version &lt;chr&gt;, vocabulary_version &lt;chr&gt;\n\ncdmName(cdm)\n\n[1] \"mock\"\n\n\nHowever, we can instead set this name to whatever else we want when creating our cdm reference.\n\ncdm &lt;- cdmFromCon(db,\n  cdmSchema = \"main\", \n  writeSchema = \"main\", \n  cdmName = \"my_cdm\")\ncdmName(cdm)\n\n[1] \"my_cdm\"\n\n\nNote that we can also get our cdm name from any of the tables in our cdm reference.\n\ncdmName(cdm$person)\n\n[1] \"my_cdm\"\n\n\n\n\n\n\n\n\nBehind the scenes\n\n\n\n\n\nThe class of the cdm reference itself is cdm_reference.\n\nclass(cdm)\n\n[1] \"cdm_reference\"\n\nclass(cdm$person)\n\n[1] \"omop_table\"            \"cdm_table\"             \"tbl_duckdb_connection\"\n[4] \"tbl_dbi\"               \"tbl_sql\"               \"tbl_lazy\"             \n[7] \"tbl\"                  \n\n\nEach of the tables has class cdm_table. If the table is one of the standard OMOP CDM tables it will also have class omop_table. This latter class is defined so that we can allow different behaviour for these core tables (person, condition_occurrence, observation_period, etc.) compared to other tables that are added to the cdm reference during the course of running a study.\n\nclass(cdm$person)\n\n[1] \"omop_table\"            \"cdm_table\"             \"tbl_duckdb_connection\"\n[4] \"tbl_dbi\"               \"tbl_sql\"               \"tbl_lazy\"             \n[7] \"tbl\"                  \n\n\nWe can see that cdmName() is a generic function, which works for both the cdm reference as a whole and individual tables.\n\nlibrary(sloop)\ns3_dispatch(cdmName(cdm))\n\n=&gt; cdmName.cdm_reference\n * cdmName.default\n\ns3_dispatch(cdmName(cdm$person))\n\n   cdmName.omop_table\n=&gt; cdmName.cdm_table\n   cdmName.tbl_duckdb_connection\n   cdmName.tbl_dbi\n   cdmName.tbl_sql\n   cdmName.tbl_lazy\n   cdmName.tbl\n * cdmName.default\n\n\n\n\n\n\n\n5.3.2 CDM version\nWe can also easily check the OMOP CDM version that is being used with the function cdmVersion from omopgenerics like so:\n\ncdmVersion(cdm)\n\n[1] \"5.3\"",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#including-cohort-tables-in-the-cdm-reference",
    "href": "cdm_reference.html#including-cohort-tables-in-the-cdm-reference",
    "title": "5  Creating a CDM reference",
    "section": "5.4 Including cohort tables in the cdm reference",
    "text": "5.4 Including cohort tables in the cdm reference\nWe’ll be seeing how to create cohorts in more detail in ?sec-creating_cohorts. For the moment, let’s just outline how we can include a cohort in our cdm reference. For this we’ll use omock to add a cohort to our local cdm and upload that to a duckdb database again.\n\ncdm_local &lt;- cdm_local |&gt; \n  mockCohort(name = \"my_study_cohort\")\ndb &lt;- dbConnect(drv = duckdb())\ncdm &lt;- insertCdmTo(cdm = cdm_local,\n                   to = dbSource(con = db, writeSchema = \"main\"))\n\nNow we can specify we want to include this existing cohort table to our cdm object when creating our cdm reference.\n\ncdm &lt;- cdmFromCon(db, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cohortTables = \"my_study_cohort\",\n                  cdmName = \"example_data\")\ncdm\n\n\ncdm$my_study_cohort |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ subject_id           &lt;int&gt; 2, 3, 3, 4, 6, 6, 7, 7, 7, 11, 12, 12, 13, 13, 15…\n$ cohort_start_date    &lt;date&gt; 1995-07-23, 2005-02-13, 2006-01-20, 1974-11-27, …\n$ cohort_end_date      &lt;date&gt; 2002-07-25, 2006-01-19, 2007-01-14, 1988-05-18, …",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#including-achilles-tables-in-the-cdm-reference",
    "href": "cdm_reference.html#including-achilles-tables-in-the-cdm-reference",
    "title": "5  Creating a CDM reference",
    "section": "5.5 Including achilles tables in the cdm reference",
    "text": "5.5 Including achilles tables in the cdm reference\nIf we have the results tables from the Achilles R package in our database, we can also include these in our cdm reference.\nJust to show how this can be done let’s upload some empty results tables in the Achilles format.\n\ndbWriteTable(db, \n             \"achilles_analysis\",\n             tibble(\n               analysis_id = NA_integer_,\n               analysis_name = NA_character_,\n               stratum_1_name = NA_character_,\n               stratum_2_name = NA_character_,\n               stratum_3_name = NA_character_,\n               stratum_4_name = NA_character_,\n               stratum_5_name = NA_character_,\n               is_default = NA_character_,\n               category = NA_character_))\ndbWriteTable(db, \n             \"achilles_results\",\n             tibble(\n               analysis_id = NA_integer_,\n               stratum_1 = NA_character_,\n               stratum_2 = NA_character_,\n               stratum_3 = NA_character_,\n               stratum_4 = NA_character_,\n               stratum_5 = NA_character_,\n               count_value = NA_character_))\ndbWriteTable(db, \n             \"achilles_results_dist\",\n             tibble(\n               analysis_id = NA_integer_,\n               stratum_1 = NA_character_,\n               stratum_2 = NA_character_,\n               stratum_3 = NA_character_,\n               stratum_4 = NA_character_,\n               stratum_5 = NA_character_,\n               count_value = NA_character_,\n               min_value = NA_character_,\n               max_value = NA_character_,\n               avg_value = NA_character_,\n               stdev_value = NA_character_,\n               median_value = NA_character_,\n               p10_value = NA_character_,\n               p25_value = NA_character_,\n               p75_value = NA_character_,\n               p90_value = NA_character_))\n\nWe can now include these achilles table in our cdm reference as in the previous case.\n\ncdm &lt;- cdmFromCon(db, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cohortTables = \"my_study_cohort\",\n                  achillesSchema = \"main\",\n                  cdmName = \"example_data\")\ncdm",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#adding-other-tables-to-the-cdm-reference",
    "href": "cdm_reference.html#adding-other-tables-to-the-cdm-reference",
    "title": "5  Creating a CDM reference",
    "section": "5.6 Adding other tables to the cdm reference",
    "text": "5.6 Adding other tables to the cdm reference\nLet’s say we have some additional local data that we want to add to our cdm reference. We can add this both to the same source (in this case a database) and to our cdm reference using insertTable from omopgenerics. We will show this with the dataset cars in-built in R.\n\ncars |&gt; \n  glimpse()\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n\n\ncdm &lt;- insertTable(cdm = cdm, \n                   name = \"cars\", \n                   table = cars, \n                   temporary = FALSE)\n\nWe can see that now this extra table has been uploaded to the database behind our cdm reference and also added to our reference.\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of example_data ───────────────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence,\ncondition_occurrence, drug_exposure, procedure_occurrence, measurement,\nobservation, cdm_source, concept, vocabulary, concept_relationship,\nconcept_synonym, concept_ancestor, drug_strength\n\n\n• cohort tables: my_study_cohort\n\n\n• achilles tables: achilles_analysis, achilles_results, achilles_results_dist\n\n\n• other tables: cars\n\n\n\ncdm$cars\n\n# Source:   table&lt;cars&gt; [?? x 2]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n   speed  dist\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     4     2\n 2     4    10\n 3     7     4\n 4     7    22\n 5     8    16\n 6     9    10\n 7    10    18\n 8    10    26\n 9    10    34\n10    11    17\n# ℹ more rows\n\n\nIf we already had the table in the database we could have instead just assigned it to our existing cdm reference. To see this let’s upload the penguins table to our duckdb database.\n\ndbWriteTable(db, \n             \"penguins\",\n             penguins)\n\nOnce we have this table in the database, we can just assign it to our cdm reference.\n\ncdm$penguins &lt;- tbl(db, \"penguins\")\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of example_data ───────────────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence,\ncondition_occurrence, drug_exposure, procedure_occurrence, measurement,\nobservation, cdm_source, concept, vocabulary, concept_relationship,\nconcept_synonym, concept_ancestor, drug_strength\n\n\n• cohort tables: my_study_cohort\n\n\n• achilles tables: achilles_analysis, achilles_results, achilles_results_dist\n\n\n• other tables: cars, penguins",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#mutability-of-the-cdm-reference",
    "href": "cdm_reference.html#mutability-of-the-cdm-reference",
    "title": "5  Creating a CDM reference",
    "section": "5.7 Mutability of the cdm reference",
    "text": "5.7 Mutability of the cdm reference\nAn important characteristic of our cdm reference is that we can alter the tables in R, but the OMOP CDM data will not be affected. We will therefore only be transforming the data in our cdm object but the original datasets behind it will remain intact.\nFor example, let’s say we want to perform a study with only people born in 1970. For this we could filter our person table to only people born in this year.\n\ncdm$person &lt;- cdm$person |&gt; \n  filter(year_of_birth == 1970)\n\ncdm$person\n\n# Source:   SQL [?? x 18]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1        88              8532          1970             12           15\n2        99              8532          1970              1           18\n3       100              8507          1970              1            2\n# ℹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nFrom now on, when we work with our cdm reference this restriction will continue to have been applied.\n\ncdm$person |&gt; \n    tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n      n\n  &lt;dbl&gt;\n1     3\n\n\nThe original OMOP CDM data itself however will remain unaffected. We can see that, indeed, if we create our reference again the underlying data is unchanged.\n\ncdm &lt;- cdmFromCon(con = db,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  cdmName = \"Synthea Covid-19 data\")\ncdm$person |&gt; \n    tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n      n\n  &lt;dbl&gt;\n1   100\n\n\nThe mutability of our cdm reference is a useful feature for studies as it means we can easily tweak our OMOP CDM data if needed. Meanwhile, leaving the underlying data unchanged is essential so that other study code can run against the data, unaffected by any of our changes.\nOne thing we can’t do, though, is alter the structure of OMOP CDM tables. For example, the following code would cause an error as the person table must always have the column person_id.\n\ncdm$person &lt;- cdm$person |&gt; \n    rename(\"new_id\" = \"person_id\")\n\nError in `newOmopTable()`:\n! person_id is not present in table person\n\n\nIn such a case we would have to call the table something else first, and then run the previous code:\n\ncdm$person_new &lt;- cdm$person |&gt; \n    rename(\"new_id\" = \"person_id\") |&gt; \n    compute(name = \"person_new\", \n            temporary = TRUE)\n\nNow we would be allowed to have this new table as an additional table in our cdm reference, knowing it was not in the format of one of the core OMOP CDM tables.\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of Synthea Covid-19 data ──────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence,\ncondition_occurrence, drug_exposure, procedure_occurrence, measurement,\nobservation, cdm_source, concept, vocabulary, concept_relationship,\nconcept_synonym, concept_ancestor, drug_strength\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -\n\n\nThe package omopgenerics provides a comprehensive list of the required features of a valid cdm reference. You can read more about it here.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#working-with-temporary-and-permanent-tables",
    "href": "cdm_reference.html#working-with-temporary-and-permanent-tables",
    "title": "5  Creating a CDM reference",
    "section": "5.8 Working with temporary and permanent tables",
    "text": "5.8 Working with temporary and permanent tables\nWhen we create new tables and our cdm reference is in a database we have a choice between using temporary or permanent tables. In most cases we can work with these interchangeably. Below we create one temporary table and one permanent table. We can see that both of these tables have been added to our cdm reference and that we can use them in the same way. Note that any new computed table will by default be temporary unless otherwise specified.\n\ncdm$person_new_temp &lt;- cdm$person |&gt; \n  head(5) |&gt; \n  compute()\n\n\ncdm$person_new_permanent &lt;- cdm$person |&gt; \n  head(5) |&gt; \n  compute(name = \"person_new_permanent\", \n          temporary = FALSE)\n\n\ncdm\n\ncdm$person_new_temp\n\n# Source:   table&lt;og_001_1745931340&gt; [?? x 18]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         1              8532          1975              4            3\n2         2              8532          1962              5            5\n3         3              8532          1994              4            9\n4         4              8532          1968              4           21\n5         5              8507          1983             11            4\n# ℹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ncdm$person_new_permanent\n\n# Source:   table&lt;person_new_permanent&gt; [?? x 18]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         1              8532          1975              4            3\n2         2              8532          1962              5            5\n3         3              8532          1994              4            9\n4         4              8532          1968              4           21\n5         5              8507          1983             11            4\n# ℹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nOne benefit of working with temporary tables is that they will be automatically dropped at the end of the session, whereas the permanent tables will be left over in the database until explicitly dropped. This helps maintain the original database structure tidy and free of irrelevant data.\nHowever, one disadvantage of using temporary tables is that we will generally accumulate more and more of them as we go (in a single R session), whereas we can overwrite permanent tables continuously. For example, if our study code contains a loop that requires a compute, we would either overwrite an intermediate permanent table 100 times or create 100 different temporary tables in the process. In the latter case we should be wary of consuming a lot of RAM, which could lead to performance issues or even crashes.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html",
    "href": "exploring_the_cdm.html",
    "title": "6  Exploring the OMOP CDM",
    "section": "",
    "text": "6.1 Counting people\nFor this chapter, we’ll use a synthetic Covid-19 dataset.\nThe OMOP CDM is person-centric, with the person table containing records to uniquely identify each person in the database. As each row refers to a unique person, we can quickly get a count of the number of individuals in the database like so\ncdm$person |&gt; \n  count()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp4Fvzbw/file2d7527c54be4.duckdb]\n      n\n  &lt;dbl&gt;\n1 10754\nThe person table also contains some demographic information, including a gender concept for each person. We can get a count grouped by this variable, but as this uses a concept we’ll also need to join to the concept table to get the corresponding concept name for each concept id.\ncdm$person |&gt; \n  group_by(gender_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept, \n            by=c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n              select(\"gender_concept_id\", \"concept_name\", \"n\") |&gt; \n  collect()\n\n# A tibble: 2 × 3\n# Groups:   gender_concept_id [2]\n  gender_concept_id concept_name     n\n              &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1              8532 FEMALE        5165\n2              8507 MALE          5589",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#counting-people",
    "href": "exploring_the_cdm.html#counting-people",
    "title": "6  Exploring the OMOP CDM",
    "section": "",
    "text": "Vocabulary tables\n\n\n\n\n\nAbove we’ve got counts by specific concept IDs recorded in the condition occurrence table. What these IDs represent is described in the concept table. Here we have the name associated with the concept, along with other information such as it’s domain and vocabulary id.\n\ncdm$concept |&gt; \n  glimpse()\n\nRows: ??\nColumns: 10\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp4Fvzbw/file2d7527c54be4.duckdb]\n$ concept_id       &lt;int&gt; 45756805, 45756804, 45756803, 45756802, 45756801, 457…\n$ concept_name     &lt;chr&gt; \"Pediatric Cardiology\", \"Pediatric Anesthesiology\", \"…\n$ domain_id        &lt;chr&gt; \"Provider\", \"Provider\", \"Provider\", \"Provider\", \"Prov…\n$ vocabulary_id    &lt;chr&gt; \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS…\n$ concept_class_id &lt;chr&gt; \"Physician Specialty\", \"Physician Specialty\", \"Physic…\n$ standard_concept &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"…\n$ concept_code     &lt;chr&gt; \"OMOP4821938\", \"OMOP4821939\", \"OMOP4821940\", \"OMOP482…\n$ valid_start_date &lt;date&gt; 1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, 1970…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nOther vocabulary tables capture other information about concepts, such as the direct relationships between concepts (the concept relationship table) and hierarchical relationships between (the concept ancestor table).\n\ncdm$concept_relationship |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp4Fvzbw/file2d7527c54be4.duckdb]\n$ concept_id_1     &lt;int&gt; 35804314, 35804314, 35804314, 35804327, 35804327, 358…\n$ concept_id_2     &lt;int&gt; 912065, 42542145, 42542145, 35803584, 42542145, 42542…\n$ relationship_id  &lt;chr&gt; \"Has modality\", \"Has accepted use\", \"Is current in\", …\n$ valid_start_date &lt;date&gt; 2021-01-26, 2019-08-29, 2019-08-29, 2019-05-27, 2019…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\ncdm$concept_ancestor |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp4Fvzbw/file2d7527c54be4.duckdb]\n$ ancestor_concept_id      &lt;int&gt; 375415, 727760, 735979, 438112, 529411, 14196…\n$ descendant_concept_id    &lt;int&gt; 4335743, 2056453, 41070383, 36566114, 4326940…\n$ min_levels_of_separation &lt;int&gt; 4, 1, 3, 2, 3, 3, 4, 3, 2, 5, 1, 3, 4, 2, 2, …\n$ max_levels_of_separation &lt;int&gt; 4, 1, 5, 3, 3, 6, 12, 3, 2, 10, 1, 3, 4, 2, 2…\n\n\nMore information on the vocabulary tables (as well as other tables in the OMOP CDM version 5.3) can be found at https://ohdsi.github.io/CommonDataModel/cdm53.html#Vocabulary_Tables.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#summarising-observation-periods",
    "href": "exploring_the_cdm.html#summarising-observation-periods",
    "title": "6  Exploring the OMOP CDM",
    "section": "6.2 Summarising observation periods",
    "text": "6.2 Summarising observation periods\nThe observation period table contains records indicating spans of time over which clinical events can be reliably observed for the people in the person table. Someone can potentially have multiple observation periods. So say we wanted a count of people grouped by the year during which their first observation period started. We could do this like so:\n\nfirst_observation_period &lt;- cdm$observation_period |&gt;\n    group_by(person_id) |&gt; \n    filter(row_number() == 1) |&gt; \n    compute()\n\ncdm$person |&gt; \n  left_join(first_observation_period,\n            by = \"person_id\") |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date)) |&gt; \n  group_by(observation_period_start_year) |&gt; \n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(observation_period_start_year, n)) +\n  theme_bw()",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#summarising-clinical-records",
    "href": "exploring_the_cdm.html#summarising-clinical-records",
    "title": "6  Exploring the OMOP CDM",
    "section": "6.3 Summarising clinical records",
    "text": "6.3 Summarising clinical records\nWhat’s the number of condition occurrence records per person in the database? We can find this out like so\n\ncdm$person |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), 0,\n    condition_occurrence_records)) |&gt; \n  group_by(condition_occurrence_records) |&gt;\n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(condition_occurrence_records, n)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHow about we were interested in getting record counts for some specific concepts related to Covid-19 symptoms?\n\ncdm$condition_occurrence |&gt; \n  filter(condition_concept_id %in% c(437663,437390,31967,\n                                     4289517,4223659, 312437,\n                                     434490,254761,77074)) |&gt; \n  group_by(condition_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept,\n            by=c(\"condition_concept_id\" = \"concept_id\")) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(concept_name, n)) +\n  theme_bw()+\n  xlab(\"\")\n\n\n\n\n\n\n\n\nWe can also use summarise for various other calculations\n\ncdm$person |&gt; \n  summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q05_year_of_birth = quantile(year_of_birth, 0.05, na.rm=TRUE),\n            mean_year_of_birth = round(mean(year_of_birth, na.rm=TRUE),0),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q95_year_of_birth = quantile(year_of_birth, 0.95, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt;  \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp4Fvzbw/file2d7527c54be4.duckdb]\n$ min_year_of_birth    &lt;int&gt; 1923\n$ q05_year_of_birth    &lt;dbl&gt; 1927\n$ mean_year_of_birth   &lt;dbl&gt; 1971\n$ median_year_of_birth &lt;dbl&gt; 1970\n$ q95_year_of_birth    &lt;dbl&gt; 2018\n$ max_year_of_birth    &lt;int&gt; 2023\n\n\nAs we’ve seen before, we can also quickly get results for various groupings or restrictions\n\ngrouped_summary &lt;- cdm$person |&gt; \n   group_by(gender_concept_id) |&gt; \n   summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q25_year_of_birth = quantile(year_of_birth, 0.25, na.rm=TRUE),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q75_year_of_birth = quantile(year_of_birth, 0.75, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt; \n  left_join(cdm$concept, \n            by=c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n   collect() \n\ngrouped_summary |&gt; \n  ggplot(aes(x = concept_name, group = concept_name,\n             fill = concept_name)) +\n  geom_boxplot(aes(\n    lower = q25_year_of_birth, \n    upper = q75_year_of_birth, \n    middle = median_year_of_birth, \n    ymin = min_year_of_birth, \n    ymax = max_year_of_birth),\n    stat = \"identity\", width = 0.5) + \n  theme_bw()+ \n  theme(legend.position = \"none\") +\n  xlab(\"\")\n\n\n\n\n\n\n\n\nWhat’s the number of condition occurrence records per person in the database? We can find this out like so\n\ncdm$person |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), 0,\n    condition_occurrence_records)) |&gt; \n  group_by(condition_occurrence_records) |&gt;\n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(condition_occurrence_records, n)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHow about we were interested in getting record counts for some specific concepts related to Covid-19 symptoms?\n\ncdm$condition_occurrence |&gt; \n  filter(condition_concept_id %in% c(437663,437390,31967,\n                                     4289517,4223659, 312437,\n                                     434490,254761,77074)) |&gt; \n  group_by(condition_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept,\n            by=c(\"condition_concept_id\" = \"concept_id\")) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(concept_name, n)) +\n  theme_bw()+\n  xlab(\"\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "adding_features.html",
    "href": "adding_features.html",
    "title": "7  Identifying patient characteristics",
    "section": "",
    "text": "7.1 Adding specific demographics\nFor this chapter, we’ll again use our example COVID-19 dataset.\nAs part of an analysis we almost always have a need to identify certain characteristics related to the individuals in our data. These characteristics might be time-invariant (ie a characteristic that does not change as time passes and a person ages) or time-varying.1\nThe PatientProfiles package makes it easy for us to add demographic information to tables in the OMOP CDM. Like the CDMConnector package we’ve seen previously, the fact that the structure of the OMOP CDM is known allows the PatientProfiles package to abstract away some common data manipulations required to do research with patient-level data.2\nLet’s say we are interested in individuals’ age and sex at time of diagnosis with COVID-19. We can add these variables to the table like so (noting that because age is time-varying, we have to specify the variable with the date for which we want to calculate age relative to).\ncdm$condition_occurrence &lt;- cdm$condition_occurrence |&gt; \n  addSex() |&gt; \n  addAge(indexDate = \"condition_start_date\")\n\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,…\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"…\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, …\ncdm$condition_occurrence |&gt; \n  addSexQuery() |&gt; \n  show_query()\n\nWarning: ! The following columns will be overwritten: sex\n\n\n&lt;SQL&gt;\nSELECT\n  condition_occurrence_id,\n  og_002_1745931361.person_id AS person_id,\n  condition_concept_id,\n  condition_start_date,\n  condition_start_datetime,\n  condition_end_date,\n  condition_end_datetime,\n  condition_type_concept_id,\n  condition_status_concept_id,\n  stop_reason,\n  provider_id,\n  visit_occurrence_id,\n  visit_detail_id,\n  condition_source_value,\n  condition_source_concept_id,\n  condition_status_source_value,\n  age,\n  RHS.sex AS sex\nFROM og_002_1745931361\nLEFT JOIN (\n  SELECT\n    person_id,\n    CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n  FROM person\n) RHS\n  ON (og_002_1745931361.person_id = RHS.person_id)\nWe now have two variables added containing values for age and sex.\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,…\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"…\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, …\nAnd with these now added it is straightforward to calculate mean age at condition start date by sex or even plot the distribution of age at diagnosis by sex.\ncdm$condition_occurrence |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE), .by = \"sex\") |&gt; \n  collect()\n\n# A tibble: 2 × 2\n  sex    mean_age\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Female     50.8\n2 Male       56.5\ncdm$condition_occurrence |&gt;\n  select(\"person_id\", \"age\", \"sex\") |&gt; \n  collect()  |&gt;\n  ggplot(aes(fill = sex)) +\n  facet_grid(sex ~ .) +\n  geom_histogram(aes(age), colour = \"black\", binwidth = 5) +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#adding-multiple-demographics-simultaneously",
    "href": "adding_features.html#adding-multiple-demographics-simultaneously",
    "title": "7  Identifying patient characteristics",
    "section": "7.2 Adding multiple demographics simultaneously",
    "text": "7.2 Adding multiple demographics simultaneously\nWe’ve now seen individual functions from PatientProfiles to add age and sex, and the package has others to add other characteristics like days of prior observation in the database (rather unimaginatively named addPriorObservation()). In additional to these individuals functions, the package also provides a more general function to get all of these characteristics at the same time.3\n\ncdm$drug_exposure &lt;- cdm$drug_exposure |&gt; \n  addDemographics(indexDate = \"drug_exposure_start_date\")\n\ncdm$drug_exposure |&gt; \n  glimpse()\n\nRows: ??\nColumns: 27\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ drug_exposure_id             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n$ person_id                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ drug_concept_id              &lt;int&gt; 40213260, 40213260, 40213260, 40213260, 4…\n$ drug_exposure_start_date     &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020…\n$ drug_exposure_start_datetime &lt;dttm&gt; 2021-04-30 16:49:39, 2020-04-24 16:49:39…\n$ drug_exposure_end_date       &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020…\n$ drug_exposure_end_datetime   &lt;dttm&gt; 2021-04-30 16:49:39, 2020-04-24 16:49:39…\n$ verbatim_end_date            &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020…\n$ drug_type_concept_id         &lt;int&gt; 32869, 32869, 32869, 32869, 32869, 32869,…\n$ stop_reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ refills                      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quantity                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ days_supply                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sig                          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ route_concept_id             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lot_number                   &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ provider_id                  &lt;int&gt; 12357, 12357, 12356, 12356, 12357, 12356,…\n$ visit_occurrence_id          &lt;int&gt; 6, 8, 6, 8, 6, 6, 7, 2, 6, 8, 9, 1, 7, 2,…\n$ visit_detail_id              &lt;int&gt; 1000006, 1000008, 1000006, 1000008, 10000…\n$ drug_source_value            &lt;chr&gt; \"121\", \"121\", \"121\", \"121\", \"113\", \"113\",…\n$ drug_source_concept_id       &lt;int&gt; 40213260, 40213260, 40213260, 40213260, 4…\n$ route_source_value           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dose_unit_source_value       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age                          &lt;int&gt; 51, 50, 51, 50, 51, 51, 53, 52, 51, 50, 4…\n$ sex                          &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"…\n$ prior_observation            &lt;int&gt; 2548, 2177, 2548, 2177, 2548, 2548, 3290,…\n$ future_observation           &lt;int&gt; 742, 1113, 742, 1113, 742, 742, 0, 371, 7…\n\n\nWith these characteristics now all added, we can now calculate mean age, prior observation (how many days have passed since the individual’s most recent observation start date), and future observation (how many days until the individual’s nearest observation end date) at drug exposure start date by sex.\n\ncdm$drug_exposure |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE),\n            mean_prior_observation = mean(prior_observation, na.rm=TRUE),\n            mean_future_observation = mean(future_observation, na.rm=TRUE),\n            .by = \"sex\") |&gt; \n  collect()\n\n# A tibble: 2 × 4\n  sex    mean_age mean_prior_observation mean_future_observation\n  &lt;chr&gt;     &lt;dbl&gt;                  &lt;dbl&gt;                   &lt;dbl&gt;\n1 Male       43.0                  2455.                   1768.\n2 Female     39.4                  2096.                   1661.\n\n\n\n\n\n\n\n\nReturning a query from PatientProfiles rather than the result\n\n\n\n\n\nIn the above examples the functions from PatientProfiles will execute queries with the results written to a table in the database (either temporary if no name is provided or a permanent table if one is given). We might though instead want to to instead just get the underlying query back so that we have more control over how and when the query will be executed.\n\ncdm$visit_occurrence |&gt; \n  addSex() |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT og_004_1745931362.*\nFROM og_004_1745931362\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSex(name = \"my_new_table\") |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT my_new_table.*\nFROM my_new_table\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSexQuery() |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT q01.*\nFROM (\n  SELECT visit_occurrence.*, sex\n  FROM visit_occurrence\n  LEFT JOIN (\n    SELECT\n      person_id,\n      CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n    FROM person\n  ) RHS\n    ON (visit_occurrence.person_id = RHS.person_id)\n) q01\nWHERE (sex = 'Male')",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#creating-categories",
    "href": "adding_features.html#creating-categories",
    "title": "7  Identifying patient characteristics",
    "section": "7.3 Creating categories",
    "text": "7.3 Creating categories\nWhen we add age, either via addAge or addDemographics, we can also add another variable containing age groups. These age groups are specified in a list of vectors, each of which contain the lower and upper bounds.\n\ncdm$visit_occurrence &lt;- cdm$visit_occurrence |&gt;\n  addAge(indexDate = \"visit_start_date\",\n    ageGroup = list(c(0,17), c(18, 64),\n                    c(65, Inf)))\n\ncdm$visit_occurrence |&gt; \n  # data quality issues with our synthetic data means we have \n  # some negative ages so will drop these\n  filter(age &gt;= 0) |&gt; \n  group_by(age_group) |&gt; \n  tally() |&gt; \n  collect() |&gt; \n  ggplot() + \n  geom_col(aes(x = age_group, y = n)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nPatientProfiles also provides a more general function for adding categories. Can you guess it’s name? That’s right, we have addCategories() for this.\n\ncdm$condition_occurrence |&gt;\n  addPriorObservation(indexDate = \"condition_start_date\") |&gt;\n  addCategories(\n    variable = \"prior_observation\",\n    categories = list(\"prior_observation_group\" = list(\n      c(0, 364), c(365, Inf)  \n    ))\n  ) |&gt; \n  glimpse()\n\nRows: ??\nColumns: 20\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,…\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663…\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"…\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, …\n$ prior_observation             &lt;int&gt; 3437, 2898, 2842, 872, 872, 872, 872, 23…\n$ prior_observation_group       &lt;chr&gt; \"365 or above\", \"365 or above\", \"365 or …",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#adding-custom-variables",
    "href": "adding_features.html#adding-custom-variables",
    "title": "7  Identifying patient characteristics",
    "section": "7.4 Adding custom variables",
    "text": "7.4 Adding custom variables\nWhile PatientProfiles provides a range of functions that can help add characteristics of interest, you may also want to add other features . Obviously we can’t cover here all possible custom characteristics you may wish to add. However, two common groups of custom features are those that are derived from other variables in the same table and others that are taken from other tables and joined to our particular table of interest.\nIn the first case where we want to add a new variable derived from other variables in our table we’ll typically be using mutate() (from dplyr package). For example, perhaps we just want to add a new variable to our observation period table containing the year of individuals’ observation period start date. This is rather straightforward.\n\ncdm$observation_period &lt;- cdm$observation_period |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date))\n\ncdm$observation_period |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ observation_period_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ person_id                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ observation_period_start_date &lt;date&gt; 2014-05-09, 1977-04-11, 2014-04-19, 201…\n$ observation_period_end_date   &lt;date&gt; 2023-05-12, 1986-09-15, 2023-04-22, 202…\n$ period_type_concept_id        &lt;int&gt; 44814724, 44814724, 44814724, 44814724, …\n$ observation_period_start_year &lt;dbl&gt; 2014, 1977, 2014, 2014, 2013, 2013, 2013…\n\n\nThe second case is normally a more complex task where adding a new variable involves joining to some other table. This table may well have been created by some intermediate query that we wrote to derive the variable of interest. For example, lets say we want to add each number of condition occurrence records for each individual to the person table (remember that we saw how to calculate this in the previous chapter). For this we will need to do a join between the person and condition occurrence tables (as some people might not have any records in the condition occurrence table). Here we’ll save the create a table containing just the information we’re interested in and compute to a temporary table.\n\ncondition_summary &lt;- cdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), \n    0, condition_occurrence_records)) |&gt; \n  compute()\n\ncondition_summary |&gt; \n  glimpse()\n\nRows: ??\nColumns: 2\nDatabase: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n$ person_id                    &lt;int&gt; 2, 7, 16, 18, 25, 36, 42, 44, 47, 51, 52,…\n$ condition_occurrence_records &lt;dbl&gt; 1, 1, 2, 2, 1, 4, 3, 2, 5, 1, 3, 2, 1, 4,…\n\n\nWe can see what goes on behind the scenes by viewing the associated SQL.\n\ncdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), \n    0, condition_occurrence_records)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  person_id,\n  CASE WHEN ((condition_occurrence_records IS NULL)) THEN 0.0 WHEN NOT ((condition_occurrence_records IS NULL)) THEN condition_occurrence_records END AS condition_occurrence_records\nFROM (\n  SELECT person.person_id AS person_id, condition_occurrence_records\n  FROM person\n  LEFT JOIN (\n    SELECT person_id, COUNT(*) AS condition_occurrence_records\n    FROM og_002_1745931361\n    GROUP BY person_id\n  ) RHS\n    ON (person.person_id = RHS.person_id)\n) q01\n\n\n\n\n\n\n\n\nTaking care with joins\n\n\n\n\n\nWhen adding variables through joins we need to pay particular attention to the dimensions of the resulting table. While sometimes we may want to have additional rows added as well as new columns, this is often not desired. If we, for example, have a table with one row per person then a left join to a table with multiple rows per person can then result in a table with multiple rows per person.\nExamples where to be careful include when joining to the observation period table, as individuals can have multiple observation periods, and when working with cohorts (which are the focus of the next chapter) as individuals can also enter the same study cohort multiple times.\nJust to underline how problematic joins can become if we don’t take care, here we join the condition occurrence table and the drug exposure table both of which have multiple records per person. Remember this is just with our small synthetic data, so when working with real patient data which is oftentimes much, much larger this would be extremely problematic (and would unlikely be needed to answer any research question). In other words, don’t try this at home!\n\ncdm$condition_occurrence |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n      n\n  &lt;dbl&gt;\n1  9967\n\ncdm$drug_exposure |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n       n\n   &lt;dbl&gt;\n1 337509\n\ncdm$condition_occurrence |&gt; \n  select(person_id, condition_start_date) |&gt; \n  left_join(cdm$drug_exposure |&gt; \n  select(person_id, drug_exposure_start_date), \n  by = \"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmpzl1dEn/file2db87515ea92.duckdb]\n       n\n   &lt;dbl&gt;\n1 410683",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#footnotes",
    "href": "adding_features.html#footnotes",
    "title": "7  Identifying patient characteristics",
    "section": "",
    "text": "In some datasets characteristics that could conceptually be considered as time-varying are encoded as time-invariant. One example for the latter is that in some cases an individual may be associated with a particular socioeconomic status or nationality that for the purposes of the data is treated as time-invariant.↩︎\nAlthough these manipulations can on the face of it seem quite simple, their implementation across different database platforms with different data granularity (for example whether day of birth has been filled in for all patients or not) presents challenges that the PatientProfiles package solves for us.↩︎\nThis function also provides a more time efficient method that getting the characteristics one by one. This is because these characteristics are all derived from the OMOP CDM person and observation period tables and so can be identified simultaneously.↩︎",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html",
    "href": "creating_cohorts.html",
    "title": "8  Adding cohorts to the CDM",
    "section": "",
    "text": "8.1 What is a cohort?\nWhen performing research with the OMOP common data model we often want to identify groups of individuals who share some set of characteristics. The criteria for including individuals can range from the seemingly simple (e.g. people diagnosed with asthma) to the much more complicated (e.g. adults diagnosed with asthma who had a year of prior observation time in the database prior to their diagnosis, had no prior history of chronic obstructive pulmonary disease, and no history of use of short-acting beta-antagonists).\nThe set of people we identify are cohorts, and the OMOP CDM has a specific structure by which they can be represented, with a cohort table having four required fields: 1) cohort definition id (a unique identifier for each cohort), 2) subject id (a foreign key to the subject in the cohort - typically referring to records in the person table), 3) cohort start date, and 4) cohort end date. Individuals can enter a cohort multiple times, but the time periods in which they are in the cohort cannot overlap. Individuals will only be considered in a cohort when they have have an ongoing observation period.\nIt is beyond the scope of this book to describe all the different ways cohorts could be created, however in this chapter we provide a summary of some of the key building blocks for cohort creation. Cohort-building pipelines can be created following these principles to create a wide range of study cohorts.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#set-up",
    "href": "creating_cohorts.html#set-up",
    "title": "8  Adding cohorts to the CDM",
    "section": "8.2 Set up",
    "text": "8.2 Set up\nWe’ll use our synthetic dataset for demonstrating how cohorts can be constructed.\n\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(CDMConnector)\nlibrary(CodelistGenerator)\nlibrary(CohortConstructor)\nlibrary(CohortCharacteristics)\nlibrary(dplyr)\n        \ndb &lt;- dbConnect(drv = duckdb(),\n                dbdir = eunomiaDir(datasetName = \"synthea-covid19-10k\"))\ncdm &lt;- cdmFromCon(db, cdmSchema = \"main\", writeSchema = \"main\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#general-concept-based-cohort",
    "href": "creating_cohorts.html#general-concept-based-cohort",
    "title": "8  Adding cohorts to the CDM",
    "section": "8.3 General concept based cohort",
    "text": "8.3 General concept based cohort\nOften study cohorts will be based around a specific clinical event identified by some set of clinical codes. Here, for example, we use the CohortConstructor package to create a cohort of people with Covid-19. For this we are identifying any clinical records with the code 37311061.\n\ncdm$covid &lt;- conceptCohort(cdm = cdm, \n                           conceptSet = list(\"covid\" = 37311061), \n                           name = \"covid\")\ncdm$covid\n\n# Source:   table&lt;covid&gt; [?? x 4]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp2y0FNI/file2dfa18269138.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1        295 2020-04-23        2020-05-21     \n 2                    1       2518 2020-10-29        2020-11-21     \n 3                    1       2807 2020-11-26        2020-12-27     \n 4                    1       6133 2021-02-08        2021-03-15     \n 5                    1       6734 2020-12-04        2020-12-15     \n 6                    1       3600 2021-01-16        2021-02-15     \n 7                    1       6785 2020-11-21        2020-12-09     \n 8                    1       9036 2021-08-16        2021-08-27     \n 9                    1       2129 2021-01-07        2021-02-03     \n10                    1       7416 2020-04-28        2020-05-26     \n# ℹ more rows\n\n\n\n\n\n\n\n\nFinding appropriate codes\n\n\n\n\n\nIn the defining the cohorts above we have needed to provide concept IDs to define our cohort. But, where do these come from?\nWe can search for codes of interest using the CodelistGenerator package. This can be done using a text search with the function CodelistGenerator::getCandidateCodes(). For example, we can have found the code we used above (and many others) like so:\n\ngetCandidateCodes(cdm = cdm, \n                  keywords = c(\"coronavirus\",\"covid\"),\n                  domains = \"condition\",\n                  includeDescendants = TRUE)\n\nLimiting to domains of interest\nGetting concepts to include\nAdding descendants\nSearch completed. Finishing up.\n✔ 37 candidate concepts identified\n\nTime taken: 0 minutes and 1 seconds\n\n\n# A tibble: 37 × 6\n   concept_id found_from   concept_name domain_id vocabulary_id standard_concept\n        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;           \n 1   40479642 From initia… Pneumonia d… Condition SNOMED        S               \n 2     439676 From initia… Coronavirus… Condition SNOMED        S               \n 3    3655973 From initia… At increase… Condition SNOMED        S               \n 4    3661406 From initia… Acute respi… Condition SNOMED        S               \n 5     756039 From initia… Respiratory… Condition OMOP Extensi… S               \n 6     756031 From initia… Bronchitis … Condition OMOP Extensi… S               \n 7    3661632 From initia… Thrombocyto… Condition SNOMED        S               \n 8    3661885 From initia… Fever cause… Condition SNOMED        S               \n 9   37310286 From initia… Infection o… Condition SNOMED        S               \n10     703447 From initia… High risk c… Condition SNOMED        S               \n# ℹ 27 more rows\n\n\nWe can also do automated searches that make use of the hierarchies in the vocabularies. Here, for example, we find the code for the drug ingredient Acetaminophen and all of it’s descendants.\n\ngetDrugIngredientCodes(cdm = cdm, name = \"acetaminophen\")\n\n\n\n\n── 1 codelist ──────────────────────────────────────────────────────────────────\n\n\n\n- 161_acetaminophen (25747 codes)\n\n\nNote that in practice clinical expertise is vital in the identification of appropriate codes so as to decide which the codes are in line with the clinical idea at hand.\n\n\n\nWe can see that as well as having the cohort entries above, our cohort table is associated with several attributes.\nFirst, we can see the settings associated with cohort.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 4\n$ cohort_definition_id &lt;int&gt; 1\n$ cohort_name          &lt;chr&gt; \"covid\"\n$ cdm_version          &lt;chr&gt; \"5.3\"\n$ vocabulary_version   &lt;chr&gt; \"v5.0 22-JUN-22\"\n\n\nSecond, we can get counts of the cohort.\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 964\n$ number_subjects      &lt;int&gt; 964\n\n\nAnd last we can see attrition related to the cohort.\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 4\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964\n$ reason_id            &lt;int&gt; 1, 2, 3, 4\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record start &lt;= rec…\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0\n\n\nAs we will see below these attributes of the cohorts become particularly useful as we apply further restrictions on our cohort.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#applying-inclusion-criteria",
    "href": "creating_cohorts.html#applying-inclusion-criteria",
    "title": "8  Adding cohorts to the CDM",
    "section": "8.4 Applying inclusion criteria",
    "text": "8.4 Applying inclusion criteria\n\n8.4.1 Only include first cohort entry per person\nLet’s say we first want to restrict to first entry.\n\ncdm$covid &lt;- cdm$covid |&gt; \n     requireIsFirstEntry() \n\n\n\n8.4.2 Restrict to study period\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireInDateRange(dateRange = c(as.Date(\"2020-09-01\"), NA))\n\n\n\n8.4.3 Applying demographic inclusion criteria\nSay for our study we want to include people with a GI bleed who were aged 40 or over at the time. We can use the add variables with these characteristics as seen in chapter 4 and then filter accordingly. The function CDMConnector::record_cohort_attrition() will then update our cohort attributes as we can see below.\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireDemographics(ageRange = c(18, 64), sex = \"Male\")\n\n\n\n8.4.4 Applying cohort-based inclusion criteria\nAs well as requirements about specific demographics, we may also want to use another cohort for inclusion criteria. Let’s say we want to exclude anyone with a history of cardiac conditions before their Covid-19 cohort entry.\nWe can first generate this new cohort table with records of cardiac conditions.\n\ncdm$cardiac &lt;- conceptCohort(\n  cdm = cdm,\n  list(\"myocaridal_infarction\" = c(\n    317576, 313217, 321042, 4329847\n  )), \nname = \"cardiac\"\n)\ncdm$cardiac\n\n# Source:   table&lt;cardiac&gt; [?? x 4]\n# Database: DuckDB v1.2.1 [unknown@Linux 6.11.0-1012-azure:R 4.5.0//tmp/Rtmp2y0FNI/file2dfa18269138.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1       1822 2005-09-28        2005-09-28     \n 2                    1       3492 2008-09-01        2008-09-01     \n 3                    1       7004 1998-07-02        1998-07-02     \n 4                    1        567 2022-04-04        2022-04-04     \n 5                    1       1063 2013-04-21        2013-04-21     \n 6                    1       1370 1952-05-06        1952-05-06     \n 7                    1       2173 2011-07-23        2011-07-23     \n 8                    1       2567 2001-11-03        2001-11-03     \n 9                    1       4304 2017-04-29        2017-04-29     \n10                    1       5369 1971-06-05        1971-06-05     \n# ℹ more rows\n\n\nAnd now we can apply the inclusion criteria that individuals have zero intersections with the table in the time prior to their Covid-19 cohort entry.\n\ncdm$covid &lt;- cdm$covid |&gt; \n  requireCohortIntersect(targetCohortTable = \"cardiac\", \n                         indexDate = \"cohort_start_date\", \n                         window = c(-Inf, -1), \n                         intersections = 0) \n\nNote if we had wanted to have required that individuals did have a history of a cardiac condition we would instead have set intersections = c(1, Inf) above.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#cohort-attributes",
    "href": "creating_cohorts.html#cohort-attributes",
    "title": "8  Adding cohorts to the CDM",
    "section": "8.5 Cohort attributes",
    "text": "8.5 Cohort attributes\nWe can see that the attributes of the cohort were updated as we applied the inclusion criteria.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 8\n$ cohort_definition_id   &lt;int&gt; 1\n$ cohort_name            &lt;chr&gt; \"covid\"\n$ age_range              &lt;chr&gt; \"18_64\"\n$ sex                    &lt;chr&gt; \"Male\"\n$ min_prior_observation  &lt;dbl&gt; 0\n$ min_future_observation &lt;dbl&gt; 0\n$ cdm_version            &lt;chr&gt; \"5.3\"\n$ vocabulary_version     &lt;chr&gt; \"v5.0 22-JUN-22\"\n\n\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 158\n$ number_subjects      &lt;int&gt; 158\n\n\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 11\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964, 964, 793, 363, 171, 171, 171,…\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964, 964, 793, 363, 171, 171, 171,…\n$ reason_id            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record start &lt;= rec…\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0, 0, 171, 430, 192, 0, 0, 13\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0, 0, 171, 430, 192, 0, 0, 13\n\n\nFor attrition, we can use CohortConstructor::summariseCohortAttrition() and then CohortConstructor::tableCohortAttrition() to better view the impact of applying the additional inclusion criteria.\n\nattrition_summary &lt;- summariseCohortAttrition(cdm$covid)\nplotCohortAttrition(attrition_summary, type = 'png')",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html",
    "href": "working_with_cohorts.html",
    "title": "9  Working with cohorts",
    "section": "",
    "text": "9.1 Cohort intersections\nPatientProfiles::addCohortIntersect()",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#intersection-between-two-cohorts",
    "href": "working_with_cohorts.html#intersection-between-two-cohorts",
    "title": "9  Working with cohorts",
    "section": "9.2 Intersection between two cohorts",
    "text": "9.2 Intersection between two cohorts",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#set-up",
    "href": "working_with_cohorts.html#set-up",
    "title": "9  Working with cohorts",
    "section": "9.3 Set up",
    "text": "9.3 Set up\n\nlibrary(CDMConnector)\nlibrary(dplyr)\nlibrary(PatientProfiles)\n\n# For this example we will use GiBleed data set\ndownloadEunomiaData(datasetName = \"GiBleed\")        \ndb &lt;- DBI::dbConnect(duckdb::duckdb(), eunomiaDir())\n\ncdm &lt;- cdmFromCon(db, cdmSchema = \"main\", writeSchema = \"main\")\n\n# cdm &lt;- cdm |&gt; \n#   generate_concept_cohort_set(concept_set = list(\"gi_bleed\" = 192671), \n#                             limit = \"all\", \n#                             end = 30,\n#                             name = \"gi_bleed\",\n#                             overwrite = TRUE) |&gt; \n#   generate_concept_cohort_set(concept_set = list(\"acetaminophen\" = c(1125315,\n#                                                               1127078,\n#                                                               1127433,\n#                                                               40229134,\n#                                                               40231925,\n#                                                               40162522,\n#                                                               19133768)), \n#                               limit = \"all\", \n#                             # end = \"event_end_date\",\n#                             name = \"acetaminophen\",\n#                             overwrite = TRUE)\n\n\n9.3.1 Flag\n\n# cdm$gi_bleed &lt;- cdm$gi_bleed |&gt; \n#   addCohortIntersectFlag(targetCohortTable = \"acetaminophen\",\n#                          window = list(c(-Inf, -1), c(0,0), c(1, Inf)))\n# \n# cdm$gi_bleed |&gt; \n#   summarise(acetaminophen_prior = sum(acetaminophen_minf_to_m1), \n#             acetaminophen_index = sum(acetaminophen_0_to_0),\n#             acetaminophen_post = sum(acetaminophen_1_to_inf)) |&gt; \n#   collect()\n\n\n\n9.3.2 Count\n\n\n9.3.3 Date and times",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "href": "working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "title": "9  Working with cohorts",
    "section": "9.4 Intersection between a cohort and tables with patient data",
    "text": "9.4 Intersection between a cohort and tables with patient data",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  }
]