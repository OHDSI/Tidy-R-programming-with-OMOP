[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy R programming with the OMOP common data model",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-this-book-for-me",
    "href": "index.html#is-this-book-for-me",
    "title": "Tidy R programming with the OMOP common data model",
    "section": "Is this book for me?",
    "text": "Is this book for me?\nWe’ve written this book for anyone interested in a working with databases mapped to the OMOP Common Data Model (CDM) in a tidyverse inspired approach. That is, human centered, consistent, composable, and inclusive (see https://design.tidyverse.org/unifying.html for more details on these principles).\nNew to the OMOP CDM? We’d recommend you pare this book with The Book of OHDSI\nNew to R? We recommend you compliment the book with R for data science",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Tidy R programming with the OMOP common data model",
    "section": "Citation",
    "text": "Citation\nTO ADD",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidy R programming with the OMOP common data model",
    "section": "License",
    "text": "License\n This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Tidy R programming with the OMOP common data model",
    "section": "Code",
    "text": "Code\nThe source code for the book can be found at this Github repository",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html",
    "href": "working_with_databases_from_r.html",
    "title": "1  A first analysis using data in a database",
    "section": "",
    "text": "1.1 Getting set up\nArtwork by @allison_horst\nBefore we start thinking about working with health care data spread across a database using the OMOP common data model, let’s first do a quick data analysis from R using a simpler dataset held in a database to quickly understand the general approach. For this we’ll use data from palmerpenguins package, which contains data on penguins collected from the Palmer Station in Antarctica.\nAssuming that you have R and RStudio already set up, first we need to install a few packages not included in base R if we don´t already have them.\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"DBI\")\ninstall.packages(\"duckdb\")\ninstall.packages(\"palmerpenguins\")\nOnce installed, we can load them like so.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#taking-a-peek-at-the-data",
    "href": "working_with_databases_from_r.html#taking-a-peek-at-the-data",
    "title": "1  A first analysis using data in a database",
    "section": "1.2 Taking a peek at the data",
    "text": "1.2 Taking a peek at the data\nWe can get an overview of the data using the glimpse() command.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nOr we could take a look at the first rows of the data using head()\n\nhead(penguins, 5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#inserting-data-into-a-database",
    "href": "working_with_databases_from_r.html#inserting-data-into-a-database",
    "title": "1  A first analysis using data in a database",
    "section": "1.3 Inserting data into a database",
    "text": "1.3 Inserting data into a database\nLet’s put our penguins data into a duckdb database. We create the database, add the penguins data, and then create a reference to the table containing the data.\n\ndb &lt;- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\ndbWriteTable(db, \"penguins\", penguins)\n\nWe can see that our database now has one table\n\nDBI::dbListTables(db)\n\n[1] \"penguins\"\n\n\nAnd now that the data is in a database we could use SQL to get the first rows that we saw before\n\ndbGetQuery(db, \"SELECT * FROM penguins LIMIT 5\")\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n\n\n\n\n\n\n\n\nConnecting to databases from R\n\n\n\n\n\nDatabase connections from R can be made using the DBI package. The back-end for DBI is facilitated by database specific driver packages. Above we we created a new, empty, in-process duckdb database which we then added database. But we could have instead connected to an existing duckdb database. This could, for example, look like\n\ndb &lt;- dbConnect(duckdb::duckdb(), \n              dbdir = here(\"my_duckdb_database.ducdkb\"))\n\nIn this book for simplicity we will be working with in-process duckdb databases with synthetic data. However, when analysing real patient data we will be more often working with client-server databases, where we are connecting from our computer to a central server with the database or working with data held in the cloud. The approaches shown throughout this book will work in the same way for these other types of database management systems, but the way to connect to the database will be different (although still using DBI). In general, creating connections are supported by associated back-end packages. For example a connection to a Postgres database would use the RPostgres R package and look something like:\n\ndb &lt;- DBI::dbConnect(RPostgres::Postgres(),\n                      dbname = Sys.getenv(\"CDM5_POSTGRESQL_DBNAME\"),\n                      host = Sys.getenv(\"CDM5_POSTGRESQL_HOST\"),\n                      user = Sys.getenv(\"CDM5_POSTGRESQL_USER\"),\n                      password = Sys.getenv(\"CDM5_POSTGRESQL_PASSWORD\"))",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#translation-from-r-to-sql",
    "href": "working_with_databases_from_r.html#translation-from-r-to-sql",
    "title": "1  A first analysis using data in a database",
    "section": "1.4 Translation from R to SQL",
    "text": "1.4 Translation from R to SQL\nInstead of using SQL, we could instead use the same R code as before. Now it will query the data held in a database. To do this, first we create a reference to the table in the database.\n\npenguins_db &lt;- tbl(db, \"penguins\")\npenguins_db\n\n# Source:   table&lt;penguins&gt; [?? x 8]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOnce we have this reference, we can then use it with familiar looking R code.\n\nhead(penguins_db, 5)\n\n# Source:   SQL [?? x 8]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe magic here is provided by the dbplyr which takes the R code and converts it into SQL, which in this case looks like the SQL we could have written instead.\n\nhead(penguins_db, 5) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT penguins.*\nFROM penguins\nLIMIT 5\n\n\nMore complicated SQL can also be generated by using familiar dplyr code. For example, we could get a summary of bill length by species like so\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    min_bill_length_mm = min(bill_length_mm),\n    median_bill_length_mm = median(bill_length_mm),\n    max_bill_length_mm = max(bill_length_mm)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(\n    min_bill_length_mm,\n    \" to \",\n    max_bill_length_mm\n  )) |&gt;\n  select(\n    \"species\",\n    \"median_bill_length_mm\",\n    \"min_max_bill_length_mm\"\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n  species   median_bill_length_mm min_max_bill_length_mm\n  &lt;fct&gt;                     &lt;dbl&gt; &lt;chr&gt;                 \n1 Adelie                     38.8 32.1 to 46.0          \n2 Chinstrap                  49.6 40.9 to 58.0          \n3 Gentoo                     47.3 40.9 to 59.6          \n\n\nThe benefit of using dbplyr now becomes quite clear if we take a look at the corresponding SQL that is generated for us.\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    min_bill_length_mm = min(bill_length_mm),\n    median_bill_length_mm = median(bill_length_mm),\n    max_bill_length_mm = max(bill_length_mm)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(min, \" to \", max)) |&gt;\n  select(\n    \"species\",\n    \"median_bill_length_mm\",\n    \"min_max_bill_length_mm\"\n  ) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  species,\n  median_bill_length_mm,\n  CONCAT_WS('', .Primitive(\"min\"), ' to ', .Primitive(\"max\")) AS min_max_bill_length_mm\nFROM (\n  SELECT\n    species,\n    MIN(bill_length_mm) AS min_bill_length_mm,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY bill_length_mm) AS median_bill_length_mm,\n    MAX(bill_length_mm) AS max_bill_length_mm\n  FROM penguins\n  GROUP BY species\n) q01\n\n\nNow instead of having to write this somewhat complex SQL we can use the friendlier dplyr syntax that may well be more familiar if coming from an R programming background.\n\n\n\n\n\n\nDifferent SQL for different database management systems\n\n\n\n\n\nWhile using the SQL programming language allows us to process information in a relational database, there are a range of SQL dialects used with different database management systems. One particular benefit of using the above approach of translating R code to SQL via dbplyr is that the SQL generated will be in the appropriate dialect for the database management system being used. Here for example we can see how the same R code will generate different SQL for PosgreSQL compared to SQL Server.\n\npenguins |&gt;\n  dbplyr::lazy_frame(con = dbplyr::simulate_postgres()) |&gt;\n  mutate(\n    category = if_else(bill_length_mm &gt; 40 & bill_depth_mm &gt; 18,\n      \"big\", \"small\"\n    ),\n    date = as.Date(paste0(\"01-01-\", year))\n  ) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  CASE WHEN (`bill_length_mm` &gt; 40.0 AND `bill_depth_mm` &gt; 18.0) THEN 'big' WHEN NOT (`bill_length_mm` &gt; 40.0 AND `bill_depth_mm` &gt; 18.0) THEN 'small' END AS `category`,\n  CAST(CONCAT_WS('', '01-01-', `year`) AS DATE) AS `date`\nFROM `df`\n\npenguins |&gt;\n  dbplyr::lazy_frame(con = dbplyr::simulate_mssql()) |&gt;\n  mutate(\n    category = if_else(bill_length_mm &gt; 40 & bill_depth_mm &gt; 18,\n      \"big\", \"small\"\n    ),\n    date = as.Date(paste0(\"01-01-\", year))\n  ) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  IIF(`bill_length_mm` &gt; 40.0 AND `bill_depth_mm` &gt; 18.0, 'big', 'small') AS `category`,\n  TRY_CAST('01-01-' + `year` AS DATE) AS `date`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#example-analysis",
    "href": "working_with_databases_from_r.html#example-analysis",
    "title": "1  A first analysis using data in a database",
    "section": "1.5 Example analysis",
    "text": "1.5 Example analysis\nTo see a little more how this approach will work in practice for perfoming a data analysis, let´s start by getting a count by species\n\npenguins_db |&gt; \n  group_by(species) |&gt; \n  count()\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n# Groups:   species\n  species       n\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nNow suppose we are particularly interested in the body mass variable. We can first notice that there are a couple of missing records for this.\n\npenguins_db |&gt;\n  mutate(missing_body_mass_g = if_else(\n    is.na(body_mass_g), 1, 0\n  )) |&gt;\n  group_by(species, missing_body_mass_g) |&gt;\n  tally()\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n  species   missing_body_mass_g     n\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie                      0   151\n2 Adelie                      1     1\n3 Gentoo                      1     1\n4 Gentoo                      0   123\n5 Chinstrap                   0    68\n\n\nWe can get the mean for each of the species (dropping those two missing records).\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(mean_body_mass_g = round(mean(body_mass_g, na.rm = TRUE), 1))\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n  species   mean_body_mass_g\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie               3701.\n2 Chinstrap            3733.\n3 Gentoo               5076 \n\n\nWe could also make a histogram of values for each of the species.\n\npenguins_db |&gt;\n  collect() |&gt;\n  ggplot(aes(group = species, fill = species)) +\n  facet_grid(species ~ .) +\n  geom_histogram(aes(body_mass_g), colour = \"black\", binwidth = 100) +\n  xlab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right time to collect\n\n\n\n\n\ncollect() brings data out of the database and into R. Above we use it to bring the entire penguins data back into R so that we can then use ggplot() to make our histogram.\nWhen working with large data sets, as is often the case when interacting with a database, we typically want to keep as much computation as possible on the database side, up until the point we need to bring the data out for further analysis steps that are not possible using SQL. This could be like the case above for plotting, but could also be for other analytic steps like fitting statistical models. In such cases it is important that we only bring out the required data for this task as we will likely have much less memory available on our local computer than is available for the database.\n\n\n\nNow let’s look at the relationship between body mass and bill depth.\n\npenguins |&gt;\n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere we see a negative correlation between body mass and bill depth which seems rather unexpected. But what about if we stratify this query by species?\n\npenguins |&gt;\n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  facet_grid(species ~ .) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs well as having an example of working with data in database from R, you also have an example of Simpson´s paradox!",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#disconnecting-from-the-database",
    "href": "working_with_databases_from_r.html#disconnecting-from-the-database",
    "title": "1  A first analysis using data in a database",
    "section": "1.6 Disconnecting from the database",
    "text": "1.6 Disconnecting from the database\nAnd now we’ve reached the end of this example, we can close our connection to the database.\n\ndbDisconnect(db)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "working_with_databases_from_r.html#further-reading",
    "href": "working_with_databases_from_r.html#further-reading",
    "title": "1  A first analysis using data in a database",
    "section": "1.7 Further reading",
    "text": "1.7 Further reading\n\nR for Data Science (Chapter 13: Relational data)\nWriting SQL with dbplyr\nData Carpentry: SQL databases and R",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_verbs.html",
    "href": "tidyverse_verbs.html",
    "title": "2  Core verbs for analytic pipelines utilising a database",
    "section": "",
    "text": "2.0.1 Tidyverse functions\nWe saw in the previous chapter that we can use familiar dplyr verbs with data held in a database. In the last chapter we were working with just a single table which we loaded into the database. When working with databases we will though typically be working with multiple tables (which we’ll see later will be true when working with data in the OMOP CDM format). For this chapter we will see more tidyverse functionality that can be used with data in a database, this time using the nycflights13 data. As we can see, now we have a set of related tables with data on flights departing from New York City airports in 2013.\nLet’s load the required libraries, add our data to a duckdb database, and then create references to each of these tables.\nFor almost all analyses we want to go from having our starting data spread out across multiple tables in the database to a single tidy table containing the data we need for an analysis. We can often get to our tidy analytic dataset using the below tidyverse functions (most of which coming from dplyr, but a couple also from the tidyr package). These functions all work with data in a database by generating SQL that will have the same purpose as if these functions were being run against data in R.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "href": "tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "title": "2  Core verbs for analytic pipelines utilising a database",
    "section": "2.1 Getting to an analytic dataset",
    "text": "2.1 Getting to an analytic dataset\nTo see a little more on how we can use the above functions, let’s say we want to do an analysis of late flights from JFK airport. We want to see whether there is some relationship between plane characteristics and the risk of delay.\nFor this we’ll first use the filter() and select() dplyr verbs to get the data from the flights table. Note, we’ll rename arr_delay to just delay.\n\ndelayed_flights_db &lt;- flights_db |&gt; \n  filter(!is.na(arr_delay),\n        origin == \"JFK\") |&gt; \n  select(dest, \n         distance, \n         carrier, \n         tailnum, \n         \"delay\" = \"arr_delay\")\n\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT dest, distance, carrier, tailnum, arr_delay AS delay\nFROM flights\nWHERE (NOT((arr_delay IS NULL))) AND (origin = 'JFK')\n\n\n\n\n\nAnd when executed, our results will look like the following\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 5]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n   dest  distance carrier tailnum delay\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 MIA       1089 AA      N619AA     33\n 2 BQN       1576 B6      N804JB    -18\n 3 MCO        944 B6      N593JB     -8\n 4 PBI       1028 B6      N793JB     -2\n 5 TPA       1005 B6      N657JB     -3\n 6 LAX       2475 UA      N29129      7\n 7 BOS        187 B6      N708JB     -4\n 8 ATL        760 DL      N3739P     -8\n 9 SFO       2586 UA      N532UA     14\n10 RSW       1074 B6      N635JB      4\n# ℹ more rows\n\n\nNow we’ll add plane characteristics from the planes table. We will use an inner join so that only records for which we have the plane characteristics will be kept. Just for the sake of making a simple example, we’ll categorise planes as large or small, with small planes defined as having 100 or less seats.\n\ndelayed_flights_db &lt;- delayed_flights_db |&gt; \n  inner_join(planes_db |&gt; \n              select(tailnum, \n                     seats),\n            by = join_by(tailnum))\n\nIt is important to note that our first query was not executed, as we didn’t use either compute() or collect(), so we’ll now have added our join to the original query.\n\n\n\n\n\n\nShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT LHS.*, seats\nFROM (\n  SELECT dest, distance, carrier, tailnum, arr_delay AS delay\n  FROM flights\n  WHERE (NOT((arr_delay IS NULL))) AND (origin = 'JFK')\n) LHS\nINNER JOIN planes\n  ON (LHS.tailnum = planes.tailnum)\n\n\n\n\n\nAnd when executed, our results will look like the following\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/:memory:]\n   dest  distance carrier tailnum delay seats\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 BQN       1576 B6      N804JB    -18   200\n 2 MCO        944 B6      N593JB     -8   200\n 3 PBI       1028 B6      N793JB     -2   200\n 4 BOS        187 B6      N708JB     -4   200\n 5 ATL        760 DL      N3739P     -8   189\n 6 SJU       1598 B6      N794JB    -21   200\n 7 PHX       2153 US      N535UW      0   379\n 8 BOS        187 B6      N805JB    -10   200\n 9 LAS       2248 B6      N558JB     -6   200\n10 SLC       1990 DL      N3763D     -9   189\n# ℹ more rows\n\n\nGetting to this tidy dataset has been done in the database via R code translated to SQL. With this, we could collect our analytic dataset and go from there. For example, say we wanted to fit a logistic regression with was_delayed as our dependent variable (which we arbitrarily define as arriving more than 30 minutes after they were expected) and size as the explanatory variable (which we arbitrarily define as small if less that 100 seats or large if 100 seats or more).\n\nlibrary(parsnip)\nlibrary(gtsummary)\ndelayed_flights &lt;- delayed_flights_db |&gt; \n  collect() |&gt; \n  mutate(was_delayed = as.factor(if_else(delay &gt; 30, 1L, 0L)),\n         size = case_when(seats &gt; 100 ~ \"Large\", \n                          seats &lt;= 100 ~ \"Small\"))\n\nlogistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(was_delayed ~ size, data = delayed_flights) |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nsize\n\n\n\n\n\n\n\n\n    Large\n—\n—\n\n\n\n\n    Small\n0.37\n0.34, 0.41\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html",
    "href": "tidyverse_expressions.html",
    "title": "3  Supported expressions for database queries",
    "section": "",
    "text": "3.1 Data types\nIn the previous chapter Chapter 2 we saw that there are a core set of tidyverse functions that can be used with databases to extact data for analysis. The SQL code used in the previous chapter would be the same for all database management systems, with only joins and variable selection being used.\nFor more complex data pipleines we will, however, often need to incorporate additional expressions within these functions. Because of differences across database management systems, the SQL these get tanslated to can vary. Moreover, some expessions may only be supported for some subset of databases. When writing code which we want to work across different database management systems we therefore need to keep in mind what is supported where. To help with this, the sections below show the available translations for common expressions we might wish to use.\nLet’s first load the packages which these expressions come from. In addition to base R types, bit64 adds support for integer64. The stringr package provides functions for working with strings, while clock has various functions for working with dates. Many other useful expressions will come from dplyr itself. Note we also set an option option so that we will see an error if an expression does not have a ready translation.\nCommonly used data types are consistently supported across database backends. We can use the base as.numeric(), as.integer(), as.charater(), as.Date(), and as.POSIXct(). We can also use as.integer64() from the bit64 to coerce to interger64, and the as_date() and as_datetime() from the clock package instead of as.Date() and as.POSIXct(), respectively.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#data-types",
    "href": "tidyverse_expressions.html#data-types",
    "title": "3  Supported expressions for database queries",
    "section": "",
    "text": "Show SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ntranslate_sql(as.numeric(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC) AS INT)\n\ntranslate_sql(as.integer64(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC(38, 0)) AS BIGINT)\n\ntranslate_sql(as.character(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS VARCHAR(MAX))\n\ntranslate_sql(as.Date(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as_datetime(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as.logical(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; TRY_CAST(`var` AS BIT)",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#comparison-and-logical-operators",
    "href": "tidyverse_expressions.html#comparison-and-logical-operators",
    "title": "3  Supported expressions for database queries",
    "section": "3.2 Comparison and logical operators",
    "text": "3.2 Comparison and logical operators\nBase r comparison operators, such as &lt;, &lt;=, ==, &gt;=, &gt;, are also well supported in all database backends. Logical operators, such as & and | can also be used as if the data was in R.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ntranslate_sql(var_1 == var_2, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1  &lt; 200, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1  &lt; 200, \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#conditional-statements",
    "href": "tidyverse_expressions.html#conditional-statements",
    "title": "3  Supported expressions for database queries",
    "section": "3.3 Conditional statements",
    "text": "3.3 Conditional statements\nThe base ifelse function, along with if_else and case_when from dplyr are transalated for each database backend. As can be seen in the translations, case_when maps to the SQL CASE WHEN statement.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = NULL), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, \n                        var == \"b\" ~ 2L, \n                        var == \"c\" ~ 3L, \n                        .default = \"something else\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#working-with-strings",
    "href": "tidyverse_expressions.html#working-with-strings",
    "title": "3  Supported expressions for database queries",
    "section": "3.4 Working with strings",
    "text": "3.4 Working with strings\nCompared to the previous sections, there is much more variation in support of functions to work with strings across database management systems. In particular, although various useful stringr functions do have translations it can be seen below that more translations are available for some databases compared to others.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; TRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g'))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, 'b')\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; (NOT(REGEXP_MATCHES(`var`, 'b')))\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, '[aeiou]')\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`,'^(?:'||'a'))\n\ntranslate_sql(str_ends(var, \"a\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; REGEXP_MATCHES((?:`var`,'a'||')$')\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_redshift())\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_redshift())\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_redshift())\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_redshift())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_redshift())\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_redshift())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_redshift())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_postgres())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), \n              con = simulate_postgres())\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), \n              con = simulate_postgres())\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_postgres())\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_postgres())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_postgres())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; TRIM(`var`)\n\ntranslate_sql(str_squish(var), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(TRIM(`var`), '\\\\s+', ' ')\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 1.0, 1.0)\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 1.0, 1.0)\n\ntranslate_sql(str_remove_all(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a')\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_snowflake())\n\nError in `REGEXP_INSTR()`:\n! Don't know how to translate `REGEXP_INSTR()`\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var),\n              con = simulate_spark_sql())\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_spark_sql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"),\n              con = simulate_spark_sql())\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_spark_sql())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ntranslate_sql(nchar(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LEN(`var`)\n\ntranslate_sql(nzchar(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), \n              con = simulate_mssql())\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var),\n              con = simulate_mssql())\n\nError in `str_to_title()`:\n! `str_to_title()` is not available in this SQL variant.\n\ntranslate_sql(str_trim(var), \n              con = simulate_mssql())\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var),\n              con = simulate_mssql())\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"),\n              con = simulate_mssql())\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"),\n              con = simulate_mssql())\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"),\n              con = simulate_mssql())\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), \n              con = simulate_mssql())\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"),\n              con = simulate_mssql())\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#working-with-dates",
    "href": "tidyverse_expressions.html#working-with-dates",
    "title": "3  Supported expressions for database queries",
    "section": "3.5 Working with dates",
    "text": "3.5 Working with dates\nLike with strings, support for working with dates is somewhat mixed. In general, we would use functions from the clock package such as get_day(), get_month(), get_year() to extact parts from a date, add_days() to add or subtract days to a date, and date_count_between() to get the number of days between two date variables.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbRedshiftPostgresSnowflakeSparkSQL Server\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL '1.0 day')\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL '1.0 year')\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_duckdb())\n\nError in `difftime()`:\n! Don't know how to translate `difftime()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_duckdb())\n\n&lt;SQL&gt; DATEDIFF('day', `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_duckdb())\n\nError in date_count_between(date_1, date_2, \"year\"): The only supported value for `precision` on SQL backends is \"day\"\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_redshift())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_redshift())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_redshift())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 day')\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 year')\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_postgres())\n\n&lt;SQL&gt; (CAST(`date_2` AS DATE) - CAST(`date_1` AS DATE))\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_postgres())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_postgres())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATE_PART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_snowflake())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_snowflake())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_snowflake())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('DAY', `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('MONTH', `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_PART('YEAR', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATE_ADD(`date_1`, 1.0)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; ADD_MONTHS('`date_1`', 1.0 * 12.0)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_spark_sql())\n\n&lt;SQL&gt; DATEDIFF(`date_2`, `date_1`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_spark_sql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_spark_sql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\n\n\n\n\ntranslate_sql(get_day(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEPART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(difftime(date_1, date_2), \n              con = simulate_mssql())\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), \n              con = simulate_mssql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`\n\ntranslate_sql(date_count_between(date_1, date_2, \"year\"), \n              con = simulate_mssql())\n\nError in `date_count_between()`:\n! Don't know how to translate `date_count_between()`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#data-aggregation",
    "href": "tidyverse_expressions.html#data-aggregation",
    "title": "3  Supported expressions for database queries",
    "section": "3.6 Data aggregation",
    "text": "3.6 Data aggregation\nWithin the context of using summarise(), we can get aggregate across entire columns using functions such as n(), n_distinct(), sum(), min(), max(), mean(), and sd(). As can be seen below, the SQL for these calculations is similar across different database management systems.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT row(`x`)) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), a = \"a\", con = simulate_mssql()) %&gt;% \n  summarise(\n          n = n(),\n          n_unique = n_distinct(x),\n          sum = sum(x, na.rm = TRUE),\n          sum_is_1 = sum(x == 1, na.rm = TRUE),\n          min = min(x, na.rm = TRUE),\n          mean = mean(x, na.rm = TRUE),\n          max = max(x, na.rm = TRUE),\n          sd = sd(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(CAST(IIF(`x` = 1.0, 1, 0) AS BIT)) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDEV(`x`) AS `sd`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#window-functions",
    "href": "tidyverse_expressions.html#window-functions",
    "title": "3  Supported expressions for database queries",
    "section": "3.7 Window functions",
    "text": "3.7 Window functions\nIn the previous section we saw how aggregate functions can be used to perform operations across entire columns. Window functions differ in that they perform calculation across rows that are in some way related to a current row. For these we now use mutate() instead of using summarise().\nWe can use window functions like cumsum() and cummean() to calculate running totals and averages, or lag() and lead() to help compare rows to their preceding or following rows.\nGiven that window functions are compare rows to rows before or after them, we will ofter use arrange() to specify the order of rows. This will translate into a ORDER BY clause in the SQL. In addition, we may well also want to apply window functions within some specific groupings in our data. Using group_by() would result in a PARTITION BY clause in the translated SQL so that window function operates on each group independently.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_duckdb()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_duckdb()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_postgres()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_postgres()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_redshift()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_redshift()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_snowflake()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_snowflake()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_spark_sql()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_spark_sql()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\nlazy_frame(x = c(10, 20, 30),\n           z = c(1, 2, 3),\n           con = simulate_mssql()) %&gt;% \n  window_order(z) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20, 30),  \n           y = c(\"a\", \"a\", \"b\"),\n           z = c(1, 2, 3),\n           con = simulate_mssql()) %&gt;% \n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(sum_x = cumsum(x),\n         mean_x = cummean(x),\n         lag_x = lag(x), \n         lead_x = lead(x)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "href": "tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "title": "3  Supported expressions for database queries",
    "section": "3.8 Calculating quantiles, including the median",
    "text": "3.8 Calculating quantiles, including the median\nSo far we’ve seen that we can perform various data manipulations and calculate summary statistics for different database management systems using the same R code. Although the translated SQL has been different, the databases all supported similar approaches to perform these queries.\nA case where this is not the case is when we are interested in summarisng distibutions of the data and estimating quantiles. For example, let’s take estimating the median as an example. Some databases only support calculating the median as an aggregation function similar to how min, mean, and max were calculated above. However, some others only support it as a window function like lead() and lag() above. Unfortunately this means that for some databases quantiles can only be calculated using the summarise aggregation approach, while in others only the mutate window approach can be used.\n\n\n\n\n\n\nShow SQL\n\n\n\n\n\n\nduckdbpostgresredshiftSnowflakeSparkSQL Server\n\n\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_duckdb()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_postgres()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nℹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_redshift()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nℹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_snowflake()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT MEDIAN(`x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = simulate_spark_sql()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `df`.*, MEDIAN(`x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1,2), con = simulate_mssql()) %&gt;% \n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `summarise()` is not supported for SQL\n  Server.\nℹ Use a combination of `distinct()` and `mutate()` for the same result:\n  `mutate(&lt;col&gt; = median(x, na.rm = TRUE)) %&gt;% distinct(&lt;col&gt;)`\n\nlazy_frame(x = c(1,2), con = simulate_mssql()) %&gt;% \n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "r_data_models.html",
    "href": "r_data_models.html",
    "title": "4  Database references in R",
    "section": "",
    "text": "In the previous chapters we’ve seen that after connecting to a database we can create references\nIntroduce dm with lahman data ….",
    "crumbs": [
      "Getting started with working databases from R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Database references in R</span>"
    ]
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Getting to know the OMOP CDM",
    "section": "",
    "text": "The OMOP CDM standardises the format and content of health care data.\n\nIn 5  Creating a cdm reference we will see how we will see how to create a reference in R to each of these OMOP CDM tables. Because we already know the structure of our data, creating this reference to the various tables will be our first step in our analysis scripts.\nThe OMOP CDM is a person-centric model, and the person and observation period tables are two key tables for any analysis. In 6  Person and observation period tables we will see more on how these tables can be used as the starting point for identifying your study participants.\nThe OMOP CDM standarises the content of health care data via the OMOP CDM vocabulary tables, which provides a set of standard concepts to represent different clinical events. The vocabulary tables are described in 7  Vocabulary tables, with these tables playing a fundamental role when we identify the clinical events of interest for our study.\nClinical records associated with individuals are spread across various OMOP CDM tables, covering various domains. In 8  Clinical tables we will see how these tables represent events and link back to the person and vocabulary tables.",
    "crumbs": [
      "Getting to know the OMOP CDM"
    ]
  },
  {
    "objectID": "cdm_reference.html",
    "href": "cdm_reference.html",
    "title": "5  Creating a cdm reference",
    "section": "",
    "text": "5.1 The OMOP CDM layout\nThe OMOP CDM standardises the structure of health care data. Data is stored across a system of tables with established relationships between them. In other words, the OMOP CDM provides a relational database structure. An entity-relationship diagram for version 5.4 of the OMOP CDM is shown below.\nThese tables are going to be described in more detail in the following chapters. But before that, we can first see how to work with our OMOP CDM data from R.",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a cdm reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#referencing-tables-from-the-omop-common-data-model",
    "href": "cdm_reference.html#referencing-tables-from-the-omop-common-data-model",
    "title": "5  Creating a cdm reference",
    "section": "5.2 Referencing tables from the OMOP common data model",
    "text": "5.2 Referencing tables from the OMOP common data model\nAs seen in the previous chapter, once a connection to the database has been created we can then create references to the various tables in the database and build queries using in a familiar dplyr style. To show this, let’s download an example dataset (synthea-covid19-10k) provided by CDMConnector.\n\nlibrary(DBI)\nlibrary(here)\nlibrary(dplyr)\n\nNow we have this downloaded, we can create a connection to a duckdb database containing the data in a similar way to how we’ve done before.\n\ndb &lt;- dbConnect(\n  duckdb::duckdb(), \n  dbdir = CDMConnector::eunomiaDir(datasetName = \"synthea-covid19-10k\"))\n\ndb |&gt; \n  tbl(\"person\")\n\n# Source:   table&lt;person&gt; [?? x 18]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         1              8532          1970              4           24\n 2         2              8532          1929              3           18\n 3         3              8532          1970              4            4\n 4         4              8507          1966              2           26\n 5         5              8532          1936              6           10\n 6         6              8507          1996              5           29\n 7         7              8507          1923             11           14\n 8         8              8507          2018              8           20\n 9         9              8532          1933              2           11\n10        10              8507          2010              3            7\n# ℹ more rows\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ndb |&gt; \n  tbl(\"observation_period\")\n\n# Source:   table&lt;observation_period&gt; [?? x 5]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n   observation_period_id person_id observation_period_s…¹ observation_period_e…²\n                   &lt;int&gt;     &lt;int&gt; &lt;date&gt;                 &lt;date&gt;                \n 1                     1         1 2014-05-09             2023-05-12            \n 2                     2         2 1977-04-11             1986-09-15            \n 3                     3         3 2014-04-19             2023-04-22            \n 4                     4         4 2014-03-22             2023-04-08            \n 5                     5         5 2013-11-13             2023-01-04            \n 6                     6         6 2013-07-17             2021-08-04            \n 7                     7         7 2013-06-26             2022-08-17            \n 8                     8         8 2018-08-20             2022-07-25            \n 9                     9         9 2013-08-03             2022-09-24            \n10                    10        10 2013-08-11             2023-04-02            \n# ℹ more rows\n# ℹ abbreviated names: ¹​observation_period_start_date,\n#   ²​observation_period_end_date\n# ℹ 1 more variable: period_type_concept_id &lt;int&gt;\n\n\nWe could also perform similar queries to those seen in chapter 1, but this time working with the patient-level, rather than penguin-level, data.\n\ndb |&gt; \n  tbl(\"person\") |&gt; \n  group_by(year_of_birth) |&gt; \n  count() \n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n# Groups:   year_of_birth\n   year_of_birth     n\n           &lt;int&gt; &lt;dbl&gt;\n 1          1923    66\n 2          1924   123\n 3          1925   140\n 4          1926   146\n 5          1927   162\n 6          1928   136\n 7          1929   146\n 8          1930   119\n 9          1931   135\n10          1932   142\n# ℹ more rows",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a cdm reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#creating-a-reference-to-the-omop-common-data-model",
    "href": "cdm_reference.html#creating-a-reference-to-the-omop-common-data-model",
    "title": "5  Creating a cdm reference",
    "section": "5.3 Creating a reference to the OMOP common data model",
    "text": "5.3 Creating a reference to the OMOP common data model\nAs the structure of the OMOP CDM is already known, we can avoid the overhead of creating individual references to the OMOP CDM tables like above by instead creating a joint reference for all OMOP CDM database tables in one go.\nThe R object representing OMOP CDM data is defined by the omopgenerics package), with the the CDMConnector package providing a means of connecting to a OMOP CDM data held in a database. As well as specifying the schema containing our OMOP CDM tables, we also specify a write schema where any database tables we create during our analysis will be stored (often our OMOP CDM tables will be in a schema that we only have read-access to and we’ll have another schema where we can have write-access where we intermediate tables are created for a given a study).\n\nlibrary(omopgenerics)\n\nWarning: package 'omopgenerics' was built under R version 4.4.2\n\n\n\nAttaching package: 'omopgenerics'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(CDMConnector)\n\n\nAttaching package: 'CDMConnector'\n\n\nThe following objects are masked from 'package:omopgenerics':\n\n    cdmName, recordCohortAttrition, uniqueTableName\n\ncdm &lt;- cdmFromCon(con = db,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  cdmName = \"Synthea Covid-19 data\")\n\nNote: method with signature 'DBIConnection#Id' chosen for function 'dbExistsTable',\n target signature 'duckdb_connection#Id'.\n \"duckdb_connection#ANY\" would also be valid\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of Synthea Covid-19 data ──────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence, visit_detail,\ncondition_occurrence, drug_exposure, procedure_occurrence, device_exposure,\nmeasurement, observation, death, note, note_nlp, specimen, fact_relationship,\nlocation, care_site, provider, payer_plan_period, cost, drug_era, dose_era,\ncondition_era, metadata, cdm_source, concept, vocabulary, domain,\nconcept_class, concept_relationship, relationship, concept_synonym,\nconcept_ancestor, source_to_concept_map, drug_strength, cohort_definition,\nattribute_definition\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -\n\n\n\n\n\n\n\n\nSetting a write prefix\n\n\n\n\n\nWe can also specify a write prefix and this will be used whenever permanent tables are created the write schema. This can be useful when we’re sharing our write schema with others and want to avoid table name conflicts and easily drop tables created as part of a particular study.\n\ncdm &lt;- cdmFromCon(con = db,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  writePrefix = \"my_study_\",\n                  cdmName = \"Synthea Covid-19 data\")\n\n\n\n\nWe can see that we now have an object that contains references to all the OMOP CDM tables. We can reference specific tables using the “$” or “[[ … ]]” operators.\n\ncdm$person\n\n# Source:   table&lt;main.person&gt; [?? x 18]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         1              8532          1970              4           24\n 2         2              8532          1929              3           18\n 3         3              8532          1970              4            4\n 4         4              8507          1966              2           26\n 5         5              8532          1936              6           10\n 6         6              8507          1996              5           29\n 7         7              8507          1923             11           14\n 8         8              8507          2018              8           20\n 9         9              8532          1933              2           11\n10        10              8507          2010              3            7\n# ℹ more rows\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ncdm[[\"observation_period\"]]\n\n# Source:   table&lt;main.observation_period&gt; [?? x 5]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n   observation_period_id person_id observation_period_s…¹ observation_period_e…²\n                   &lt;int&gt;     &lt;int&gt; &lt;date&gt;                 &lt;date&gt;                \n 1                     1         1 2014-05-09             2023-05-12            \n 2                     2         2 1977-04-11             1986-09-15            \n 3                     3         3 2014-04-19             2023-04-22            \n 4                     4         4 2014-03-22             2023-04-08            \n 5                     5         5 2013-11-13             2023-01-04            \n 6                     6         6 2013-07-17             2021-08-04            \n 7                     7         7 2013-06-26             2022-08-17            \n 8                     8         8 2018-08-20             2022-07-25            \n 9                     9         9 2013-08-03             2022-09-24            \n10                    10        10 2013-08-11             2023-04-02            \n# ℹ more rows\n# ℹ abbreviated names: ¹​observation_period_start_date,\n#   ²​observation_period_end_date\n# ℹ 1 more variable: period_type_concept_id &lt;int&gt;",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a cdm reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#mutability-of-the-cdm-reference",
    "href": "cdm_reference.html#mutability-of-the-cdm-reference",
    "title": "5  Creating a cdm reference",
    "section": "5.4 Mutability of the cdm reference",
    "text": "5.4 Mutability of the cdm reference\nAn important characteristic of our cdm reference is that we can alter the tables in R, but the OMOP CDM data will not be affected.\nFor example, let’s say we want to perform a study with only people born in 1970. For this we could filter our person table to only people born in this year.\n\ncdm$person &lt;- cdm$person |&gt; \n  filter(year_of_birth == 1970)\n\ncdm$person\n\n# Source:   SQL [?? x 18]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         1              8532          1970              4           24\n 2         3              8532          1970              4            4\n 3        50              8532          1970              8           21\n 4       126              8507          1970              7           16\n 5       207              8507          1970              6           18\n 6       239              8532          1970              8            8\n 7       535              8532          1970              4            7\n 8       549              8532          1970              6            4\n 9       587              8532          1970             11           22\n10       967              8532          1970              9            2\n# ℹ more rows\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nFrom now on, when we work with our cdm reference this restriction will continue to have been applied.\n\ncdm$person |&gt; \n    tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n      n\n  &lt;dbl&gt;\n1    78\n\n\nThe original OMOP CDM data itself however will remain unaffected. And we can see if we create our reference again that the underlying data is unchanged.\n\ncdm &lt;- cdmFromCon(con = db,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  cdmName = \"Synthea Covid-19 data\")\ncdm$person |&gt; \n    tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n      n\n  &lt;dbl&gt;\n1 10754\n\n\nThe mutability of our cdm reference is a useful feature for studies as it means we can easily tweak our OMOP CDM data if needed. Meanwhile, leaving the underlying data unchanged is essential so that other study code can run against the data unaffected by any of our changes.\nOne thing we can’t do though is alter the structure of OMOP CDM tables. For example this code would cause an error as the person table must have the column person_id.\n\ncdm$person &lt;- cdm$person |&gt; \n    rename(\"new_id\" = \"person_id\")\n\nIn such a case we would have to call the table something esle.\n\ncdm$person_new &lt;- cdm$person |&gt; \n    rename(\"new_id\" = \"person_id\") |&gt; \n    compute()\n\nNow we would have this new table as an additional table in our cdm reference, knowing it was not in the format of one of the core OMOP CDM tables.\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of Synthea Covid-19 data ──────────────────────\n\n\n• omop tables: person, observation_period, visit_occurrence, visit_detail,\ncondition_occurrence, drug_exposure, procedure_occurrence, device_exposure,\nmeasurement, observation, death, note, note_nlp, specimen, fact_relationship,\nlocation, care_site, provider, payer_plan_period, cost, drug_era, dose_era,\ncondition_era, metadata, cdm_source, concept, vocabulary, domain,\nconcept_class, concept_relationship, relationship, concept_synonym,\nconcept_ancestor, source_to_concept_map, drug_strength, cohort_definition,\nattribute_definition\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a cdm reference</span>"
    ]
  },
  {
    "objectID": "cdm_reference.html#cdm-attributes",
    "href": "cdm_reference.html#cdm-attributes",
    "title": "5  Creating a cdm reference",
    "section": "5.5 CDM attributes",
    "text": "5.5 CDM attributes\n\n5.5.1 CDM name\nOur cdm reference will be associated with a name. By default this name will be taken from the cdm source name field from the cdm source table. We can though set this to a different name when creating our cdm reference. This cdm name attribute of our reference is particularly useful in the context of network studies to keep track of which results are associated with which database.\n\ncdm &lt;- cdmFromCon(db,\n  cdmSchema = \"main\", \n  writeSchema = \"main\")\ncdm$cdm_source\n\n# Source:   table&lt;main.cdm_source&gt; [?? x 10]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb]\n  cdm_source_name cdm_source_abbreviation cdm_holder source_description    \n  &lt;chr&gt;           &lt;chr&gt;                   &lt;chr&gt;      &lt;chr&gt;                 \n1 Synthea         Synthea                 \"\"         Synthea Synthetic Data\n# ℹ 6 more variables: source_documentation_reference &lt;chr&gt;,\n#   cdm_etl_reference &lt;chr&gt;, source_release_date &lt;date&gt;,\n#   cdm_release_date &lt;date&gt;, cdm_version &lt;chr&gt;, vocabulary_version &lt;chr&gt;\n\ncdmName(cdm)\n\n[1] \"Synthea\"\n\ncdm &lt;- cdmFromCon(db,\n  cdmSchema = \"main\", \n  writeSchema = \"main\", \n  cdmName = \"my_cdm\")\ncdmName(cdm)\n\n[1] \"my_cdm\"\n\n\n\n\n5.5.2 CDM version\nWe can also easily check the OMOP CDM version that is being used\n\ncdmVersion(cdm)\n\n[1] \"5.3\"\n\n\n\n\n5.5.3 CDM Source\nAlthough typically we won’t need to use them for writing study code, we can also access lower-level information on the source, such as the database connection.\n\nattr(cdmSource(cdm), \"dbcon\")\n\n&lt;duckdb_connection ce7c0 driver=&lt;duckdb_driver dbdir='C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp0Q1yIl\\filea07417a23371.duckdb' read_only=FALSE bigint=numeric&gt;&gt;",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a cdm reference</span>"
    ]
  },
  {
    "objectID": "omop_person_obs_period.html",
    "href": "omop_person_obs_period.html",
    "title": "6  Person and observation period tables",
    "section": "",
    "text": "6.1 Working with dates\nFor this chapter, lets again use the synthetic Covid-19 dataset.\nThe OMOP CDM is person-centric, with the person table containing records to identify each person in the database. Individuals should have only one record in the person table.\nAs each row refers to a unique person, we can quickly get a count of the number of individuals in the database like so\nThe person table also contains some demographic information, including a gender concept for each person. We can get a count grouped by this variable, but as this uses a concept we’ll also need to join to the concept table to get the corresponding concept name for each concept id.\nMeanwhile, the observation period table contains records indicating spans of time over which clinical events can be reliably observed for the people in the person table. It is important to note that individuals can potentially have multiple observation periods. In some datasets all individuals will have one record per person (which is common, for example, with primary care data) while other datasets individuals may leave and re-enter multiple times (seen commonly, for example, in insurance claims data).\nSay we wanted a count of people grouped by the year during which their first observation period started. We could do this like so:\nWhen working with dates, the best supported functions come from the clock package. In particular, a lot of the date manipulations we might be interested in can be achieved through the use of add_days (to add days to or subtract days from a date), add_years (same as add_days by with years as the timescale), and date_count_between (to get the difference between two dates). For example let’s see how these can be applied to date fields in the observation period table.\nlibrary(clock)\nlibrary(ggplot2)\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n           add_days(observation_period_start_date, 30L),\n         observation_period_start_date_plus_10_years = \n           add_years(observation_period_start_date, 10L)) |&gt; \n  glimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpS6sHuH\\file865834e1e58.duckdb]\n$ observation_period_id                       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9,…\n$ person_id                                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9,…\n$ observation_period_start_date               &lt;date&gt; 2014-05-09, 1977-04-11, 2…\n$ observation_period_end_date                 &lt;date&gt; 2023-05-12, 1986-09-15, 2…\n$ period_type_concept_id                      &lt;int&gt; 44814724, 44814724, 448147…\n$ observation_period_start_plus_30_days       &lt;dttm&gt; 2014-06-08, 1977-05-11, 2…\n$ observation_period_start_date_plus_10_years &lt;dttm&gt; 2024-05-09, 1987-04-11, 2…\n\ncdm$observation_period |&gt; \n  dplyr::mutate(observation_days = date_count_between(\"observation_period_start_date\", \n                             \"observation_period_end_date\", \"day\"))  |&gt; \n  dplyr::mutate(observation_years = observation_days/ 365.25) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_histogram(aes(observation_years), \n                 binwidth=2, colour=\"grey\") +\n  theme_bw()",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Person and observation period tables</span>"
    ]
  },
  {
    "objectID": "omop_person_obs_period.html#working-with-dates",
    "href": "omop_person_obs_period.html#working-with-dates",
    "title": "6  Person and observation period tables",
    "section": "",
    "text": "Piping and SQL\n\n\n\n\n\nAlthough piping queries has little impact on performance when using R with data in memory, when working with a database the SQL generated will differ when using multiple function calls (with a separate operation specified in each) instead of multiple operations within a single function call.\nFor example, a single mutate function above would generate the below SQL.\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n                add_days(observation_period_start_date, 30L),\n         observation_period_start_date_plus_10_years = \n                add_years(observation_period_start_date, 10L)) |&gt; \n  select(\"observation_period_id\", \"person_id\", \n         \"observation_period_start_plus_30_days\",\n         \"observation_period_start_date_plus_10_years\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  observation_period_id,\n  person_id,\n  DATE_ADD(observation_period_start_date, INTERVAL '30 day') AS observation_period_start_plus_30_days,\n  DATE_ADD(observation_period_start_date, INTERVAL '10 year') AS observation_period_start_date_plus_10_years\nFROM main.observation_period\n\n\nWhereas the SQL will be different if using multiple mutate calls (now using a sub-query).\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n               add_days(observation_period_start_date, 30L)) |&gt; \n  mutate(observation_period_start_date_plus_10_years = \n               add_years(observation_period_start_date, 10L)) |&gt; \n  select(\"observation_period_id\", \"person_id\", \n         \"observation_period_start_plus_30_days\",\n         \"observation_period_start_date_plus_10_years\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  observation_period_id,\n  person_id,\n  observation_period_start_plus_30_days,\n  DATE_ADD(observation_period_start_date, INTERVAL '10 year') AS observation_period_start_date_plus_10_years\nFROM (\n  SELECT\n    observation_period.*,\n    DATE_ADD(observation_period_start_date, INTERVAL '30 day') AS observation_period_start_plus_30_days\n  FROM main.observation_period\n) q01",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Person and observation period tables</span>"
    ]
  },
  {
    "objectID": "omop_person_obs_period.html#statistical-summaries",
    "href": "omop_person_obs_period.html#statistical-summaries",
    "title": "6  Person and observation period tables",
    "section": "6.2 Statistical summaries",
    "text": "6.2 Statistical summaries\nWe can also use summarise for various other calculations\n\ncdm$person |&gt; \n  summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q05_year_of_birth = quantile(year_of_birth, 0.05, na.rm=TRUE),\n            mean_year_of_birth = round(mean(year_of_birth, na.rm=TRUE),0),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q95_year_of_birth = quantile(year_of_birth, 0.95, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt;  \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpS6sHuH\\file865834e1e58.duckdb]\n$ min_year_of_birth    &lt;int&gt; 1923\n$ q05_year_of_birth    &lt;dbl&gt; 1927\n$ mean_year_of_birth   &lt;dbl&gt; 1971\n$ median_year_of_birth &lt;dbl&gt; 1970\n$ q95_year_of_birth    &lt;dbl&gt; 2018\n$ max_year_of_birth    &lt;int&gt; 2023\n\n\nAs we’ve seen before, we can also quickly get results for various groupings or restrictions\n\ngrouped_summary &lt;- cdm$person |&gt; \n   group_by(gender_concept_id) |&gt; \n   summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q25_year_of_birth = quantile(year_of_birth, 0.25, na.rm=TRUE),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q75_year_of_birth = quantile(year_of_birth, 0.75, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt; \n  left_join(cdm$concept, \n            by=c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n   collect() \n\ngrouped_summary |&gt; \n  ggplot(aes(x = concept_name, group = concept_name,\n             fill = concept_name)) +\n  geom_boxplot(aes(\n    lower = q25_year_of_birth, \n    upper = q75_year_of_birth, \n    middle = median_year_of_birth, \n    ymin = min_year_of_birth, \n    ymax = max_year_of_birth),\n    stat = \"identity\", width = 0.5) + \n  theme_bw()+ \n  theme(legend.position = \"none\") +\n  xlab(\"\")",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Person and observation period tables</span>"
    ]
  },
  {
    "objectID": "omop_vocabularies.html",
    "href": "omop_vocabularies.html",
    "title": "7  Vocabulary tables",
    "section": "",
    "text": "7.1 Concept",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vocabulary tables</span>"
    ]
  },
  {
    "objectID": "omop_vocabularies.html#concept-ancestor",
    "href": "omop_vocabularies.html#concept-ancestor",
    "title": "7  Vocabulary tables",
    "section": "7.2 Concept ancestor",
    "text": "7.2 Concept ancestor",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vocabulary tables</span>"
    ]
  },
  {
    "objectID": "omop_vocabularies.html#counting-records",
    "href": "omop_vocabularies.html#counting-records",
    "title": "7  Vocabulary tables",
    "section": "7.3 Counting records",
    "text": "7.3 Counting records\nWhat’s the number of condition occurrence records per person in the database? We can find this out like so\n\ncdm$person |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), 0,\n    condition_occurrence_records)) |&gt; \n  group_by(condition_occurrence_records) |&gt;\n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(condition_occurrence_records, n)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHow about we were interested in getting record counts for some specific concepts related to Covid-19 symptoms?\n\ncdm$condition_occurrence |&gt; \n  filter(condition_concept_id %in% c(437663,437390,31967,\n                                     4289517,4223659, 312437,\n                                     434490,254761,77074)) |&gt; \n  group_by(condition_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept,\n            by=c(\"condition_concept_id\" = \"concept_id\")) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(concept_name, n)) +\n  theme_bw()+\n  xlab(\"\")",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vocabulary tables</span>"
    ]
  },
  {
    "objectID": "omop_clinical_tables.html",
    "href": "omop_clinical_tables.html",
    "title": "8  Clinical tables",
    "section": "",
    "text": "8.1 Vocabulary tables\nAbove we’ve got counts by specific concept IDs recorded in the condition occurrence table. What these IDs represent is described in the concept table. Here we have the name associated with the concept, along with other information such as it’s domain and vocabulary id.\ncdm$concept |&gt; \n  glimpse()\n\nRows: ??\nColumns: 10\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp8M17Hb\\filea85c7d5963ac.duckdb]\n$ concept_id       &lt;int&gt; 45756805, 45756804, 45756803, 45756802, 45756801, 457…\n$ concept_name     &lt;chr&gt; \"Pediatric Cardiology\", \"Pediatric Anesthesiology\", \"…\n$ domain_id        &lt;chr&gt; \"Provider\", \"Provider\", \"Provider\", \"Provider\", \"Prov…\n$ vocabulary_id    &lt;chr&gt; \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS…\n$ concept_class_id &lt;chr&gt; \"Physician Specialty\", \"Physician Specialty\", \"Physic…\n$ standard_concept &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"…\n$ concept_code     &lt;chr&gt; \"OMOP4821938\", \"OMOP4821939\", \"OMOP4821940\", \"OMOP482…\n$ valid_start_date &lt;date&gt; 1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, 1970…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\nOther vocabulary tables capture other information about concepts, such as the direct relationships between concepts (the concept relationship table) and hierarchical relationships between (the concept ancestor table).\ncdm$concept_relationship |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp8M17Hb\\filea85c7d5963ac.duckdb]\n$ concept_id_1     &lt;int&gt; 35804314, 35804314, 35804314, 35804327, 35804327, 358…\n$ concept_id_2     &lt;int&gt; 912065, 42542145, 42542145, 35803584, 42542145, 42542…\n$ relationship_id  &lt;chr&gt; \"Has modality\", \"Has accepted use\", \"Is current in\", …\n$ valid_start_date &lt;date&gt; 2021-01-26, 2019-08-29, 2019-08-29, 2019-05-27, 2019…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\ncdm$concept_ancestor |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp8M17Hb\\filea85c7d5963ac.duckdb]\n$ ancestor_concept_id      &lt;int&gt; 375415, 727760, 735979, 438112, 529411, 14196…\n$ descendant_concept_id    &lt;int&gt; 4335743, 2056453, 41070383, 36566114, 4326940…\n$ min_levels_of_separation &lt;int&gt; 4, 1, 3, 2, 3, 3, 4, 3, 2, 5, 1, 3, 4, 2, 2, …\n$ max_levels_of_separation &lt;int&gt; 4, 1, 5, 3, 3, 6, 12, 3, 2, 10, 1, 3, 4, 2, 2…\nMore information on the vocabulary tables (as well as other tables in the OMOP CDM version 5.3) can be found at https://ohdsi.github.io/CommonDataModel/cdm53.html#Vocabulary_Tables.",
    "crumbs": [
      "Getting to know the OMOP CDM",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clinical tables</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html",
    "href": "exploring_the_cdm.html",
    "title": "9  Exploring the OMOP CDM",
    "section": "",
    "text": "9.1 Counting people\nFor this chapter, lets again use the synthetic Covid-19 dataset.\nThe OMOP CDM is person-centric, with the person table containing records to uniquely identify each person in the database. As each row refers to a unique person, we can quickly get a count of the number of individuals in the database like so\ncdm$person |&gt; \n  count()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n      n\n  &lt;dbl&gt;\n1 10754\nThe person table also contains some demographic information, including a gender concept for each person. We can get a count grouped by this variable, but as this uses a concept we’ll also need to join to the concept table to get the corresponding concept name for each concept id.\ncdm$person |&gt; \n  group_by(gender_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept, \n            by=c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n              select(\"gender_concept_id\", \"concept_name\", \"n\") |&gt; \n  collect()\n\n# A tibble: 2 × 3\n# Groups:   gender_concept_id [2]\n  gender_concept_id concept_name     n\n              &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1              8532 FEMALE        5165\n2              8507 MALE          5589\nThe observation period table contains records indicating spans of time over which clinical events can be reliably observed for the people in the person table. Someone can potentially have multiple observation periods. So say we wanted a count of people grouped by the year during which their first observation period started. We could do this like so:\nfirst_observation_period &lt;- cdm$observation_period |&gt;\n    group_by(person_id) |&gt; \n    filter(row_number() == 1) |&gt; \n    compute()\n\ncdm$person |&gt; \n  left_join(first_observation_period,\n            by = \"person_id\") |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date)) |&gt; \n  group_by(observation_period_start_year) |&gt; \n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(observation_period_start_year, n)) +\n  theme_bw()",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#counting-people",
    "href": "exploring_the_cdm.html#counting-people",
    "title": "9  Exploring the OMOP CDM",
    "section": "",
    "text": "Computing intermediate queries\n\n\n\n\n\nThe compute() function will force the computation of a query (by default to a temporary table in the database). In the example above we use it to split up two queries; the first to keep the first observation period record for each individual.\n\ncdm$observation_period |&gt;\n    group_by(person_id) |&gt; \n    filter(row_number() == 1) |&gt; \n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  observation_period_id,\n  person_id,\n  observation_period_start_date,\n  observation_period_end_date,\n  period_type_concept_id\nFROM (\n  SELECT\n    observation_period.*,\n    ROW_NUMBER() OVER (PARTITION BY person_id) AS col01\n  FROM main.observation_period\n) q01\nWHERE (col01 = 1.0)\n\n\nFollowed by a second query that left joins the person table with the result from the first (which is now in a temporary table), followed by extracted the year in which peoples first observation period starts and then, finally, a count by year.\n\ncdm$person |&gt; \n  left_join(first_observation_period,\n            by = \"person_id\") |&gt; \n  mutate(observation_period_start_year=year(observation_period_start_date)) |&gt; \n  group_by(observation_period_start_year) |&gt; \n  count() |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT observation_period_start_year, COUNT(*) AS n\nFROM (\n  SELECT\n    q01.*,\n    EXTRACT(year FROM observation_period_start_date) AS observation_period_start_year\n  FROM (\n    SELECT\n      person.*,\n      observation_period_id,\n      observation_period_start_date,\n      observation_period_end_date,\n      period_type_concept_id\n    FROM main.person\n    LEFT JOIN og_001_1736244102\n      ON (person.person_id = og_001_1736244102.person_id)\n  ) q01\n) q01\nGROUP BY observation_period_start_year\n\n\nWe could, however, have done this without compute, with instead the SQL being done all at once.\n\ncdm$person |&gt; \n  left_join(cdm$observation_period |&gt;\n    group_by(person_id) |&gt; \n    filter(row_number() == 1),\n            by = \"person_id\") |&gt; \n  mutate(observation_period_start_year=year(observation_period_start_date)) |&gt; \n  group_by(observation_period_start_year) |&gt; \n  count() |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT observation_period_start_year, COUNT(*) AS n\nFROM (\n  SELECT\n    q01.*,\n    EXTRACT(year FROM observation_period_start_date) AS observation_period_start_year\n  FROM (\n    SELECT\n      person.*,\n      observation_period_id,\n      observation_period_start_date,\n      observation_period_end_date,\n      period_type_concept_id\n    FROM main.person\n    LEFT JOIN (\n      SELECT\n        observation_period_id,\n        person_id,\n        observation_period_start_date,\n        observation_period_end_date,\n        period_type_concept_id\n      FROM (\n        SELECT\n          observation_period.*,\n          ROW_NUMBER() OVER (PARTITION BY person_id) AS col01\n        FROM main.observation_period\n      ) q01\n      WHERE (col01 = 1.0)\n    ) RHS\n      ON (person.person_id = RHS.person_id)\n  ) q01\n) q01\nGROUP BY observation_period_start_year\n\n\nIn this case the SQL is not much more complicated than before. However, you can imagine that without using computation to intermediate tables, the SQL associated with a series of data manipulations could quickly become unmanageable. Although we don’t want to overuse computation of intermediate queries, it is often a necessity when writing study analysis scripts.\nA particular advantage of computing a query, is that we can then use the result for multiple subsequent queries. For example, say we want a count of condition occurrence and drug exposure records for those born before 1970. We could get these counts independently:\n\ncdm$person |&gt; \n  filter(year_of_birth &lt; \"1970\") |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence,\n            by=\"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n      n\n  &lt;dbl&gt;\n1  9305\n\ncdm$person |&gt; \n  filter(year_of_birth &lt; \"1970\") |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$drug_exposure,\n            by=\"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n       n\n   &lt;dbl&gt;\n1 165681\n\n\nBut we could have instead first subsetted the person table and then used the result for both queries.\n\ncdm$person_pre_1970 &lt;- cdm$person |&gt; \n  filter(year_of_birth &lt; \"1970\") |&gt; \n  compute()\n\ncdm$person_pre_1970 |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence,\n            by=\"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n      n\n  &lt;dbl&gt;\n1  9305\n\ncdm$person_pre_1970 |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$drug_exposure,\n            by=\"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n       n\n   &lt;dbl&gt;\n1 165681",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#counting-records",
    "href": "exploring_the_cdm.html#counting-records",
    "title": "9  Exploring the OMOP CDM",
    "section": "9.2 Counting records",
    "text": "9.2 Counting records\nWhat’s the number of condition occurrence records per person in the database? We can find this out like so\n\ncdm$person |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), 0,\n    condition_occurrence_records)) |&gt; \n  group_by(condition_occurrence_records) |&gt;\n  count() |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(condition_occurrence_records, n)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHow about we were interested in getting record counts for some specific concepts related to Covid-19 symptoms?\n\ncdm$condition_occurrence |&gt; \n  filter(condition_concept_id %in% c(437663,437390,31967,\n                                     4289517,4223659, 312437,\n                                     434490,254761,77074)) |&gt; \n  group_by(condition_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept,\n            by=c(\"condition_concept_id\" = \"concept_id\")) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_col(aes(concept_name, n)) +\n  theme_bw()+\n  xlab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVocabulary tables\n\n\n\n\n\nAbove we’ve got counts by specific concept IDs recorded in the condition occurrence table. What these IDs represent is described in the concept table. Here we have the name associated with the concept, along with other information such as it’s domain and vocabulary id.\n\ncdm$concept |&gt; \n  glimpse()\n\nRows: ??\nColumns: 10\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n$ concept_id       &lt;int&gt; 45756805, 45756804, 45756803, 45756802, 45756801, 457…\n$ concept_name     &lt;chr&gt; \"Pediatric Cardiology\", \"Pediatric Anesthesiology\", \"…\n$ domain_id        &lt;chr&gt; \"Provider\", \"Provider\", \"Provider\", \"Provider\", \"Prov…\n$ vocabulary_id    &lt;chr&gt; \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS…\n$ concept_class_id &lt;chr&gt; \"Physician Specialty\", \"Physician Specialty\", \"Physic…\n$ standard_concept &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"…\n$ concept_code     &lt;chr&gt; \"OMOP4821938\", \"OMOP4821939\", \"OMOP4821940\", \"OMOP482…\n$ valid_start_date &lt;date&gt; 1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, 1970…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nOther vocabulary tables capture other information about concepts, such as the direct relationships between concepts (the concept relationship table) and hierarchical relationships between (the concept ancestor table).\n\ncdm$concept_relationship |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n$ concept_id_1     &lt;int&gt; 35804314, 35804314, 35804314, 35804327, 35804327, 358…\n$ concept_id_2     &lt;int&gt; 912065, 42542145, 42542145, 35803584, 42542145, 42542…\n$ relationship_id  &lt;chr&gt; \"Has modality\", \"Has accepted use\", \"Is current in\", …\n$ valid_start_date &lt;date&gt; 2021-01-26, 2019-08-29, 2019-08-29, 2019-05-27, 2019…\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099…\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\ncdm$concept_ancestor |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n$ ancestor_concept_id      &lt;int&gt; 375415, 727760, 735979, 438112, 529411, 14196…\n$ descendant_concept_id    &lt;int&gt; 4335743, 2056453, 41070383, 36566114, 4326940…\n$ min_levels_of_separation &lt;int&gt; 4, 1, 3, 2, 3, 3, 4, 3, 2, 5, 1, 3, 4, 2, 2, …\n$ max_levels_of_separation &lt;int&gt; 4, 1, 5, 3, 3, 6, 12, 3, 2, 10, 1, 3, 4, 2, 2…\n\n\nMore information on the vocabulary tables (as well as other tables in the OMOP CDM version 5.3) can be found at https://ohdsi.github.io/CommonDataModel/cdm53.html#Vocabulary_Tables.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#working-with-dates",
    "href": "exploring_the_cdm.html#working-with-dates",
    "title": "9  Exploring the OMOP CDM",
    "section": "9.3 Working with dates",
    "text": "9.3 Working with dates\nWhen working with dates, the best supported functions come from the clock package. In particular, a lot of the date manipulations we might be interested in can be achieved through the use of add_days (to add days to or subtract days from a date), add_years (same as add_days by with years as the timescale), and date_count_between (to get the difference between two dates). For example let’s see how these can be applied to date fields in the observation period table.\n\nlibrary(clock)\nlibrary(ggplot2)\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n           add_days(observation_period_start_date, 30L),\n         observation_period_start_date_plus_10_years = \n           add_years(observation_period_start_date, 10L)) |&gt; \n  glimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n$ observation_period_id                       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9,…\n$ person_id                                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9,…\n$ observation_period_start_date               &lt;date&gt; 2014-05-09, 1977-04-11, 2…\n$ observation_period_end_date                 &lt;date&gt; 2023-05-12, 1986-09-15, 2…\n$ period_type_concept_id                      &lt;int&gt; 44814724, 44814724, 448147…\n$ observation_period_start_plus_30_days       &lt;dttm&gt; 2014-06-08, 1977-05-11, 2…\n$ observation_period_start_date_plus_10_years &lt;dttm&gt; 2024-05-09, 1987-04-11, 2…\n\ncdm$observation_period |&gt; \n  dplyr::mutate(observation_days = date_count_between(\"observation_period_start_date\", \n                             \"observation_period_end_date\", \"day\"))  |&gt; \n  dplyr::mutate(observation_years = observation_days/ 365.25) |&gt; \n  collect() |&gt; \n  ggplot() +\n  geom_histogram(aes(observation_years), \n                 binwidth=2, colour=\"grey\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiping and SQL\n\n\n\n\n\nAlthough piping queries has little impact on performance when using R with data in memory, when working with a database the SQL generated will differ when using multiple function calls (with a separate operation specified in each) instead of multiple operations within a single function call.\nFor example, a single mutate function above would generate the below SQL.\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n                add_days(observation_period_start_date, 30L),\n         observation_period_start_date_plus_10_years = \n                add_years(observation_period_start_date, 10L)) |&gt; \n  select(\"observation_period_id\", \"person_id\", \n         \"observation_period_start_plus_30_days\",\n         \"observation_period_start_date_plus_10_years\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  observation_period_id,\n  person_id,\n  DATE_ADD(observation_period_start_date, INTERVAL '30 day') AS observation_period_start_plus_30_days,\n  DATE_ADD(observation_period_start_date, INTERVAL '10 year') AS observation_period_start_date_plus_10_years\nFROM main.observation_period\n\n\nWhereas the SQL will be different if using multiple mutate calls (now using a sub-query).\n\ncdm$observation_period |&gt; \n  mutate(observation_period_start_plus_30_days = \n               add_days(observation_period_start_date, 30L)) |&gt; \n  mutate(observation_period_start_date_plus_10_years = \n               add_years(observation_period_start_date, 10L)) |&gt; \n  select(\"observation_period_id\", \"person_id\", \n         \"observation_period_start_plus_30_days\",\n         \"observation_period_start_date_plus_10_years\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  observation_period_id,\n  person_id,\n  observation_period_start_plus_30_days,\n  DATE_ADD(observation_period_start_date, INTERVAL '10 year') AS observation_period_start_date_plus_10_years\nFROM (\n  SELECT\n    observation_period.*,\n    DATE_ADD(observation_period_start_date, INTERVAL '30 day') AS observation_period_start_plus_30_days\n  FROM main.observation_period\n) q01",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "exploring_the_cdm.html#statistical-summaries",
    "href": "exploring_the_cdm.html#statistical-summaries",
    "title": "9  Exploring the OMOP CDM",
    "section": "9.4 Statistical summaries",
    "text": "9.4 Statistical summaries\nWe can also use summarise for various other calculations\n\ncdm$person |&gt; \n  summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q05_year_of_birth = quantile(year_of_birth, 0.05, na.rm=TRUE),\n            mean_year_of_birth = round(mean(year_of_birth, na.rm=TRUE),0),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q95_year_of_birth = quantile(year_of_birth, 0.95, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt;  \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpETj2RL\\file7c885fc021a6.duckdb]\n$ min_year_of_birth    &lt;int&gt; 1923\n$ q05_year_of_birth    &lt;dbl&gt; 1927\n$ mean_year_of_birth   &lt;dbl&gt; 1971\n$ median_year_of_birth &lt;dbl&gt; 1970\n$ q95_year_of_birth    &lt;dbl&gt; 2018\n$ max_year_of_birth    &lt;int&gt; 2023\n\n\nAs we’ve seen before, we can also quickly get results for various groupings or restrictions\n\ngrouped_summary &lt;- cdm$person |&gt; \n   group_by(gender_concept_id) |&gt; \n   summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q25_year_of_birth = quantile(year_of_birth, 0.25, na.rm=TRUE),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q75_year_of_birth = quantile(year_of_birth, 0.75, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt; \n  left_join(cdm$concept, \n            by=c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n   collect() \n\ngrouped_summary |&gt; \n  ggplot(aes(x = concept_name, group = concept_name,\n             fill = concept_name)) +\n  geom_boxplot(aes(\n    lower = q25_year_of_birth, \n    upper = q75_year_of_birth, \n    middle = median_year_of_birth, \n    ymin = min_year_of_birth, \n    ymax = max_year_of_birth),\n    stat = \"identity\", width = 0.5) + \n  theme_bw()+ \n  theme(legend.position = \"none\") +\n  xlab(\"\")",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html",
    "href": "creating_cohorts.html",
    "title": "10  Adding cohorts to the CDM",
    "section": "",
    "text": "10.1 What is a cohort?\nWhen performing research with the OMOP common data model we often want to identify groups of individuals who share some set of characteristics. The criteria for including individuals can range from the seemingly simple (e.g. people diagnosed with asthma) to the much more complicated (e.g. adults diagnosed with asthma who had a year of prior observation time in the database prior to their diagnosis, had no prior history of chronic obstructive pulmonary disease, and no history of use of short-acting beta-antagonists).\nThe set of people we identify are cohorts, and the OMOP CDM has a specific structure by which they can be represented, with a cohort table having four required fields: 1) cohort definition id (a unique identifier for each cohort), 2) subject id (a foreign key to the subject in the cohort - typically referring to records in the person table), 3) cohort start date, and 4) cohort end date. Individuals can enter a cohort multiple times, but the time periods in which they are in the cohort cannot overlap. Individuals will only be considered in a cohort when they have have an ongoing observation period.\nIt is beyond the scope of this book to describe all the different ways cohorts could be created, however in this chapter we provide a summary of some of the key building blocks for cohort creation. Cohort-building pipelines can be created following these principles to create a wide range of study cohorts.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#set-up",
    "href": "creating_cohorts.html#set-up",
    "title": "10  Adding cohorts to the CDM",
    "section": "10.2 Set up",
    "text": "10.2 Set up\nWe’ll use our synthetic dataset for demonstrating how cohorts can be constructed.\n\nlibrary(CDMConnector)\nlibrary(CodelistGenerator)\nlibrary(CohortConstructor)\nlibrary(CohortCharacteristics)\nlibrary(dplyr)\n        \ndb &lt;- DBI::dbConnect(duckdb::duckdb(),\n              dbdir = eunomiaDir(datasetName = \"synthea-covid19-10k\"))\ncdm &lt;- cdmFromCon(db, cdmSchema = \"main\", writeSchema = \"main\")",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#general-concept-based-cohort",
    "href": "creating_cohorts.html#general-concept-based-cohort",
    "title": "10  Adding cohorts to the CDM",
    "section": "10.3 General concept based cohort",
    "text": "10.3 General concept based cohort\nOften study cohorts will be based around a specific clinical event identified by some set of clinical codes. Here, for example, we use the CohortConstructor package to create a cohort of people with Covid-19. For this we are identifying any clinical records with the code 37311061.\n\ncdm$covid &lt;- conceptCohort(cdm = cdm, \n                     conceptSet = list(\"covid\" = 37311061), \n                     name = \"covid\")\ncdm$covid\n\n# Source:   table&lt;main.covid&gt; [?? x 4]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp4I09Pr\\fileadcc1a59546c.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1       7779 2020-04-11        2020-04-25     \n 2                    1       8119 2021-01-12        2021-02-16     \n 3                    1       3633 2021-08-18        2021-09-02     \n 4                    1       3691 2021-01-06        2021-01-26     \n 5                    1       5799 2020-10-21        2020-11-07     \n 6                    1       6433 2021-04-10        2021-04-18     \n 7                    1       7933 2020-12-15        2021-01-12     \n 8                    1       4032 2021-04-26        2021-05-22     \n 9                    1        394 2020-04-09        2020-05-16     \n10                    1       2686 2021-01-27        2021-02-27     \n# ℹ more rows\n\n\n\n\n\n\n\n\nFinding appropriate codes\n\n\n\n\n\nIn the defining the cohorts above we have needed to provide concept IDs to define our cohort. But, where do these come from?\nWe can search for codes of interest using the CodelistGenerator package. This can be done using a text search with the function CodelistGenerator::getCandidateCodes(). For example, we can have found the code we used above (and many others) like so:\n\ngetCandidateCodes(cdm = cdm, \n                  keywords = c(\"coronavirus\",\"covid\"),\n                  domains = \"condition\",\n                  includeDescendants = TRUE)\n\nLimiting to domains of interest\nGetting concepts to include\nAdding descendants\nSearch completed. Finishing up.\n✔ 37 candidate concepts identified\n\nTime taken: 0 minutes and 3 seconds\n\n\n# A tibble: 37 × 6\n   concept_id found_from   concept_name domain_id vocabulary_id standard_concept\n        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;           \n 1    3656669 From initia… Dyspnea cau… Condition SNOMED        S               \n 2     756039 From initia… Respiratory… Condition OMOP Extensi… S               \n 3    3661885 From initia… Fever cause… Condition SNOMED        S               \n 4    3655975 From initia… Sepsis due … Condition SNOMED        S               \n 5     703447 From initia… High risk c… Condition SNOMED        S               \n 6   37310283 From initia… Gastroenter… Condition SNOMED        S               \n 7    3655977 From initia… Rhabdomyoly… Condition SNOMED        S               \n 8   40479642 From initia… Pneumonia d… Condition SNOMED        S               \n 9   37016927 From initia… Pneumonia c… Condition SNOMED        S               \n10    3656667 From initia… Cardiomyopa… Condition SNOMED        S               \n# ℹ 27 more rows\n\n\nWe can also do automated searches that make use of the hierarchies in the vocabularies. Here, for example, we find the code for the drug ingredient Acetaminophen and all of it’s descendants.\n\nCodelistGenerator::getDrugIngredientCodes(cdm = cdm, \n                                          name = \"acetaminophen\")\n\n\n\n\n── 1 codelist ──────────────────────────────────────────────────────────────────\n\n\n\n- 161_acetaminophen (25747 codes)\n\n\nNote that in practice clinical expertise is vital in the identification of appropriate codes so as to decide which the codes are in line with the clinical idea at hand.\n\n\n\nWe can see that as well as having the cohort entries above, our cohort table is associated with several attributes.\nFirst, we can see the settings associated with cohort.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 4\n$ cohort_definition_id &lt;int&gt; 1\n$ cohort_name          &lt;chr&gt; \"covid\"\n$ cdm_version          &lt;chr&gt; \"5.3\"\n$ vocabulary_version   &lt;chr&gt; \"v5.0 22-JUN-22\"\n\n\nSecond, we can get counts of the cohort.\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 964\n$ number_subjects      &lt;int&gt; 964\n\n\nAnd last we can see attrition related to the cohort.\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 4\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964\n$ reason_id            &lt;int&gt; 1, 2, 3, 4\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record start &lt;= rec…\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0\n\n\nAs we will see below these attributes of the cohorts become particularly useful as we apply further restrictions on our cohort.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#applying-inclusion-criteria",
    "href": "creating_cohorts.html#applying-inclusion-criteria",
    "title": "10  Adding cohorts to the CDM",
    "section": "10.4 Applying inclusion criteria",
    "text": "10.4 Applying inclusion criteria\n\n10.4.1 Only include first cohort entry per person\nLet’s say we first want to restrict to first entry.\n\ncdm$covid &lt;- cdm$covid |&gt; \n     requireIsFirstEntry() \n\n\n\n10.4.2 Restrict to study period\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireInDateRange(dateRange = c(as.Date(\"2020-09-01\"), NA))\n\n\n\n10.4.3 Applying demographic inclusion criteria\nSay for our study we want to include people with a GI bleed who were aged 40 or over at the time. We can use the add variables with these characteristics as seen in chapter 4 and then filter accordingly. The function CDMConnector::record_cohort_attrition() will then update our cohort attributes as we can see below.\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireDemographics(ageRange = c(18, 64), sex = \"Male\")\n\n\n\n10.4.4 Applying cohort-based inclusion criteria\nAs well as requirements about specific demographics, we may also want to use another cohort for inclusion criteria. Let’s say we want to exclude anyone with a history of cardiac conditions before their Covid-19 cohort entry.\nWe can first generate this new cohort table with records of cardiac conditions.\n\ncdm$cardiac &lt;- conceptCohort(\n  cdm = cdm,\n  list(\"myocaridal_infarction\" = c(\n    317576, 313217, 321042, 4329847\n  )), \nname = \"cardiac\"\n)\ncdm$cardiac\n\n# Source:   table&lt;main.cardiac&gt; [?? x 4]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\Rtmp4I09Pr\\fileadcc1a59546c.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1        120 2000-04-16        2000-04-16     \n 2                    1       2466 1976-01-30        1976-01-30     \n 3                    1       4756 1962-02-03        1962-02-03     \n 4                    1       6282 1998-04-13        1998-04-13     \n 5                    1       8868 2013-07-09        2013-07-09     \n 6                    1        402 2012-03-10        2012-03-10     \n 7                    1       1198 1993-08-14        1993-08-14     \n 8                    1       6861 2011-04-21        2011-04-21     \n 9                    1        518 2018-03-11        2018-03-11     \n10                    1       2127 2018-03-07        2018-03-07     \n# ℹ more rows\n\n\nAnd now we can apply the inclusion criteria that individuals have zero intersections with the table in the time prior to their Covid-19 cohort entry.\n\ncdm$covid &lt;- cdm$covid |&gt; \n  requireCohortIntersect(targetCohortTable = \"cardiac\", \n                         indexDate = \"cohort_start_date\", \n                         window = c(-Inf, -1), \n                         intersections = 0) \n\nNote if we had wanted to have required that individuals did have a history of a cardiac condition we would instead have set intersections = c(1, Inf) above.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "creating_cohorts.html#cohort-attributes",
    "href": "creating_cohorts.html#cohort-attributes",
    "title": "10  Adding cohorts to the CDM",
    "section": "10.5 Cohort attributes",
    "text": "10.5 Cohort attributes\nWe can see that the attributes of the cohort were updated as we applied the inclusion criteria.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 8\n$ cohort_definition_id   &lt;int&gt; 1\n$ cohort_name            &lt;chr&gt; \"covid\"\n$ age_range              &lt;chr&gt; \"18_64\"\n$ sex                    &lt;chr&gt; \"Male\"\n$ min_prior_observation  &lt;dbl&gt; 0\n$ min_future_observation &lt;dbl&gt; 0\n$ cdm_version            &lt;chr&gt; \"5.3\"\n$ vocabulary_version     &lt;chr&gt; \"v5.0 22-JUN-22\"\n\n\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 158\n$ number_subjects      &lt;int&gt; 158\n\n\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 11\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964, 964, 793, 363, 171, 171, 171,…\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964, 964, 793, 363, 171, 171, 171,…\n$ reason_id            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record start &lt;= rec…\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0, 0, 171, 430, 192, 0, 0, 13\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0, 0, 171, 430, 192, 0, 0, 13\n\n\nFor attrition, we can use CohortConstructor::summariseCohortAttrition() and then CohortConstructor::tableCohortAttrition() to better view the impact of applying the additional inclusion criteria.\n\nattrition_summary &lt;- summariseCohortAttrition(cdm$covid)\nplotCohortAttrition(attrition_summary)",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "adding_features.html",
    "href": "adding_features.html",
    "title": "11  Identifying patient characteristics",
    "section": "",
    "text": "11.1 Adding specific demographics\nFor this chapter, we’ll again use our example COVID-19 dataset.\nAs part of an analysis we almost always have a need to identify certain characteristics related to the individuals in our data. These characteristics might be time-invariant (ie a characteristic that does not change as time passes and a person ages) or time-varying.1\nThe PatientProfiles package makes it easy for us to add demographic information to tables in the OMOP CDM. Like the CDMConnector package we’ve seen previously, the fact that the structure of the OMOP CDM is known allows the PatientProfiles package to abstract away some common data manipulations required to do research with patient-level data.2\nLet’s say we are interested in individuals’ age and sex at time of diagnosis with COVID-19. We can add these variables to the table like so (noting that because age is time-varying, we have to specify the variable with the date for which we want to calculate age relative to).\ncdm$condition_occurrence &lt;- cdm$condition_occurrence |&gt; \n  addSex() |&gt; \n  addAge(indexDate = \"condition_start_date\")\n\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 3, 8, 9, 12, 18, 19, 20, 21, 23, 24, …\n$ person_id                     &lt;int&gt; 2, 7, 16, 16, 25, 42, 42, 42, 44, 47, 47…\n$ condition_concept_id          &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 67, 168, 171, 257, 439, 437, 435, 45…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000067, 1000168, 1000171, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"230690007\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Female\", \"M…\n$ age                           &lt;int&gt; 57, 97, 75, 77, 55, 52, 5, 48, 35, 29, 2…\ncdm$condition_occurrence |&gt; \n  addSexQuery() |&gt; \n  dplyr::show_query()\n\nWarning: ! The following columns will be overwritten: sex\n\n\n&lt;SQL&gt;\nSELECT\n  condition_occurrence_id,\n  og_002_1736244199.person_id AS person_id,\n  condition_concept_id,\n  condition_start_date,\n  condition_start_datetime,\n  condition_end_date,\n  condition_end_datetime,\n  condition_type_concept_id,\n  condition_status_concept_id,\n  stop_reason,\n  provider_id,\n  visit_occurrence_id,\n  visit_detail_id,\n  condition_source_value,\n  condition_source_concept_id,\n  condition_status_source_value,\n  age,\n  RHS.sex AS sex\nFROM og_002_1736244199\nLEFT JOIN (\n  SELECT\n    person_id,\n    CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n  FROM (\n    SELECT\n      LHS.person_id AS person_id,\n      gender_concept_id,\n      year_of_birth,\n      month_of_birth,\n      day_of_birth\n    FROM (\n      SELECT DISTINCT person_id\n      FROM og_002_1736244199\n    ) LHS\n    INNER JOIN main.person\n      ON (LHS.person_id = person.person_id)\n  ) q01\n) RHS\n  ON (og_002_1736244199.person_id = RHS.person_id)\nWe now have two variables added containing values for age and sex.\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 3, 8, 9, 12, 18, 19, 20, 21, 23, 24, …\n$ person_id                     &lt;int&gt; 2, 7, 16, 16, 25, 42, 42, 42, 44, 47, 47…\n$ condition_concept_id          &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 67, 168, 171, 257, 439, 437, 435, 45…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000067, 1000168, 1000171, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"230690007\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Female\", \"M…\n$ age                           &lt;int&gt; 57, 97, 75, 77, 55, 52, 5, 48, 35, 29, 2…\nAnd with these now added it is straightforward to calculate mean age at condition start date by sex or even plot the distribution of age at diagnosis by sex.\ncdm$condition_occurrence |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE), .by = \"sex\") |&gt; \n  collect()\n\n# A tibble: 2 × 2\n  sex    mean_age\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Female     50.8\n2 Male       56.5\ncdm$condition_occurrence |&gt;\n  select(\"person_id\", \"age\", \"sex\") |&gt; \n  collect()  |&gt;\n  ggplot(aes(fill = sex)) +\n  facet_grid(sex ~ .) +\n  geom_histogram(aes(age), colour = \"black\", binwidth = 5) +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#adding-multiple-demographics-simultaneously",
    "href": "adding_features.html#adding-multiple-demographics-simultaneously",
    "title": "11  Identifying patient characteristics",
    "section": "11.2 Adding multiple demographics simultaneously",
    "text": "11.2 Adding multiple demographics simultaneously\nWe’ve now seen individual functions from PatientProfiles to add age and sex, and the package has others to add other characteristics like days of prior observation in the database (rather unimaginatively named PatientProfiles::addPriorObservation()). In additional to these individuals functions, the package also provides a more general function to get all of these characteristics at the same time.3\n\ncdm$drug_exposure &lt;- cdm$drug_exposure |&gt; \n  addDemographics(indexDate = \"drug_exposure_start_date\")\n\ncdm$drug_exposure |&gt; \n  glimpse()\n\nRows: ??\nColumns: 27\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ drug_exposure_id             &lt;int&gt; 245761, 245762, 245763, 245765, 245766, 2…\n$ person_id                    &lt;int&gt; 7764, 7764, 7764, 7764, 7764, 7764, 7764,…\n$ drug_concept_id              &lt;int&gt; 40213227, 40213201, 40213198, 40213154, 4…\n$ drug_exposure_start_date     &lt;date&gt; 2015-02-08, 2010-01-10, 2010-01-10, 2016…\n$ drug_exposure_start_datetime &lt;dttm&gt; 2015-02-08 22:40:04, 2010-01-10 22:40:04…\n$ drug_exposure_end_date       &lt;date&gt; 2015-02-08, 2010-01-10, 2010-01-10, 2016…\n$ drug_exposure_end_datetime   &lt;dttm&gt; 2015-02-08 22:40:04, 2010-01-10 22:40:04…\n$ verbatim_end_date            &lt;date&gt; 2015-02-08, 2010-01-10, 2010-01-10, 2016…\n$ drug_type_concept_id         &lt;int&gt; 32869, 32869, 32869, 32869, 32869, 32869,…\n$ stop_reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ refills                      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quantity                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ days_supply                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sig                          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ route_concept_id             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lot_number                   &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ provider_id                  &lt;int&gt; 14656, 14656, 14656, 14656, 14656, 14656,…\n$ visit_occurrence_id          &lt;int&gt; 80896, 80891, 80891, 80895, 80896, 80894,…\n$ visit_detail_id              &lt;int&gt; 1080896, 1080891, 1080891, 1080895, 10808…\n$ drug_source_value            &lt;chr&gt; \"113\", \"33\", \"133\", \"140\", \"140\", \"140\", …\n$ drug_source_concept_id       &lt;int&gt; 40213227, 40213201, 40213198, 40213154, 4…\n$ route_source_value           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dose_unit_source_value       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age                          &lt;int&gt; 71, 66, 66, 72, 71, 69, 67, 70, 64, 68, 6…\n$ sex                          &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ prior_observation            &lt;int&gt; 2597, 742, 742, 2968, 2597, 1855, 1113, 2…\n$ future_observation           &lt;int&gt; 896, 2751, 2751, 525, 896, 1638, 2380, 12…\n\n\nWith these characteristics now all added, we can now calculate mean age, prior observation (how many days have passed since the individual’s most recent observation start date), and future observation (how many days until the individual’s nearest observation end date) at drug exposure start date by sex.\n\ncdm$drug_exposure |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE),\n            mean_prior_observation = mean(prior_observation, na.rm=TRUE),\n            mean_future_observation = mean(future_observation, na.rm=TRUE),\n            .by = \"sex\") |&gt; \n  collect()\n\n# A tibble: 2 × 4\n  sex    mean_age mean_prior_observation mean_future_observation\n  &lt;chr&gt;     &lt;dbl&gt;                  &lt;dbl&gt;                   &lt;dbl&gt;\n1 Female     39.4                  2096.                   1661.\n2 Male       43.0                  2455.                   1768.\n\n\n\n\n\n\n\n\nReturning a query from PatientProfiles rather than the result\n\n\n\n\n\nIn the above examples the functions from PatientProfiles will execute queries with the results written to a table in the database (either temporary if no name is provided or a permanent table if one is given). We might though instead want to to instead just get the underlying query back so that we have more control over how and when the query will be executed.\n\ncdm$visit_occurrence |&gt; \n  addSex() |&gt; \n  filter(sex == \"Male\") |&gt; \n  dplyr::show_query()\n\n&lt;SQL&gt;\nSELECT og_004_1736244204.*\nFROM og_004_1736244204\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSex(name = \"my_new_table\") |&gt; \n  filter(sex == \"Male\") |&gt; \n  dplyr::show_query()\n\n&lt;SQL&gt;\nSELECT my_new_table.*\nFROM main.my_new_table\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSexQuery() |&gt; \n  filter(sex == \"Male\") |&gt; \n  dplyr::show_query()\n\n&lt;SQL&gt;\nSELECT q01.*\nFROM (\n  SELECT visit_occurrence.*, sex\n  FROM main.visit_occurrence\n  LEFT JOIN (\n    SELECT\n      person_id,\n      CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n    FROM (\n      SELECT\n        LHS.person_id AS person_id,\n        gender_concept_id,\n        year_of_birth,\n        month_of_birth,\n        day_of_birth\n      FROM (\n        SELECT DISTINCT person_id\n        FROM main.visit_occurrence\n      ) LHS\n      INNER JOIN main.person\n        ON (LHS.person_id = person.person_id)\n    ) q01\n  ) RHS\n    ON (visit_occurrence.person_id = RHS.person_id)\n) q01\nWHERE (sex = 'Male')",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#creating-categories",
    "href": "adding_features.html#creating-categories",
    "title": "11  Identifying patient characteristics",
    "section": "11.3 Creating categories",
    "text": "11.3 Creating categories\nWhen we add age, either via addAge or addDemographics, we can also add another variable containing age groups. These age groups are specified in a list of vectors, each of which contain the lower and upper bounds.\n\ncdm$visit_occurrence &lt;- cdm$visit_occurrence |&gt;\n  addAge(indexDate = \"visit_start_date\",\n    ageGroup = list(c(0,17), c(18, 64),\n                    c(65, Inf)))\n\ncdm$visit_occurrence |&gt; \n  # data quality issues with our synthetic data means we have \n  # some negative ages so will drop these\n  filter(age &gt;= 0) |&gt; \n  group_by(age_group) |&gt; \n  tally() |&gt; \n  collect() |&gt; \n  ggplot() + \n  geom_col(aes(x = age_group, y = n)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nPatientProfiles also provides a more general function for adding categories. Can you guess it’s name? That’s right, we have PatientProfiles::addCategories() for this.\n\ncdm$condition_occurrence |&gt;\n  addPriorObservation(indexDate = \"condition_start_date\") |&gt;\n  addCategories(\n    variable = \"prior_observation\",\n    categories = list(\"prior_observation_group\" = list(\n      c(0, 364), c(365, Inf)  \n    ))\n  ) |&gt; \n  glimpse()\n\nRows: ??\nColumns: 20\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 3, 8, 9, 12, 18, 19, 20, 21, 23, 24, …\n$ person_id                     &lt;int&gt; 2, 7, 16, 16, 25, 42, 42, 42, 44, 47, 47…\n$ condition_concept_id          &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-04-07, 2020-02-11, 202…\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, …\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ visit_occurrence_id           &lt;int&gt; 19, 67, 168, 171, 257, 439, 437, 435, 45…\n$ visit_detail_id               &lt;int&gt; 1000019, 1000067, 1000168, 1000171, 1000…\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"230690007\", \"230690007\", \"…\n$ condition_source_concept_id   &lt;int&gt; 381316, 381316, 381316, 313217, 381316, …\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Female\", \"M…\n$ age                           &lt;int&gt; 57, 97, 75, 77, 55, 52, 5, 48, 35, 29, 2…\n$ prior_observation             &lt;int&gt; 3437, 2842, 2366, 2968, 1904, 17003, 0, …\n$ prior_observation_group       &lt;chr&gt; \"365 or above\", \"365 or above\", \"365 or …",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#adding-custom-variables",
    "href": "adding_features.html#adding-custom-variables",
    "title": "11  Identifying patient characteristics",
    "section": "11.4 Adding custom variables",
    "text": "11.4 Adding custom variables\nWhile PatientProfiles provides a range of functions that can help add characteristics of interest, you may also want to add other features . Obviously we can’t cover here all possible custom characteristics you may wish to add. However, two common groups of custom features are those that are derived from other variables in the same table and others that are taken from other tables and joined to our particular table of interest.\nIn the first case where we want to add a new variable derived from other variables in our table we’ll typically be using dplyr::mutate(). For example, perhaps we just want to add a new variable to our observation period table containing the year of individuals’ observation period start date. This is rather straightforward.\n\ncdm$observation_period &lt;- cdm$observation_period |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date))\n\ncdm$observation_period |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ observation_period_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ person_id                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ observation_period_start_date &lt;date&gt; 2014-05-09, 1977-04-11, 2014-04-19, 201…\n$ observation_period_end_date   &lt;date&gt; 2023-05-12, 1986-09-15, 2023-04-22, 202…\n$ period_type_concept_id        &lt;int&gt; 44814724, 44814724, 44814724, 44814724, …\n$ observation_period_start_year &lt;dbl&gt; 2014, 1977, 2014, 2014, 2013, 2013, 2013…\n\n\nThe second case is normally a more complex task where adding a new variable involves joining to some other table. This table may well have been created by some intermediate query that we wrote to derive the variable of interest. For example, lets say we want to add each number of condition occurrence records for each individual to the person table (remember that we saw how to calculate this in the previous chapter). For this we will need to do a join between the person and condition occurrence tables (as some people might not have any records in the condition occurrence table). Here we’ll save the create a table containing just the information we’re interested in and compute to a temporary table.\n\ncondition_summary &lt;- cdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), \n    0, condition_occurrence_records)) |&gt; \n  compute()\n\ncondition_summary |&gt; \n  glimpse()\n\nRows: ??\nColumns: 2\nDatabase: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n$ person_id                    &lt;int&gt; 2, 7, 16, 18, 25, 36, 42, 44, 47, 51, 52,…\n$ condition_occurrence_records &lt;dbl&gt; 1, 1, 2, 2, 1, 4, 3, 2, 5, 1, 3, 2, 1, 4,…\n\n\nWe can see what goes on behind the scenes by viewing the associated SQL.\n\ncdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(cdm$condition_occurrence |&gt; \n  group_by(person_id) |&gt; \n  count(name = \"condition_occurrence_records\"),\n  by=\"person_id\") |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = if_else(\n    is.na(condition_occurrence_records), \n    0, condition_occurrence_records)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  person_id,\n  CASE WHEN ((condition_occurrence_records IS NULL)) THEN 0.0 WHEN NOT ((condition_occurrence_records IS NULL)) THEN condition_occurrence_records END AS condition_occurrence_records\nFROM (\n  SELECT person.person_id AS person_id, condition_occurrence_records\n  FROM main.person\n  LEFT JOIN (\n    SELECT person_id, COUNT(*) AS condition_occurrence_records\n    FROM og_002_1736244199\n    GROUP BY person_id\n  ) RHS\n    ON (person.person_id = RHS.person_id)\n) q01\n\n\n\n\n\n\n\n\nTaking care with joins\n\n\n\n\n\nWhen adding variables through joins we need to pay particular attention to the dimensions of the resulting table. While sometimes we may want to have additional rows added as well as new columns, this is often not desired. If we, for example, have a table with one row per person then a left join to a table with multiple rows per person can then result in a table with multiple rows per person.\nExamples where to be careful include when joining to the observation period table, as individuals can have multiple observation periods, and when working with cohorts (which are the focus of the next chapter) as individuals can also enter the same study cohort multiple times.\nJust to underline how problematic joins can become if we don’t take care, here we join the condition occurrence table and the drug exposure table both of which have multiple records per person. Remember this is just with our small synthetic data, so when working with real patient data which is oftentimes much, much larger this would be extremely problematic (and would unlikely be needed to answer any research question). In other words, don’t try this at home!\n\ncdm$condition_occurrence |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n      n\n  &lt;dbl&gt;\n1  9967\n\ncdm$drug_exposure |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n       n\n   &lt;dbl&gt;\n1 337509\n\ncdm$condition_occurrence |&gt; \n  select(person_id, condition_start_date) |&gt; \n  left_join(cdm$drug_exposure |&gt; \n  select(person_id, drug_exposure_start_date), \n  by = \"person_id\") |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB v1.1.3 [eburn@Windows 10 x64:R 4.4.0/C:\\Users\\eburn\\AppData\\Local\\Temp\\RtmpsZeKJp\\file4bec4a911c4e.duckdb]\n       n\n   &lt;dbl&gt;\n1 410683",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "adding_features.html#footnotes",
    "href": "adding_features.html#footnotes",
    "title": "11  Identifying patient characteristics",
    "section": "",
    "text": "In some datasets characteristics that could conceptually be considered as time-varying are encoded as time-invariant. One example for the latter is that in some cases an individual may be associated with a particular socioeconomic status or nationality that for the purposes of the data is treated as time-invariant.↩︎\nAlthough these manipulations can on the face of it seem quite simple, their implementation across different database platforms with different data granularity (for example whether day of birth has been filled in for all patients or not) presents challenges that the PatientProfiles package solves for us.↩︎\nThis function also provides a more time efficient method that getting the characteristics one by one. This is because these characteristics are all derived from the OMOP CDM person and observation period tables and so can be identified simultaneously.↩︎",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html",
    "href": "working_with_cohorts.html",
    "title": "12  Working with cohorts",
    "section": "",
    "text": "12.1 Cohort intersections\nPatientProfiles::addCohortIntersect()",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#intersection-between-two-cohorts",
    "href": "working_with_cohorts.html#intersection-between-two-cohorts",
    "title": "12  Working with cohorts",
    "section": "12.2 Intersection between two cohorts",
    "text": "12.2 Intersection between two cohorts",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#set-up",
    "href": "working_with_cohorts.html#set-up",
    "title": "12  Working with cohorts",
    "section": "12.3 Set up",
    "text": "12.3 Set up\n\nlibrary(CDMConnector)\nlibrary(dplyr)\nlibrary(PatientProfiles)\n\n# For this example we will use GiBleed data set\nCDMConnector::downloadEunomiaData(datasetName = \"GiBleed\")        \ndb &lt;- DBI::dbConnect(duckdb::duckdb(), eunomia_dir())\n\ncdm &lt;- cdmFromCon(db, cdmSchema = \"main\", writeSchema = \"main\")\n\ncdm &lt;- cdm |&gt; \n  generate_concept_cohort_set(concept_set = list(\"gi_bleed\" = 192671), \n                            limit = \"all\", \n                            end = 30,\n                            name = \"gi_bleed\",\n                            overwrite = TRUE) |&gt; \n  generate_concept_cohort_set(concept_set = list(\"acetaminophen\" = c(1125315,\n                                                              1127078,\n                                                              1127433,\n                                                              40229134,\n                                                              40231925,\n                                                              40162522,\n                                                              19133768)), \n                              limit = \"all\", \n                            # end = \"event_end_date\",\n                            name = \"acetaminophen\",\n                            overwrite = TRUE)\n\n\n12.3.1 Flag\n\ncdm$gi_bleed &lt;- cdm$gi_bleed |&gt; \n  addCohortIntersectFlag(targetCohortTable = \"acetaminophen\",\n                         window = list(c(-Inf, -1), c(0,0), c(1, Inf)))\n\ncdm$gi_bleed |&gt; \n  summarise(acetaminophen_prior = sum(acetaminophen_minf_to_m1), \n            acetaminophen_index = sum(acetaminophen_0_to_0),\n            acetaminophen_post = sum(acetaminophen_1_to_inf)) |&gt; \n  collect()\n\n# A tibble: 1 × 3\n  acetaminophen_prior acetaminophen_index acetaminophen_post\n                &lt;dbl&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n1                 467                 467                476\n\n\n\n\n12.3.2 Count\n\n\n12.3.3 Date and times",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "href": "working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "title": "12  Working with cohorts",
    "section": "12.4 Intersection between a cohort and tables with patient data",
    "text": "12.4 Intersection between a cohort and tables with patient data",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "summarising_cohorts.html",
    "href": "summarising_cohorts.html",
    "title": "13  Summarising cohorts",
    "section": "",
    "text": "13.1 Summarising patient demographics\nPatientProfiles::summariseCharacteristics …..",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summarising cohorts</span>"
    ]
  },
  {
    "objectID": "summarising_cohorts.html#large-scale-characterisation",
    "href": "summarising_cohorts.html#large-scale-characterisation",
    "title": "13  Summarising cohorts",
    "section": "13.2 Large scale characterisation",
    "text": "13.2 Large scale characterisation\nPatientProfiles::summariseLargeScaleCharacteristics …..\nTO ADD",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summarising cohorts</span>"
    ]
  }
]